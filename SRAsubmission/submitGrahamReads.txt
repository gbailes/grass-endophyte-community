## let's get graham's reads online 

## for synching up github repos on office comp:

git remote set-url origin git@github.com:gbailes/grass-endophyte-community.git

## get the files off the lab computer onto office comp, for ease of access:

mkdir -p "/media/vol/graham/rawReads"

## from office comp:
getFile="/media/vol1/daniel/graham/rawReads/"
putItHere="/media/vol/graham/rawReads"
scp -r test@132.180.112.115:$getFile $putItHere


##################

## repeat the process for the fichtelgebirge reads

## the files for that are here:
/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sraSubmission

##### submit the raw read files #######

## we need the reads themselves. 

## put these on the office tower and lab comp, so we can start an upload:

getFile="/home/daniel/Downloads/reads.zip"
putItHere="/media/vol1/daniel/graham"
scp $getFile test@132.180.112.115:$putItHere

## that will need to be gzipped:
mkdir "/media/vol1/daniel/graham/rawReads"

unzip reads.zip
mv *fastq /media/vol1/daniel/graham/rawReads/

## ftp upload, can this be automated?

## following:
https://www.ncbi.nlm.nih.gov/sra/docs/submitfiles/#file-transfer-protocol-ftp

## should be something like this:


ftp ftp-private.ncbi.nlm.nih.gov 

subftp

iodrac9octOckEpp

cd /uploads/danchurchthomas_gmail.com_Gq8kA048/grahamRawRead_try2


Address: ftp-private.ncbi.nlm.nih.gov
Username: subftp
Password: iodrac9octOckEpp

cd uploads/danchurchthomas_gmail.com_Gq8kA048

rename 
/uploads/danchurchthomas_gmail.com_Gq8kA048/grahamRawReads

cd /media/vol1/daniel/graham/rawReads
{ nohup ftp -n ftp-private.ncbi.nlm.nih.gov <<- ENDOFMESSAGE
user subftp iodrac9octOckEpp    
binary
prompt
cd /uploads/danchurchthomas_gmail.com_Gq8kA048/grahamRawRead_try
mput  *fastq.gz
bye
End-Of-Session 
ENDOFMESSAGE
} &

## 142738
## this keeps dying. try without automating it:

ftp ftp-private.ncbi.nlm.nih.gov 

prompt

subftp

iodrac9octOckEpp

cd /uploads/danchurchthomas_gmail.com_Gq8kA048/grahamRawRead_try2

mput  *fastq.gz

## meh. Internet is shitty. 

## let's check our files, make sure they all made it:
## store this in: whatsInGrahamTry2.txt 

## let's verify that all files are there

## on 

## look at the hand edited version, just filenames and sizes:

ls -l /home/daniel/Documents/analyses/graham/rawReads

ls -l /home/daniel/Documents/analyses/graham/rawReads | grep -c "fastq.gz"  ## 324

grep -c "fastq.gz" whatsinthere.csv ## 323

python3 

import pandas as pd
import os

aa = pd.read_csv('whatsinthere.csv', names=("size","name"))
bb = pd.Series(os.listdir('/home/daniel/Documents/analyses/graham/rawReads'))

aa['name']

cc = aa['name'].apply(lambda x: x in bb.values)
dd = bb.apply(lambda x: x in aa['name'].values)

cc.all() ## all of our uploaded files are found in the raw reads directory
dd.all() ## but not all of the files in our raw reads directory is found in our uploads

## which one?
bb[~dd] ## s257,R2:

'lane1-s257-index-GAGGACTT-CCTAAGTCNNNN-PosI_S257_L001_R2_001.fastq.gz'

## files sizes otherwise okay?

os.chdir("/home/daniel/Documents/analyses/graham/rawReads")

fileSizesRaw = [ os.path.getsize(i) for i in os.listdir() ]

ee = aa['size'].apply(lambda x: x in fileSizesRaw) 

ee.all() ## one is not right

aa[~ee].iloc[0,1] ## s006, R1 

'lane1-s006-index-CTAGGTGA-CCTGTCAANNNN-Fr14_S6_L001_R1_001.fastq.gz'

aa[~ee]                             ## 4630704 in our uploaded files 
os.path.getsize(aa[~ee].iloc[0,1]) ## 16370601 on our raw read file

## so, way too small, upload interrupted I guess. 

## can I just upload these two failed files?

## remind myself - on the lab computer?

nanoComp

cd /media/vol1/daniel/graham/rawReads ## yup

## from there, try to start an upload:


ftp ftp-private.ncbi.nlm.nih.gov 

subftp

iodrac9octOckEpp

cd /uploads/danchurchthomas_gmail.com_Gq8kA048/grahamRawRead_try2

#put lane1-s006-index-CTAGGTGA-CCTGTCAANNNN-Fr14_S6_L001_R1_001.fastq.gz
#put lane1-s257-index-GAGGACTT-CCTAAGTCNNNN-PosI_S257_L001_R2_001.fastq.gz

ls -l lane1-s006-index-CTAGGTGA-CCTGTCAANNNN-Fr14_S6_L001_R1_001.fastq.gz ## 16370601
ls -l lane1-s257-index-GAGGACTT-CCTAAGTCNNNN-PosI_S257_L001_R2_001.fastq.gz ## 10387755

## okay, should be good. Reran above checks, now looks good.


########### datasheets ###########

## I think I remember three sheets were required. With examples from last time:

## on laptop
sulArnieSubmitReadDir="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sraSubmission/"

## on office comp:
sulArnieSubmitReadDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sraSubmission/"

## a sample sheet:
less ${sulArnieSubmitReadDir}carbon4dMicrobialSamples.tsv

## a metadata sheet:
less ${sulArnieSubmitReadDir}carbon4dMetadata.tsv

## and can't seem to find anything else. I guess just two.

## start with the sample sheet ("metagenome or environmental" biosample sheet)

## we have sample data, etc, from graham here:

danNotesD="/home/daniel/Documents/analyses/graham/grass-endophyte-community/danNotes/"

ls ${danNotesD}

less ${danNotesD}grahamSample.csv

## columns are explained here:
## https://submit.ncbi.nlm.nih.gov/biosample/template/?package-0=Metagenome.environmental.1.0&action=definition
## further information here:
## https://www.ncbi.nlm.nih.gov/biosample/docs/organism/
## for the organism, we will use "leaf metagenome"
## Taxonomy ID: 1297858 
## however, for control, we will use "synthetic metagenome"
## Taxonomy ID: 1235509
## on office comp, use a conda env already made for something else:

## graham's preprint is here, may be useful:
https://docs.google.com/document/d/0B8oJrvuky7NXTExCdDdBdUJmSHpqbXNLbk84WnlPc2E3QmNj/edit?usp=sharing&ouid=106808919302528987604&resourcekey=0-UDH87Wym_Nwk_PcjJE-ucg&rtpof=true&sd=true

conda activate spatialDirt

python3

import pandas as pd
import os, re
import natsort as ns
import numpy as np

## if we want to see the whole df:
#pd.set_option('display.max_rows', None)
## to reset
#pd.reset_option('display.max_rows')
## to see full cell contents:
#pd.set_option("max_colwidth", None)
#pd.set_option("max_colwidth", 20)
#pd.reset_option("max_colwidth")

## how do we get our sample names? 

## laptop:
rawReadFiles = pd.Series(os.listdir('/home/daniel/Documents/analyses/graham/rawReads'))
## office comp
#rawReadFiles = pd.Series(os.listdir("/media/vol/graham/rawReads"))
## looks like we can split this up using dashes, then underscores.
## but there are some read files with too many dashes. Just to note for later:
weirdNames=[
"lane1-s160-index-AAGCACTG-GTGATCCANNNN-Dc-X_S160_L001_R1_001.fastq.gz",
"lane1-s160-index-AAGCACTG-GTGATCCANNNN-Dc-X_S160_L001_R2_001.fastq.gz",
"lane1-s161-index-AAGCACTG-TTCGTACGNNNN-Dc-PosG_S161_L001_R1_001.fastq.gz",
"lane1-s161-index-AAGCACTG-TTCGTACGNNNN-Dc-PosG_S161_L001_R2_001.fastq.gz",
"lane1-s162-index-AAGCACTG-ATGACAGGNNNN-Dc-PosI_S162_L001_R1_001.fastq.gz",
"lane1-s162-index-AAGCACTG-ATGACAGGNNNN-Dc-PosI_S162_L001_R2_001.fastq.gz",
"lane1-s163-index-AAGCACTG-CGACCTAANNNN-Dc-Neg_S163_L001_R1_001.fastq.gz",
"lane1-s163-index-AAGCACTG-CGACCTAANNNN-Dc-Neg_S163_L001_R2_001.fastq.gz"
]
## so should be something like:
aa = rawReadFiles.str.replace("Dc-","Dc") ## get rid of the extra dashes in weird names
bb = aa.str.split("-", expand=True)[5]
cc = bb.str.split("_", expand=True)
dd = (cc[[0,1,3]]
       .copy()
       .rename({0:"fieldSample", 
                1:"labSample",
                3:"direction"}, axis=1)
     )
## might be good to add file names back in? for now, wait on this...
## sort it to make this pretty
fileNameDF = (dd.loc[ns.index_natsorted(dd["fieldSample"])]).reset_index(drop=True)
## theoretically, we should have two directions, R1/R2 per each unique field sample name
fileNameDF.reset_index().groupby(['fieldSample'])['direction'].nunique()
fileNameDF.reset_index().groupby(['fieldSample'])['direction'].nunique().to_list()
## all 2, good
## does each field sample have a unique labsample? Or are there multiple 
## labsamples per?
(fileNameDF[["fieldSample","labSample"]]
    .groupby("fieldSample")
    .nunique()["labSample"]
    .to_list()
) ## all 1, good
## these both look good. 
## so, now what? to get our unique field and lab names:
firstInstance = fileNameDF.reset_index().groupby(['fieldSample'])['index'].min().to_list()
sampleNamesUnique = fileNameDF.loc[firstInstance, ['fieldSample','labSample']]
sampleNamesUnique.reset_index(drop=True, inplace=True)
sampleNamesUnique = (sampleNamesUnique.loc[ns.index_natsorted(sampleNamesUnique["fieldSample"])]
                      .reset_index(drop=True))
## let's use the lab sample names as the official sample names, 
## and let's give the field names to the second column, "sample title"
## let's start our "sample sheet"
sampleSheet = sampleNamesUnique.copy().rename({'labSample':'sample_name','fieldSample':'sample_title'}, axis=1)
## as mentioned above, most of these are leaf "metagenomes" according to NCBI
sampleSheet["organism"] = "leaf metagenome"
## but the controls are going to be "synthetic metagenome"
controlPatt = re.compile("Neg|Pos|X")
controls = sampleSheet[sampleSheet['sample_title'].str.contains(controlPatt)].index.to_list()
sampleSheet.loc[controls, 'organism'] = "synthetic metagenome"
## host will be "Danthonia californica", "Festuca roemeri", and NA for controls
festucaSamples = sampleSheet[sampleSheet['sample_title'].str.contains("Fr")].index.to_list()
sampleSheet.loc[festucaSamples, 'host'] = "Festuca roemeri"
danthSamples = sampleSheet[sampleSheet['sample_title'].str.contains("Dc")].index.to_list()
sampleSheet.loc[danthSamples, 'host'] = "Danthonia californica"
## until graham tells me other wise, the "Dc-" controls will be considered to not have a host
controls = sampleSheet[sampleSheet['sample_title'].str.contains(controlPatt)].index.to_list()
sampleSheet.loc[controls, 'host'] = "not applicable"
## for other information, we need this sheet:
## on laptop
grahamSampleDataR = pd.read_csv("/home/daniel/Documents/analyses/graham/grass-endophyte-community/danNotes/grahamsSampleData.csv")
## on workComp
#grahamSampleDataR = pd.read_csv("/home/daniel/Documents/projects/grass-endophyte-community/danNotes/grahamsSampleData.csv")
## let's match up the sample names between these two:
grahamSampleDataR.rename({'Unnamed: 0':'sample_name'}, axis=1, inplace=True)
grahamSampleDataR['sample_name'] = ( pd.Series( ['S']* sampleSheet['sample_name'].shape[0] ) +
                                            grahamSampleDataR['sample_name'].str.split("g", expand=True)[0] )
## for isolation source, let's use the site info? not sure if there is a better place for this.
sampleSheet["isolation_source"] = (sampleSheet['sample_name']
                                     .apply(
                                        lambda x: grahamSampleDataR.set_index('sample_name').loc[x]['site']))
## we should probably add some ecological information to this, but do this later
## geo location name may be tricky. Where are these places?
## these should work for google maps:
pd.concat([grahamSampleDataR[['site','longitude','latitude']].groupby('site')['latitude'].min(),
             grahamSampleDataR[['site','longitude','latitude']].groupby('site')['longitude'].min() ],
             axis=1)
## gives us:
siteLookup = {
"French_flat" : "USA: Oregon, Cave Junction, French Flat",
"Hazel_Dell" : "USA: Oregon, Eugene, Hazel Dell",
"Horse_Rock" : "USA: Oregon, Crawfordsville, Horse Rock Ridge",
"Lower_Table" : "USA: Oregon, Medford, Lower Table Rock",
"Roxy_Ann" : "USA: Oregon, Medford, Roxy Ann Peak",
"Upper_Table" : "USA: Medford, Upper Table Rock",
"Upper_Weir" : "USA: Washington, Rainier, Upper Weir",
"Whetstone" : "USA: Oregon, Medford, Whetstone",
"Whidbey" : "USA: Washington, Coupeville, Whidbey Island",
"Control" : "USA: University of Oregon"
}
sampleSheet["geo_loc_name"] = (sampleSheet["isolation_source"].apply(lambda x: siteLookup[x]))
## lat lon has a specific formatting
## should look like: 38.98 N 77.11 W
## for us, this looks like:
sampleSheet['latitude'] = sampleSheet['sample_name'].apply( 
                   lambda x: grahamSampleDataR.set_index("sample_name")
                            .loc[x,'latitude'])
sampleSheet['longitude'] = sampleSheet['sample_name'].apply( 
                   lambda x: grahamSampleDataR.set_index("sample_name")
                            .loc[x,'longitude']).abs()
## we can add the coordinates for the controls, even though they are meaningless:
sampleSheet.loc[sampleSheet['isolation_source'] == "Control","longitude"] = 123.07
sampleSheet.loc[sampleSheet['isolation_source'] == "Control","latitude"] = 44.05
## and combine these in the format that they need...
## should look like: 38.98 N 77.11 W
sampleSheet["lat_lon"] = sampleSheet['latitude'].astype('string') + " N " + sampleSheet['longitude'].astype('string') + " W"
## drop the individual lat/lon fields:
sampleSheet.drop(['latitude', 'longitude'], axis='columns', inplace=True)
## last required field = collection date...where is this information?
## graham gives us the following info in his email from 11.9.2024:
#F. roemeri
#    French Flat: 2015/05/30
#    Roxy Ann Peak: 2015/06/01
#    Upper Table Rock: 2015/06/01
#    Hazel Dell: 2015/06/10
#    Horse Rock: 2015/06/24
#    Upper Weir:2015/06/13
#    Whidbey: 2015/06/20
#D. californica
#    French Flat: 2015/05/30
#    Whetstone: 2015/05/31
#    Lower Table Rock: 2015/05/31
#    Hazel Dell: 2015/06/05
#    Horse Rock: 2015/06/15
#    Whidbey: 2015/06/21
FrDates = {
 'French_flat':"2015-05-30",
  'Hazel_Dell':"2015-06-10",
  'Horse_Rock':"2015-06-24",
 'Lower_Table':None,
    'Roxy_Ann':"2015-06-01",
 'Upper_Table':"2015-06-01",
  'Upper_Weir':"2015-06-13",
   'Whetstone':None,
     'Whidbey':"2015-06-20",
     'Control':"missing: control sample"
}
DcDates = {
 'French_flat':"2015-05-30",
  'Hazel_Dell':"2015-06-05",
  'Horse_Rock':"2015-06-15",
 'Lower_Table':"2015-05-31",
    'Roxy_Ann':None,
 'Upper_Table':None,
  'Upper_Weir':None,
   'Whetstone':"2015-05-31",
     'Whidbey':"2015-06-21",
     'Control':"missing: control sample"
}
sampleSheet["collection_date"] = None
dcPatt = re.compile("Dc[0-9]")
sampleSheet.loc[sampleSheet['sample_title'].str.contains(dcPatt), "collection_date"] \
  = sampleSheet.loc[sampleSheet['sample_title'].str.contains(dcPatt), "isolation_source"].apply(lambda x: DcDates[x])
FrPatt = re.compile("Fr")
sampleSheet.loc[sampleSheet['sample_title'].str.contains(FrPatt), "collection_date"] \
  = sampleSheet.loc[sampleSheet['sample_title'].str.contains(FrPatt), "isolation_source"].apply(lambda x: FrDates[x])
sampleSheet.loc[sampleSheet['isolation_source'] == "Control", "collection_date"] = "missing: control sample"
## had to add the following columns, new since last time. Details here:
#https://github.com/EnvironmentOntology/envo/wiki/Using-ENVO-with-MIxS
sampleSheet['env_broad_scale'] = "temperate grassland biome [ENVO_01000193]"
sampleSheet['env_local_scale'] = "environment associated with a plant part or small plant [ENVO 01001057]"
sampleSheet['env_medium'] = "plant matter [ENVO:01001121]"
## for all controls, broad scale would be:
sampleSheet.loc[sampleSheet['isolation_source'] == "Control", "env_broad_scale"] = "laboratory environment [ENVO 01001405]"
## positive controls local and environmental medium:
sampleSheet.loc[sampleSheet['sample_title'].str.contains("Pos"), "env_local_scale"] = "culturing environment [ENVO 01000312]"
sampleSheet.loc[sampleSheet['sample_title']== ("DcX"), "env_local_scale"] = "culturing environment [ENVO 01000312]"
sampleSheet.loc[sampleSheet['sample_title'].str.contains("Pos"), "env_medium"] = "mock community culture [ENVO 01001059]"
sampleSheet.loc[sampleSheet['sample_title']== ("DcX"), "env_medium"] = "single strain cell culture [ENVO_01001060]"
## negative controls:
sampleSheet.loc[sampleSheet['sample_title'].str.contains("Neg"), "env_local_scale"] = "liquid water [ENVO_00002006]"
sampleSheet.loc[sampleSheet['sample_title'].str.contains("Neg"), "env_medium"] = "sterile water [ENVO_00005791]"

###### metadata sheet #########

################################################################
## our data sheet from fichtelgebirge project looks like this:
## on laptop
#sulArnieSubmitReadDir="/home/daniel/Documents/projects/fichtelgebirge_project/sulariArneSoils/fichtelgebirgeSoils/sraSubmission/"
## on office comp:
sulArnieSubmitReadDir="/home/daniel/Documents/projects/fichtelSoils/fichtelgebirgeSoils/sraSubmission/"
less ${sulArnieSubmitReadDir}carbon4dMetadata.tsv
################################################################


metaCol = ['sample_name','library_ID','title','library_strategy',
            'library_source','library_selection','library_layout','platform',
             'instrument_model','design_description','filetype','filename',
                   'filename2','filename3','filename4','assembly','fasta_file']

## sample_name should be identical:
grahamMetadata = pd.DataFrame(index=sampleSheet["sample_name"].copy())
## in the past, I made it so the 
## library id is the part of the file name that joins both the R1 and R2 reads, 
## and is unique for each row/sample. 
## to get this:
aa = rawReadFiles.str.split("-", expand=True).iloc[:,1:5]
libs = aa[1] + "-" + aa[2] + "-" + aa[3] + "-" + aa[4]
smallSzeroPatt = re.compile("s(0){0,2}")
libs.index = aa[1].str.replace(smallSzeroPatt,'S', regex=True)
libs.drop_duplicates(inplace=True)
grahamMetadata['library_ID'] = libs
## title - last time we use a quick description of the sample origin
def getTitle(x):
    sash = sampleSheet.set_index('sample_name')
    host=sash.loc[x,'host']
    site=sash.loc[x,'geo_loc_name']
    return( f"ITS metabarcoding of {host} fungal endophytes from {site}")

metaTitles = grahamMetadata.reset_index()['sample_name'].apply(getTitle)
metaTitles.index = grahamMetadata.index
grahamMetadata['title'] = metaTitles
## fix controls
sampleSheet.loc[sampleSheet['isolation_source'] == "Control"]
grahamMetadata.loc[['S163','S255'],"title"] = "ITS metabarcoding of lab negative control"
grahamMetadata.loc[['S161', 'S256'],"title"] = "ITS metabarcoding of mock community control, genomic DNA"
grahamMetadata.loc[['S162', 'S257'],"title"] = "ITS metabarcoding of mock community control, ITS amplicons"
grahamMetadata.loc['S160',"title"] = "ITS metabarcoding of single species control, genomic DNA"
## the next ones are easy:
grahamMetadata['library_selection'] = 'PCR'
grahamMetadata['library_layout'] = 'paired'
grahamMetadata['platform'] = 'Illumina'
grahamMetadata['instrument_model'] = 'Illumina MiSeq' 
## design_description: something like:
grahamMetadata['design_description'] = \
"DNA extracted from plant leaves using column extraction kits, \
amplified with 16S ITS1F/ITS2 primers, \
PCR product diluted to 7.013 ng/µl for sequencing."
## controls a little different
(grahamMetadata
  .loc[sampleSheet.set_index('sample_name')['isolation_source'] == "Control","design_description"]) = \
"DNA extracted from individual fungal cultures or pure water controls \
 using column extraction kits, DNA extracts combined \
in equimolar amounts and amplified with 16S ITS1F/ITS2 primers, \
PCR product diluted to 7.013 ng/µl for sequencing."
## filetype:
grahamMetadata['filetype'] = "fastq"
## filename and filename2:
## each library_ID should have two files associated with it:
def getFilesFromLibID(x,R):
    if R == 1: libIDRpatt = re.compile(x+(".*R1.*"))
    if R == 2: libIDRpatt = re.compile(x+(".*R2.*"))
    r = rawReadFiles[ rawReadFiles.str.contains(libIDRpatt) ].iloc[0]
    return(r)

grahamMetadata['filename'] = grahamMetadata['library_ID'].apply(getFilesFromLibID,R=1)
grahamMetadata['filename2'] = grahamMetadata['library_ID'].apply(getFilesFromLibID,R=2)
## forgot these:
grahamMetadata['library_strategy'] = "AMPLICON"
grahamMetadata['library_source'] = "METAGENOMIC"
## think we need a biosample accession column:
grahamMetadata['biosample_accession'] = None

####### prep forms for submission #####

## this section has be run after all of the above
## tried submission, and we need some last minute changes
## looks like we need to switch our sample_name and sample_title 
## because we used a similar S__ scheme in a previous 
## submission, apparently this is not permitted. 
## can we switch the lab and field ID, as sample_title/sample_name?

sampleSheet.rename({'sample_name':'sample_title', 'sample_title':'sample_name'}, axis=1, inplace=True)
## also looks like we need to create another column to differentiate samples beyond
## just the sample name/title   
sampleSheet['UID'] = sampleSheet['isolation_source'] + sampleSheet['sample_name']
## we need to adjust our metadata samplenames also:
grahamMetadata.reset_index(inplace=True)
grahamMetadata.rename({'sample_name':'sample_title'}, inplace=True, axis=1)
## check order:
(sampleSheet['sample_title'] == grahamMetadata['sample_title']).all()
## looks good:
grahamMetadata['sample_name'] = sampleSheet['sample_name']

## reorder to make sure they fit the formatting of template sheets:
grahamMetadata = grahamMetadata[['sample_name','biosample_accession','library_ID', 'title', 'library_strategy','library_source',
                'library_selection', 'library_layout', 'platform', 
                'instrument_model', 'design_description', 'filetype',
                'filename', 'filename2']]

sampleSheet = sampleSheet[['sample_name', 'sample_title', 'organism', 
                'host', 'isolation_source', 'collection_date', 'geo_loc_name', 'lat_lon',
                'env_broad_scale', 'env_local_scale', 'env_medium', 'UID']]

sampleSheet.to_csv('sampleSheet.tsv',index=False, sep="\t")

grahamMetadata.to_csv('grahamMetadata.tsv',index=False, sep="\t")


## oh god, that sucked. 
