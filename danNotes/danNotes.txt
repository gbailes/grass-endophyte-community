
## let's have a look at Graham's pipeline. 
## for the record, I've added SSH key access for this repo.
## In case I need to do this again (github just stopped 
## password access), you have to change the remote url to a
## format that ssh can work with:

git remote set-url origin git@github.com:gbailes/grass-endophyte-community.git

## Grahams notebook for making his biom can be viewed here:
https://nbviewer.jupyter.org/github/gbailes/grass-endophyte-community/blob/master/alt_biom.ipynb

## as far as grahams paper, I think I was supposed to:

 """
 1. Check trend surfaces of community matrix varation - can we describe the 
 variation in graham's community matrices with a north/sound trend in diversity 
 and dissimilarity? I think I recommended this in place of the original dbmem analysis, 
 if it turns out to be informative.  

 2. Attempt to quantify the apparent S->N increase in diversity. Means repeating 
 Grahams diversity metrics on the pre-variance-stabilization (pre-deseq), then 
 check to see if there is enough statistical power to build a point pattern model from these.  

 3. Figure out the story of the french flat site, the only place where the two grass 
 host species share a lot of endophyte species. What are the shared species? 
 Are they unique to the site? Are they interesting, as in maybe one of the species 
 that have been shown to give panicum grass salt and drought tolerance?
 """

 this all comes from a 1.5 year-old email (Feb 2, 2020), "review of goals for Dan with Graham Paper"
 #3 was the doozy. 

## what is the general strategy here...

## the first two goals (#1 and #2) will be in R. Not sure I agree with the 
## approaches I suggested originally, with point patterns, etc. I think a 
## simple.

## the second will be sort of a treasure hunt. I think that one will take 
## the most time. 

## let's go through them in order...

##### remembering trend surfaces ########

## let's jog our memories about trend surfaces
## with the tutorial from Borcard, on the orobatid mite data:

## ugh, there is a new version of R out, they are on 4.x.x now. The beast grows
## in its power... let's stay in the 3.x.x world for the moment. 

## get the "s.value" function out the supp materials for borcard:

R 

library('ade4')
library('vegan')

source('/home/daniel/Documents/Books/Stats/numericalEcologyR/NEwR-2ed_code_data/NEwR-2ed_code_data/NEwR2-Functions/sr.value.R')

xygrid <- expand.grid(1:10, 1:10)

plot(xygrid)

xygrid.c <- scale(xygrid, scale=FALSE)

X <- xygrid.c[,1]
Y <- xygrid.c[,2]

par(mfrow=c(3,2))

par(mfrow=c(1,1))

sr.value(xygrid, (X))

sr.value(xygrid, (Y))
sr.value(xygrid, (X+Y))
sr.value(xygrid, (X^2+Y))
sr.value(xygrid, (X^2+Y^2))
sr.value(xygrid, (X^3))

sr.value(xygrid, (X^2))


## cool. Lots of patterns are possible. 

## onward with the tutorial...

data('mite')
data('mite.env')
data('mite.xy')

mite.h <- decostand(mite, "hellinger")

mite.xy.c <- scale(mite.xy, center=TRUE, scale=FALSE)

mite.poly <- poly(as.matrix(mite.xy.c), degree=3, raw=TRUE)

## raw=TRUE retains all the requested polynomials. if FALSE,
## apparently it retains only the truly orthogonal ones.

## so we can try all kinds of polynomial surfaces out as 
## possible models. 

## modeling the N-S bioiversity trend (#2 above) should be
## pretty simple, then. Just use richness, simpsons, whatever, 
## and model it on a linear S-N increasing plane. Univariate,
## so very straightforward

## butu for modeling patterns in community variation, 
## how can we use these polynomial surfaces to model a multivariate
## community? 

## borcard says with RDAs:

colnames(mite.poly) = c("X", "X2", "X3", "Y", "XY", "X2Y", "Y2", "XY2", "Y3")  

mite.trend.rda <- rda(mite.h ~ ., data=as.data.frame(mite.poly))

R2adj.poly <- RsquareAdj(mite.trend.rda)

## from here Borcard backs up, and uses the orthogonal polynomials.
## I don't really understand how orthogonal polys are made, its magic
## to me. But here we go:

mite.poly.ortho <- poly(as.matrix(mite.xy), degree=3)
colnames(mite.poly.ortho) = c("X", "X2", "X3", "Y", "XY", "X2Y", "Y2", "XY2", "Y3")  
mite.trend.rda.ortho <- rda(mite.h ~ ., data=as.data.frame(mite.poly.ortho))


mite.trend.rda.ortho

mite.trend.rda

R2adj.poly <- RsquareAdj(mite.trend.rda.ortho)$adj.r.squared

## for some reason, these are giving me exactly the same results.
## not going to take the time to figure out why...probably typo...

## forward selection 
## forward.sel doesn't seem to exist anymore.
## I think the ortholog here is ordistep or ordiR2step.

mod0 <- rda(mite.h ~ 1, data=as.data.frame(mite.poly.ortho))
mod1 <- rda(mite.h ~ ., data=as.data.frame(mite.poly.ortho))

mite.trend.fwd <- ordiR2step(mod0, mod1)

## as per example, six terms retained, I think?
## use these

mite.trend.rda2 <- rda(mite.h ~ . 

as.data.frame(mite.poly)

as.data.frame(mite.poly)[

mite.trend.fwd[,2]


str(mite.trend.fwd)
str(mod1)

## great, so ordiR2step drops the uninformative components for you. 

## so how do we know from this which were the important polynomials?


str(mite.trend.fwd)

str(mite.trend.fwd$terminfo)

str(mite.trend.fwd$terminfo$terms)

## um, is this the only way to extract this?
attributes(mite.trend.fwd$terminfo$ordered)$names

## probably not. seems stupid. But for the moment it 
## will work:

sigTerms <- attributes(mite.trend.fwd$terminfo$ordered)$names

## check significance:

anova.cca(mite.trend.fwd)

anova.cca(mite.trend.fwd, by="axis")
## RDA1 is by far the most important. But I don't really understand these 
## new axes. Are they in terms the spatial predictors or the community 
## matrix "response" variables? 
## I assume they are in terms of the spatial predictors, the polynomials,
## because there are six of them, matching the number of polynomials.

## can we look at them?

mite.trend.fit <- scores.cca(mite.trend.fwd, choices = c(1,2,3), display="lc", scaling=1)

## that don't work anymore...how can we get these out of the model....
## looks like they updated their methods to fit in with everybody else:

mite.trend.fit <- scores(mite.trend.fwd, choices = c(1,2,3), display="lc", scaling=1)

par(mfrow=c(1,3))
sr.value(mite.xy,mite.trend.fit[,1])
sr.value(mite.xy,mite.trend.fit[,2])
sr.value(mite.xy,mite.trend.fit[,3])

## great. that all makes some kind of sense. How do we apply to our 
## situation?
## we will have a multivariate community matrix (Graham's endophyte matrix), and a 
## a single variate diversity index. 
## we want to check them both for trend surfaces.

## with the biodiversity, we suspect a simple, first order linear equation 
## with a northward increase in biodiversity

## for the multivariate community data, I think we just want to look for interesting
## spatial patterns. So maybe, rerun the above analysis on grahams community data.

## either way, we need his biome table. From this we can work backword get diversity 
## metrics. 

## where is this? graham has a biome file ready...

## so next step will be to remember how to manipulate 

## phyloseq, etc. Oh jeezus. 
  
########## get graham's biom table into memory ##########

## the way to do this used to be in R, in phyloseq. Are folks still doing this?

## a quick lit search shows lots of citations for phyloseq...

## let's assume phyloseq is still cool. 

library(phyloseq)

biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')

head(sample_data(biom97))

colnames(sample_data(biom97))

tax_table(biom97)[0:3,]

head(tax_table(biom97))


otu_table(biom97)[0:3,]

dim(otu_table(biom97))

## great. This looks pre-deseq transformation, which is also good. 

## so what's task number one?

## the most obvious task is the bioiversity regression. 

## we need to rarefy to an even number of sequences, and 
## get good, UTM-style xy coordinates for these sites. 

## so, for the moment, get the estimated UTMs for each site. How?

## let's go pyproj. lat/long epsg is 4326

## let's export the sample data to something python can use:

class(sample_data(biom97))

str(sample_data(biom97))

write.csv(sample_data(biom97), file='grahamSample.csv')


##### over to python, to wrestle out the UTMs for these sites ######

import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
from scipy import stats
from sklearn.linear_model import LinearRegression


grahamSample = pd.read_csv('grahamSample.csv', index_col=0)

grahamSample.columns

grahamSample.site.to_list()

grahamSample.site.unique().shape

## pare down to one row for each site:

grahamSample.head()

#aa = grahamSample[['samplename','site','latitude','longitude']]
## don't keep unique ids

grahamSample.head()

## seems like we have two locations for Hazel Dell:
grs = grahamSample.groupby('site')
grs.get_group('Hazel_Dell').latitude.unique()
grs.get_group('Hazel_Dell').longitude.unique()
## so let's change all Hazel dell latitudes to be 
## the same, keeping the more southerly (lower) value:

grahamSample.latitude.dtype

sum(grahamSample.latitude == 44.03) ## 12 samples
sum(grahamSample.latitude == 44.02) ## also 12 samples

## assign all to a single value:
hd4403 = grahamSample.latitude == 44.03
grahamSample.latitude[hd4403] = 44.02

sum(grahamSample.latitude == 44.03)
sum(grahamSample.latitude == 44.02)
## looks good

aa = grahamSample[['site','latitude','longitude']]
bb = aa.reset_index(drop=True)
## just to make sure the lat longs are indeed the same for each site:
#cc = bb.groupby('site')
#cc.get_group('Control')
#cc.get_group('French_flat') ## etc
#cc.groups.keys()
## anyway, 
dd = bb.drop_duplicates()
dd.set_index('site', inplace=True)
ee = dd.drop('Control')
## great. Now how to convert these? go to geopanda? 
grahamSites = (gpd.GeoDataFrame(
    ee, geometry=gpd.points_from_xy(ee.longitude, ee.latitude))
    .drop(['latitude','longitude'], axis=1)
)



grahamSites.crs

grahamSites.set_crs('EPSG:4326', inplace=True)

grahamSites.crs

## reproject to UTM zone 10, which is EPSG:26710
grahamSites.to_crs('EPSG:26710', inplace=True)

grahamSites.crs

#grahamSites.to_csv('grahamSiteCoords.csv')
## not a perfect export, have to modify with vim to get a good csv
## but works

## and maybe save us some work later:
#grahamSites.to_pickle('grahamSiteCoords.p')

## check it on some maps. Here are some publicly available geojsons:

## in shell ##
wget https://raw.githubusercontent.com/johan/world.geo.json/master/countries/USA/WA.geo.json
wget https://raw.githubusercontent.com/johan/world.geo.json/master/countries/USA/OR.geo.json
## in shell ##

## so how does it look?:


oregon = gpd.read_file('OR.geo.json')
oregon.to_crs('EPSG:26710', inplace=True)
washington = gpd.read_file('WA.geo.json')
washington.to_crs('EPSG:26710', inplace=True)

plt.close('all')
fig, ax = plt.subplots()
oregon.plot(ax=ax)
washington.plot(ax=ax)
grahamSites.plot(ax=ax, color='red')

## great. What else do we need here? Not much, methinks...back to R?

###### task #2 species richness as a function of latitude ########

## now, we want to give species richness, simpsons or something for each 
## site.

## we need to rarify to some sort of common of denominator. How are our 
## read depths?

## drop the controls

aa = subset_samples(biom97, site != 'Control')

controls = subset_samples(biom97, site == 'Control')


sample_data(biom97)$site

options(scipen=999)

par(mfrow=c(1,3))
barplot(sort(sample_sums(biom97), decreasing=TRUE))
barplot(sort(sample_sums(aa), decreasing=TRUE))
barplot(sort(sample_sums(controls), decreasing=TRUE))


par(mfrow=c(1,1))
barplot(sort(sample_sums(controls), decreasing=TRUE))

sample_sums(biom97)

sort(sample_sums(biom97))


data[order(data$num,decreasing = TRUE),]

bar_plot(sample_sums(biom97))

aalost <- aalost[order(aalost[,'164wood'], decreasing = TRUE),]

data(enterotype)
TopNOTUs = names(sort(taxa_sums(enterotype), TRUE)[1:10])
ent10 = prune_taxa(TopNOTUs, enterotype)
plot_bar(ent10, "SeqTech", fill = "Enterotype", facet_grid = ~Genus)


biom97rar = rarefy_even_depth(biom97)
## wow, we lost a lot of diversity that way,
## but to be expected, I guess. 

aarar = rarefy_even_depth(aa)
## that helps a lot, there was a very small sample in the controls,
## probably the single species control
## we still rarefy down to 7101, which is a pretty low common denominator...
## how much do we lose if we go to 20,000?

min(sample_sums(aa)) 

sort(sample_sums(aa))[0:20]

sum(sample_sums(aa) < 20000) ## six samples. 
## that's not too many out of 155 samples...
## are they important? where are they?

sortedSamples <- sort(sample_sums(aa))

size = 20000
smallerThanSize <- sortedSamples[sortedSamples < size]


sample_data(biom97)[names(smallerThanSize),'site']
## shit. These are mostly french flat. We will lose
## 5 of 12 FF sites if we drop our 5 lowest sample 
## depths. Sucks. 


bb = subset_samples(biom97, site != 'Control')

sample_sums(aarar)

## to answer if I can get rid of this, have to ask a stupid question:
##  why are there so many samples? 
## 9 sites, 2 hosts, some controls... I would have predicted 
## = ~25 samples.
## but graham has 155. 
## how do these part out by site?

biom97 

head(sample_data(biom97))

head(sample_data(biom97))

## I head data exploration in R. look at this with pandas

write.csv(sample_data(biom97), file="grahamsSampleData.csv")

##### back to python #####

## imports above

## I'm betting that graham kept geographic info for each 
## tuft that he sampled, or the origin of each transect,
## and the location of that grass on its transect. 

## so for each of the nine sites, there is probably a sub-map
## that could be made. 

## don't want to make it, don't think we need it. 
## the main question is, can we pool these plants somehow
## to make the sites comparable for diversity metrics?

sampleData = pd.read_csv("grahamsSampleData.csv", index_col=0)

sampleData.columns

aa = sampleData[['host', 'site']]

## there are two hosts, we are going to need to work with each 
## host individually. Not sure, may also be useful to pool them
## at some point. But can't see the point of that for the moment.

## I think we'll want to start with one host, and then repeat 
## all analyses for for the other, copy-n-paste. 

sampleFroem = sampleData[sampleData.host == "F. roemeri"] 
## 84 plants F.roemerii sampled

sampleDanth = sampleData[sampleData.host == 'D. californica'] 
## 71 of Danthonia

## are these relatively evenly distributed among the sites?

plt.close('all')
fig, ax = plt.subplots(1,2)
sampleDanth.site.hist(ax = ax[0])
sampleFroem.site.hist(ax = ax[1])

## yep. Missing one from horse rock,
## but otherwise all have 12 plants each

sampleFroem.site


sampleDanth.site.unique().shape



sampleDanth.site.unique()

dir(sampleFroem.site.unique())

aa = pd.Series(sampleFroem.site.unique())

bb = pd.Series(sampleDanth.site.unique())

aa[aa.isin(bb)]

bb[~bb.isin(aa)]

## great, so we can afford to lose a few 
## low-abundance samples. problem is,
## listed above, these are almost all 
## from french flat
## another option is to go to 10000 reads,
## which seems like a respectable number

## so let's do several levels, rarefy to 20000 reads,
## and also to 10,000 and 7,000. Then we can compare and
## see how much information is lost by going down to 7000.


#### and back in R. #####

## does phyloseq have a built-in adjustment 
## for this?

biom97_noCon = subset_samples(biom97, site != 'Control')
controls = subset_samples(biom97, site == 'Control')


sample_data(biom97)$site

options(scipen=999)

sampsize=7000
biom97_rar.i = rarefy_even_depth(biom97_noCon, sample.size = sampsize, rngseed = 1)

## 8220 OTUs lost. 

sample_sums(biom97_rar.i)

sample_sums(biom97_noCon)

biom97_rar.i
biom97_noCon

biom97_rar.i

## so now, get species richness for each:

plot_richness(biom97_rar.i)

plot_richness(biom97_rar.i, measures='Observed')

## great. but as usual, phyloseq is doing more than I 
## want it to. Can we just get the richnness values without 
## the fancy graphics?

## I think this is what I need:
?estimate_richness

## vegan has some options, too
?estimateR
?vegan::diversity

sample_data(biom97_rar.i)

dim(sample_data(biom97_rar.i))

aa <- estimate_richness(biom97_rar.i, measures=c('Simpson', 'Observed'))

?estimate_richness

head(aa)

## we'll need these:

write.csv(aa, file='diversity7000.csv')
write.csv(sample_data(biom97_rar.i), file='biom97_rar7000.csv')

## great. but we also need site, x, and y on this. I hate doing this 
## shit in R. Anyway, for the moment I think we got what we need out of 
## of phyloseq?  

## pingpong back to python

## okay, so we want to make a geopanda with: 
## unique sample name, host type, site, UTMs
## for each sample...

aa = pd.read_csv('diversity7000.csv')
aa.columns=['sampleName','Observed','Simpson']
aa['sampleName'] = aa.sampleName.str.replace('X', '')
aa.set_index('sampleName', inplace=True)
bb = pd.read_csv('biom97_rar7000.csv', index_col=0)
bb = bb[['host','site', 'samplename']]
cc = pd.read_pickle('grahamSiteCoords.p')
dd = bb.merge(aa, left_index=True, right_index=True)
## make sure to merge using the GPD so it stays GPD
grahamDivGPD = cc.merge(dd, right_on='site', left_index=True)
#grahamDivGPD.to_pickle('grahamDivGPD.p')


## now what? We want to check a general north/south trend in 
## biodiversity...

## can we get our y-coords out somehow?

grahamDivGPD['geometry'].y

## so can we regress this against the richness and simpsons?:

stats.linregress(
grahamDivGPD['geometry'].y,
grahamDivGPD['Observed'],
)

## yeah, that is significant, r=.26, so r2 ~ 0.067?

## tomorrow plot this. 

## here is tomorrow. 

fig, ax = plt.subplots()
plt.scatter(grahamDivGPD['geometry'].y, grahamDivGPD['Observed'])

## combine these into points with error

grahamDivGPD

aa = grahamDivGPD.groupby('site').mean()
bb = grahamDivGPD.groupby('site').std()
cc = aa.merge(bb, left_index=True, right_index=True)
cc.columns = ['richness', 'simpson', 'richnessSD', 'simpsonSD']
siteRichness = grahamSites.merge(cc, left_index=True, right_index=True)

plt.close('all')
fig, ax = plt.subplots()
ax.errorbar(x=siteRichness['geometry'].y, 
            y=siteRichness['richness'],
            yerr=siteRichness['richnessSD'],
            fmt='o',
)
## ols line
X = siteRichness['geometry'].y.to_numpy().reshape(-1,1)
Y = siteRichness['richness'].to_numpy().reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), 
        c='k', linewidth=2, #linestyle= "dotted",
        label='OLS linear regression model'
       )
## site labels:
labs = siteRichness.index.to_series()
xy = zip(siteRichness['geometry'].y, siteRichness['richness'])
for i,j in enumerate(xy):
    ax.annotate(labs[i], xy=j)

## so to sum up:

import os
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
from scipy import stats
from sklearn.linear_model import LinearRegression

## we need a siteRichness gpd for each level...

def siteRichnessGPD(gpdDiv):
    ## convert diversity results
    aa = pd.read_csv(gpdDiv)
    aa.columns=['sampleName','Observed','Simpson']
    aa['sampleName'] = aa.sampleName.str.replace('X', '')
    aa.set_index('sampleName', inplace=True)
    ## convert sample data, just to get the site name and host
    bb = pd.read_csv('grahamSample.csv', index_col=0)
    bb = bb[['host','site', 'samplename']]
    ## bring our site geo info again:
    grahamSites = pd.read_pickle('grahamSiteCoords.p')
    ## attach name and host onto the diversity data
    dd = bb.merge(aa, left_index=True, right_index=True)
    ## make a gpdf
    grahamDivGPD = grahamSites.merge(dd, right_on='site', left_index=True)
    del(aa,bb,dd)
    aa = grahamDivGPD.groupby('site').mean()
    bb = grahamDivGPD.groupby('site').std()
    cc = aa.merge(bb, left_index=True, right_index=True)
    cc.columns = ['richness', 'simpson', 'richnessSD', 'simpsonSD']
    siteRichness = grahamSites.merge(cc, left_index=True, right_index=True)
    return(siteRichness)

def plotDiv(gpdRich, tit='', ax=None):
    if ax is not None: ax=ax
    if ax is None: fig, ax = plt.subplots()
    ax.errorbar(x=gpdRich['geometry'].y, 
                y=gpdRich['richness'],
                yerr=gpdRich['richnessSD'],
                fmt='o',)
    ## ols line
    X = gpdRich['geometry'].y.to_numpy().reshape(-1,1)
    Y = gpdRich['richness'].to_numpy().reshape(-1,1)
    ax.plot( X, LinearRegression().fit(X, Y).predict(X), 
            c='k', linewidth=2, #linestyle= "dotted",
            label='OLS linear regression model'
           )
    ## site labels:
    labs = gpdRich.index.to_series()
    xy = zip(gpdRich['geometry'].y, gpdRich['richness'])
    for i,j in enumerate(xy):
        ax.annotate(labs[i], xy=j)
    ## include linear regression results
    corStats = stats.linregress( gpdRich['geometry'].y, gpdRich['richness'])
    corString = (f'Pearson\'s r = {round(corStats[2],3)}, p = {round(corStats[3],3)}')
    top = ax.get_ylim()[1]
    right = ax.get_xlim()[1]
    ax.text(right, top, corString,
            horizontalalignment='right',
            verticalalignment='top')
    ax.set_title(tit)


## using these two functions, plus the above import block, we should
## be able to streamline this process:

gpdDiv_filename = 'diversity7000.csv'
gpdRich = siteRichnessGPD(gpdDiv_filename)
plotDiv(gpdRich, tit=gpdDiv_filename)

fig, axes = plt.subplots(1,2)
axes = np.ravel(axes)
plotDiv(gpdRich, tit=gpdDiv_filename, ax=axes[1])

## great. Let's go back to R and make a pipeline for creating richness data 
## from each level of rarefaction:


##### in R #########

## need the following:

library(phyloseq)
## read in graham's data:
biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')
## get rid of controls
biom97_noCon = subset_samples(biom97, site != 'Control')


## split by host, with a few different read depths:
sampsizes = c(7000, 18000, 30000, 40000)
hosts = c("F. roemeri", "D. californica")

for (j in hosts){
    biom97.i = subset_samples(biom97_noCon, host == j)
    for (i in sampsizes){
        filename = paste(j,"diversity",i,".csv", sep='')
        biom97_rar.i = rarefy_even_depth(biom97.i, sample.size = i, rngseed = 1)
        aa <- estimate_richness(biom97_rar.i, measures=c('Simpson', 'Observed'))
        write.csv(aa, file=filename)
    }
}


#### now try these out in our code above, in python ####

import os

os.listdir()

## using the above functions and import block:

gpdDiv_filenames = ['diversity30000.csv','diversity20000.csv', 'diversity10000.csv', 'diversity7000.csv']

fig, axes = plt.subplots(2,1)

axes = np.ravel(axes)

gpdDiv_filenames = ['diversity7000.csv','diversity20000.csv']

for i,j in enumerate(gpdDiv_filenames):
    gpdRich = siteRichnessGPD(j)
    plotDiv(gpdRich, tit=j)

gpdDiv_filenames = ['diversity7000.csv','diversity20000.csv']

for i,j in enumerate(gpdDiv_filenames):
    gpdRich = siteRichnessGPD(j)
    fig, axes = plt.subplots(2,1, figsize=(8,20))
    plotDiv(gpdRich, tit=j, ax=axes[i])

## yeah, not much difference. 

## let's start a notebook, to keep a sensible record of all this.

## okay, I've lost track of where I'm going with this
## also, we forgot to split up by host. 
## and it looks like we need to keep the latitudes 
## as hosts were sampled from two different sites, 
## with different latitudes

## how can we straighten all this out....

## I think we need to debug the pipeline as I have it to this point
 
## and then redo as above, split by host, etc.

## honestly, I don't think that the slight difference in 
## latitude of the HD sites matters right now, so leave it for
## simplicity's sake.

## so task number one is split the above analysis by host.

## as it is set up right now, simple to do the all-host 
## div-by-lat check:

gpdDiv_filenames = ['diversity7000.csv','diversity20000.csv']

fig, axes = plt.subplots(2,1, figsize=(8,16))

for i,j in enumerate(gpdDiv_filenames):
    gpdRich = siteRichnessGPD(j)    
    plotDiv(gpdRich, tit=j, ax=axes[i])

## but we have to the split before this, the species richness
## calculations are done in R. So back to the beast.  

plt.close('all')

gpdDiv_filenames = [ i for i in os.listdir() if "diversity" in i ]

for i,j in enumerate(gpdDiv_filenames):
    gpdRich = siteRichnessGPD(j)    
    plotDiv(gpdRich, tit=j)

## in general, it looks like Danthonia has a strong N/S diversity
## gradient, but not Festuca. 

## so, what do we do with this? report it in the notebook...

## wait for feedback from Graham and Bitty on that...

## and move on to another task

############## Task #1 trend surfaces for community matrix ######

## let's apply the above tutorial to grahams data, now that we 
## have some familiarity with graham's setup


library('ade4')
library('vegan')
library('phyloseq')

## custom plot function. It is weird that the R plotter has such a 
## hard time with this. It's just a scatter plot with variable radii.

source('/home/daniel/Documents/Books/Stats/numericalEcologyR/NEwR-2ed_code_data/NEwR-2ed_code_data/NEwR2-Functions/sr.value.R')
i## example data 

data('mite')
data('mite.env')
data('mite.xy')


biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')
## get rid of controls
biom97_noCon = subset_samples(biom97, site != 'Control')

danthBiom = subset_samples(biom97_noCon, host == 'D. californica' )
festuBiom = subset_samples(biom97_noCon, host == 'F. roemeri')

danthOTU = otu_table(danthBiom)
festuOTU = otu_table(festuBiom)

## run it through for Danthonia


## try it all with danthonia
## our community matrix is 
## the summed-by-site matrix made below in python

danSiteOTUsums <- read.csv('danSiteOTUsums.csv', row.names='site')

head(danSiteOTUsums, 1)

danSiteOTUsums[1:5,1:5]

rowSums(danSiteOTUsums)

sum(colSums(danSiteOTUsums) == 0) ## 1395 empty columns. Is this a problem?

nonzeroCols <- colSums(danSiteOTUsums) != 0

aa <- danSiteOTUsums[,nonzeroCols]

dim(danSiteOTUsums)
dim(aa)

## does this change the hellinger transformation results?:

danthOTU.h <- decostand(danSiteOTUsums, "hellinger")

danthOTUnonzero.h <- decostand(aa, "hellinger")


rowSums(danthOTU.h)
rowSums(danthOTUnonzero.h)

## zero columns don't affect the numbers resulting from 
## the transformation  
## let's keep going with both, see if there are any down
## stream effects of zero columns. Don't think there should
## be problems, but worth checking.

## we need graham's danthonia sampling scheme as an xy 

grahamSiteCoords = read.csv('grahamSiteCoords.csv', row.names='site')

grahamSiteCoords

dim(danSiteOTUsums)

danSiteOTUsums[1:5,1:5]

danSiteCoords <- grahamSiteCoords[rownames(danSiteOTUsums),]

head(mite.xy)

head(danSiteCoords)

danSiteCoords.c <- scale(danSiteCoords, center=TRUE, scale=FALSE)

danSiteCoords.c

danth.poly <- poly(as.matrix(danSiteCoords.c), degree=3, raw=TRUE)

danth.poly

head(mite.poly)

head(mite.h)

dim(as.data.frame(mite.poly))

dim(mite.h)

as.data.frame(mite.poly)

danthOTU.h

danthOTUnonzero.h

dim(danthOTU.h)

danth.trend.rda <- rda(danthOTU.h ~ ., data=as.data.frame(danth.poly))

R2adj.poly <- RsquareAdj(danth.trend.rda)


R2adj.poly ## r2 = 100%, that's fishy. Although, makes sense, with a million polynomials

## can we pare down the model...
## still following borcard:

## x^3, y^3
danth.poly.ortho3 <- poly(as.matrix(danSiteCoords), degree=3)
danth.poly.ortho3
danth.poly.ortho3.DF <- as.data.frame(danth.poly.ortho3)
exDeg <- colnames(danth.poly.ortho3.DF)
colnames(danth.poly.ortho3.DF) <- c("X", "X2", "X3", "Y", "XY", "X2Y", "Y2", "XY2", "Y3")  
## x^2, y^2
danth.poly.ortho2 <- poly(as.matrix(danSiteCoords), degree=2)
danth.poly.ortho2
danth.poly.ortho2.DF <- as.data.frame(danth.poly.ortho2)
danth.poly.ortho2.DF
colnames(danth.poly.ortho2.DF) <- c("X", "X2", "Y", "XY", "Y2")  
## x^1, y^1
danth.poly.ortho1 <- poly(as.matrix(danSiteCoords), degree=1)
danth.poly.ortho1
danth.poly.ortho1.DF <- as.data.frame(danth.poly.ortho1)
danth.poly.ortho1.DF
colnames(danth.poly.ortho1.DF) <- c("X", "Y")  

danth.poly.ortho2.DF

danth.poly.ortho


danth.trend.rda.ortho2 <- rda(danthOTU.h ~ ., data=as.data.frame(danth.poly.ortho2.DF))

## still 100 explained. but large differences among axes...

R2adj.poly <- RsquareAdj(danth.trend.rda.ortho) ## pointless, but in the example

## do the model selection as above
mod0 <- rda(danthOTU.h ~ 1, data=danth.poly.ortho2.DF)
mod1 <- rda(danthOTU.h ~ ., data=danth.poly.ortho2.DF)

danth.trend.fwd <- ordiR2step(mod0, mod1)

## huh, that doesn't work. 
## says too many terms. 
## makes sense, we've got 5 explanatory terms for six sites...

mod0 <- rda(danthOTU.h ~ 1, data=danth.poly.ortho1.DF)
mod1 <- rda(danthOTU.h ~ ., data=danth.poly.ortho1.DF)
danth.trend.fwd <- ordiR2step(mod0, mod1)

## meh, generally, this approach is not working
## just not enough points

mod1
danth.trend.rda.ortho 

anova.cca(mod1)

anova.cca(danth.trend.fwd)

## what above just a north-south pattern, centered or not?

aa <- rda(danthOTU.h ~ danSiteCoords$X)
anova.cca(aa)

aa <- rda(danthOTU.h ~ danth.poly.ortho3.DF$Y2)
anova.cca(aa)
## well, that explains a fair amount of variance
## and is statistically significance

## how can we visualize?

## I think we need to visualize our sample scheme here:

plot(danSiteCoords)

X = danSiteCoords$X
Y = danSiteCoords$Y

plot(danSiteCoords, asp=1)

sr.value(danSiteCoords, Y)
## this plotting function sucks. Probably need to redo in matplotlib.

aa = rda(danthOTU.h ~ danth.poly.ortho3.DF$Y1)
anova.cca(aa)

aa = rda(danthOTU.h ~ danth.poly.ortho3.DF$Y2)
anova.cca(aa)
## this is the most relevant patter

danth.poly.ortho3.DF$Y2

aa = rda(danthOTU.h ~ danth.poly.ortho3.DF$Y3)
anova.cca(aa)

## so to review:
## y
danthYmodel <- rda(danthOTU.h ~ danSiteCoords$Y)
#danthYmodel <- rda(danthOTU.h ~ danth.poly.ortho3.DF$Y)
anova.cca(danthYmodel)
## y2 
danthY2model <- rda(danthOTU.h ~ danth.poly.ortho3.DF$Y2)
anova.cca(danthY2model)
## y3
danthY3model <- rda(danthOTU.h ~ danth.poly.ortho3.DF$Y3)
anova.cca(danthY3model)

## y2 is the only significant trend. 
## this is probably a reflection of the general north south 
## trend in diversity, coupled with the weirdness in the 
## south of french flat. The shifted parabolla allows 
## a weird southern point (FF), and monotonic increase after
## that due to increasing similarity

This is probably a reflection of the general north south trend in diversity, coupled with the weirdness in the south of french flat. The shifted parabolla allows a weird southern point (FF), and monotonic increase after that due to increasing diversity.

write.csv(danthOTU, file='danthOTU.csv')
write.csv(festuOTU, file='festOTU.csv')

#### python #####

sampleData = pd.read_csv("grahamsSampleData.csv", index_col=0)

aa = pd.read_csv('danthOTU.csv', index_col=0)
## pretty sure we need rows to be sites, columns to be otus


danOTU = aa.T

danOTU.iloc[0:5,0:5]

sampleData.iloc[0:5,0:5]

sampleData.site

sampleData.shape

danOTU.shape

aa = danOTU.merge(sampleData.site, left_index=True, right_index=True)
## we want a site (not sample) by OTU
## we need to get site info on the OTU table, then split and sum the columns. 
danSiteOTUsums = aa.groupby('site').sum()

## this is what we should transform for further analysis

danSiteOTU.to_csv('danSiteOTUsums.csv')

## now repeat for festuca

aa = pd.read_csv('festOTU.csv')
festOTU = aa.T
aa = festOTU.merge(sampleData.site, left_index=True, right_index=True)
festSiteOTUsums = aa.groupby('site').sum()
festSiteOTUsums.to_csv('festSiteOTUsums.csv')

## and back to R, as above. 

## to check polynomial trends in festuca

library('ade4')
library('vegan')
library('phyloseq')

biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')
## get rid of controls
biom97_noCon = subset_samples(biom97, site != 'Control')
festuBiom = subset_samples(biom97_noCon, host == 'F. roemeri')
festuOTU = otu_table(festuBiom)
festSiteOTUsums <- read.csv('festSiteOTUsums.csv', row.names='site')
festOTU.h <- decostand(festSiteOTUsums, "hellinger")
grahamSiteCoords = read.csv('grahamSiteCoords.csv', row.names='site')
festSiteCoords <- grahamSiteCoords[rownames(festSiteOTUsums),]
fest.poly.ortho3 <- poly(as.matrix(festSiteCoords), degree=3)
fest.poly.ortho3.DF <- as.data.frame(fest.poly.ortho3)
colnames(fest.poly.ortho3.DF) <- c("X", "X2", "X3", "Y", "XY", "X2Y", "Y2", "XY2", "Y3")  

festYmodel <- rda(festOTU.h ~ fest.poly.ortho3.DF$Y)
anova.cca(festYmodel)

festY2model <- rda(festOTU.h ~ fest.poly.ortho3.DF$Y2)
anova.cca(festY2model)

festY3model <- rda(festOTU.h ~ fest.poly.ortho3.DF$Y3)
anova.cca(festY3model)

## back up, here is a cleaner version of the danth pipeline:

danthBiom = subset_samples(biom97_noCon, host == 'F. roemeri')
danthOTU = otu_table(danthBiom)
danthSiteOTUsums <- read.csv('danSiteOTUsums.csv', row.names='site')
danthOTU.h <- decostand(danthSiteOTUsums, "hellinger")
grahamSiteCoords = read.csv('grahamSiteCoords.csv', row.names='site')
danthSiteCoords <- grahamSiteCoords[rownames(danthSiteOTUsums),]
danth.poly.ortho3 <- poly(as.matrix(danthSiteCoords), degree=3)
danth.poly.ortho3.DF <- as.data.frame(danth.poly.ortho3)
colnames(danth.poly.ortho3.DF) <- c("X", "X2", "X3", "Y", "XY", "X2Y", "Y2", "XY2", "Y3")

## yeah, not seeing much there. 

## update notebook, start thinking about the scraping problem.



######## task #3 ######

## we need to 

## 1. see what species are shared at FF among the two hosts, and
## 2. get the sequences for these
## 4. automate blast for these
## 3. figure out if there are any interesting ecological stories known 
##    about these fungi

## 3 will require reading over the sources that blast comes up with, 
## also just generally searching the lit for the names that pop up

## step one should be pretty "easy", with phyloseq:

library(phyloseq)

biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')
ffBiom = subset_samples(biom97, site == "French_flat")
festuFFbiom = subset_samples(ffBiom, host == 'F. roemeri')
danthFFbiom = subset_samples(ffBiom, host == 'D. californica')

festuFFOTU = otu_table(festuFFbiom)
festuFFOTUnoZero = festuFFOTU[rowSums(festuFFOTU) != 0]

danthFFOTU = otu_table(danthFFbiom)
danthFFOTUnoZero = danthFFOTU[rowSums(danthFFOTU) != 0]

danthFFOTUnoZero
festuFFOTUnoZero

## we should be able to do an inner join on these:
class(as.data.frame(danthFFOTUnoZero))

#aa = merge(danthOTUnoZero, festuOTUnoZero)
## okay, that was a very bad idea. almost core dumped. 

## do it the old fashioned way: 

dim(danthFFOTUnoZero) ## 843 species

dim(festuFFOTUnoZero) ## 884 species

sum(rownames(danthFFOTUnoZero) %in% rownames(festuFFOTUnoZero))
sum(rownames(festuFFOTUnoZero) %in% rownames(danthFFOTUnoZero))
## 463 shared species among festuca and danthonia at french flat

sharedSppFilter <- rownames(festuFFOTUnoZero) %in% rownames(danthFFOTUnoZero)

dim(danthFFOTUnoZero)

dim(festuFFOTUnoZero)

length(sharedSppFilter)

sharedSpp <- rownames(festuFFOTUnoZero[sharedSppFilter])

for (i in sharedSpp){print(i)}

sink('sharedFFspecies.txt')
    for (i in sharedSpp){cat(i);cat("\n")}
sink()

## great. how can we get the sequences for this?


## graham has kindly supplied the sequence data for his otus:
## in shell what do these look like:
head ../otus_97_uclust.fasta
## I think this is what we want, has unique identifiers
head otus_97_uclust_relabel.fasta 

##### back to python #####

from Bio import SeqIO
import re

## we should be able to query the the sequences using 
## the otu names

## read in the shared otus

## we need a regex I think
## should work for cleaning up both files:
p =  re.compile('(OTU).*(grass)') 

## shared species are small enough it's okay to read it all in
with open('sharedFFspecies.txt', 'r') as sharedOTUs:
    aa = sharedOTUs.readlines()
    cleanedSharedSpp = [ p.match(i).group() for i in aa]

## ugh, biopython, haven't used in a while. How do we read in the 
## fasta file again?

help(SeqIO.parse)

cc = []
bb = SeqIO.parse('../otus_97_uclust_relabel.fasta', "fasta" )
for i in bb: 
    if p.match(i.id).group() in cleanedSharedSpp:
        i.name = p.match(i.id).group()
        i.id = p.match(i.id).group()
        i.description = ''
        cc.append(i)

SeqIO.write(cc, "ffSharedSpp.faa", "fasta")

## did that work? looks right. 

## check it tomorrow. 

## its tomorrow. Just do a few sanity checks

## so, for example, back in R

library(phyloseq)

biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')
ffBiom = subset_samples(biom97, site == "French_flat")
festuFFbiom = subset_samples(ffBiom, host == 'F. roemeri')
danthFFbiom = subset_samples(ffBiom, host == 'D. californica')
festuFFOTU = otu_table(festuFFbiom)
festuFFOTUnoZero = festuFFOTU[rowSums(festuFFOTU) != 0]
danthFFOTU = otu_table(danthFFbiom)
danthFFOTUnoZero = danthFFOTU[rowSums(danthFFOTU) != 0]

## some shared species examples would be:

tail(festuFFOTUnoZero)

festuFFOTUnoZero['OTU2:5grass',]
###########
danthFFOTUnoZero['OTU2:5grass',]
## common in both.

festuFFOTUnoZero['OTU2164:93grass',] ## just one observation

danthFFOTUnoZero['OTU2164:93grass',] ## common, >half sites

## and how species that are definitely not in both: 

NOTsharedFestu <- !(rownames(festuFFOTUnoZero) %in% rownames(danthFFOTUnoZero))
festuFFOTUnoZero[NOTsharedFestu,]
festuFFOTUnoZero['OTU608:16grass']

danthFFOTUnoZero['OTU608:16grass']
danthFFOTU['OTU608:16grass'] ## yup, not in Danthonia

## flip it

NOTsharedDanth <- !(rownames(danthFFOTUnoZero) %in% rownames(festuFFOTUnoZero))

danthFFOTUnoZero[NOTsharedDanth,]

## two examples
OTU3697:100grass
OTU218:38grass


festuFFOTUnoZero['OTU3697:100grass'] ## nope
festuFFOTU['OTU3697:100grass'] ## all zeros

festuFFOTUnoZero['OTU218:38grass'] ## nope
festuFFOTU['OTU218:38grass'] ## all zeros

## okay, looks good to me. sanity checked 

## now we want to know, what are these fungi?

## the database of choice is important...
## should we use UNITE? or genbank?

## this might actually be a time when low quality accessions 
## may be of interest. 

## start with UNITE, I guess. Time to update UNITE. I hate doing this. 

## get it here:

wget https://files.plutof.ut.ee/public/orig/6A/F9/6AF94919CCB48307734D6256CACA50AE1ECBC0839F644D4B661E3673525E41A4.tgz 

## generally can get the latest at this website:
https://unite.ut.ee/repository.php

## we'll keep it here:
uniteFile = ("/home/daniel/Documents/analyses/UNITE/"
            "sh_general_release_s_10.05.2021/"
            "sh_general_release_dynamic_s_10.05.2021.fasta")

## make a local blastn database, in Bash:

ln -s \
/home/daniel/Documents/analyses/UNITE/sh_general_release_s_10.05.2021/sh_general_release_dynamic_s_10.05.2021.fasta \
unite2021.fasta

makeblastdb -in unite2021.fasta -dbtype nucl -logfile dberrors.txt

## now, how get our fungi identified...

blastn -query ffSharedSpp.faa -db unite2021.fasta -out FrenchFlatSharedSpeciesBlast.txt -num_descriptions 10 -num_alignments 3

## at first glance, a lot of these are pretty typical endophytes (epicoccum, cladosporium, etc)

## might be good to narrow this down to endophytes that are only found at french flat, and that are shared.

## also, we should get Rusty Rodriguez's endophytes from stress environment, see if any are present here...

## atcc's strain of Vishniacozyma victoriae comes from Antarctica? 

## anyway, we need a first glance sheet, to see if anything sticks out:


blastn -query ffSharedSpp.faa -db unite2021.fasta -out FrenchFlatSharedSpeciesBlast.txt -num_descriptions 10 -num_alignments 3

blastn -query ffSharedSpp.faa -db unite2021.fasta -out FrenchFlatSharedSpeciesBlastQuickRead.csv -outfmt 10 -max_target_seqs 1

## we want a nice header on that:
sed '1 i\qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' FrenchFlatSharedSpeciesBlastQuickRead.csv -i

## that can be played with in Pandas.

## but before we go there, should we repeat the above pipeline, but subset to just otus that 
## are unique to FF?:

## back in R

library(phyloseq)
biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')

## we want only those otus that are:
## 1) present in both hosts
## 2) present only at french flat. 

## first, get OTUs unique to french flat:
ffBiom = subset_samples(biom97, site == "French_flat")
## but we also need only those species that are in both danth and festu. We saved these
## somewhere...


## our non-FrenchFlat biomtable
nonFFbiom = subset_samples(biom97, site != "French_flat")

ffBiomOTU <- otu_table(ffBiom)

##get rid of zeros
dim(otu_table(ffBiomOTU)[rowSums(otu_table(ffBiomOTU)) != 0]) ## 1264


ffOTUnoZeros <- ffBiomOTU[rowSums(ffBiomOTU) != 0]

colSums(ffOTUnoZeros) ## no zero sites, of course

nonFFbiomOTU <- otu_table(nonFFbiom)

nonFFbiomNoZeros <- nonFFbiomOTU[rowSums(nonFFbiomOTU) != 0]

dim(otu_table(nonFFbiom))

dim(nonFFbiomNoZeros)
## we lose ~200 species when we subset to nonFF sites. What are they?

rownames(nonFFbiomNoZeros)

rownames(ffOTUnoZeros)

## but how do we subset to species found only at FF? and no where else?

## should be this:
uniqFFfilt <- !(rownames(ffOTUnoZeros) %in% rownames(nonFFbiomNoZeros))
uniqFFotu <- ffOTUnoZeros[uniqFFfilt,]

## but got to get out the shared species, which we figured out
## above:

sharedFFspecies <- scan("sharedFFspecies.txt", what='character')

sum(sharedFFspecies %in% rownames(uniqFFotu)) ## only eight species. 

## and they are?
sharedOnlyFFfilt <- rownames(uniqFFotu) %in% sharedFFspecies 
sharedOnlyFF <- uniqFFotu[sharedOnlyFFfilt,]

for (i in rownames(sharedOnlyFF)){
    print(i)
    }

## write it out:
#sink('uniqueFFspeciesSharedBothHosts.txt')
#    for (i in rownames(sharedOnlyFF)){cat(i);cat("\n")}
#sink()

## do we have a blast match for these?

## OTU1879:6grass : no hits
## OTU3919:89grass : Sporormiella maybe?, 96% match
## OTU534:7grass : good hits (99%) to completely unidentified fungi
## OTU284:8grass : good hits (99%) to completely unidentified fungi
## OTU3455:8grass : weak hits (96%) to completely unidentified fungi
## OTU870:9grass : weak hits (93%) to tremellales
## OTU3897:98grass : weak hits (95%) to Rachicladosporium, black rock fungi
## OTU2164:93grass : okay hits (97%) to completely unidentified fungi

## great, so we have a slew of unidentified fungi

## so what now? let's get out the sequences of the unique, shared FF species

## dip into python:

from Bio import SeqIO
import re

p =  re.compile('(OTU).*(grass)') 

#with open('sharedFFspecies.txt', 'r') as sharedOTUs:
with open('uniqueFFspeciesSharedBothHosts.txt', 'r') as sharedOTUs:
    aa = sharedOTUs.readlines()
    cleanedSharedSpp = [ p.match(i).group() for i in aa]


cc = []
bb = SeqIO.parse('../otus_97_uclust_relabel.fasta', "fasta" )
for i in bb: 
    if p.match(i.id).group() in cleanedSharedSpp:
        i.name = p.match(i.id).group()
        i.id = p.match(i.id).group()
        i.description = ''
        cc.append(i)

SeqIO.write(cc, "uniqueFFspeciesSharedBothHosts.fasta", "fasta")

## back to bash 

## try to use rdp or sintax, get some idea of what these are. 

## we'll use Edgar's sintax program. 

## make a database:

ln -s /home/daniel/Documents/analyses/UNITE/sh_general_release_s_10.05.2021/UNITE_usearch.fasta  UNITE_usearch.fasta

usearch11 -makeudb_usearch UNITE_usearch.fasta -output UNITE_usearch.udb

UNITEUS=/home/daniel/Documents/analyses/UNITE/sh_general_release_s_10.05.2021/UNITE_usearch.udb

ls -l $UNITEUS

## let's start with just the OTUs unique to FF, held by both hosts:

usearch11 -sintax uniqueFFspeciesSharedBothHosts.fasta -db $UNITEUS \
 -tabbedout uniqueFFspeciesSharedBothHosts.sintax \
 -strand both -sintax_cutoff 0.8

less uniqueFFspeciesSharedBothHosts.sintax

## not a ton of new information

## great. 

## next steps. 

###############################

## we might want to see if we can quantify ecologically important species 
## at the FF site 

## two strategies come to mind: do an indicator species analysis, 
## and transform/PCA the species matrix, check the loadings on the big axes

## in R

library(phyloseq)
library(indicspecies)

## indicator species:


## need a sites as rows OTU table:
biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')
biom97_noCon = subset_samples(biom97, site != 'Control')

biom97_noCon

help(otu_table)

biomNoConOTU <- t(otu_table(biom97_noCon))

help(multipatt)

## we want to use site as our partitioning variable


sample_data(biom97_noCon)[1:5,1:5]

siteNu = which( colnames(sample_data(biom97_noCon)) == "site")
sitesDF <- sample_data(biom97_noCon)[,siteNu, drop=FALSE]

biomNoConOTU

## can we use this
all(rownames(biomNoConOTU) == rownames(sitesDF))

## great:

sites <- sample_data(biom97_noCon)$site

bothHostIndSpec <- multipatt(biomNoConOTU, sites, func = 'r.g', control=how(nperm=9999))

bothHostIndSpec <- habIndSpp

#save(bothHostIndSpec, "bothHostIndSpec.rda")

load("bothHostIndSpec.rda")

## and that takes forever

## and done.

## interesting, only one of the above unique species for FF appears as an indicator 
## species: OTU1879:6grass

## and it isn't that strong of an indicator. 

## okay, so what do we do with this? can we extract just the FF-associated species?

## look at the blast results for these:

str(bothHostIndSpec)

str(bothHostIndSpec$comb) ## not what we want...

bothHostIndSpec$comb[1:5,]

bothHostIndSpec$comb

## think this is what we need:
bothHostIndSpec$sign

str(bothHostIndSpec$sign)

stat p.value

aa <- bothHostIndSpec$sign

## don't want to fuck with this in R. get out to Pandas

write.csv(aa, file="indSpecResults.csv")

import numpy as np
import pandas as pd
from Bio import SeqIO
import re

## we want only those OTUS that have a hit for FF, and only FF

indSpecResults = pd.read_csv("indSpecResults.csv", index_col=0)
## those periods are fucking things up. 
indSpecResults.columns = indSpecResults.columns.str.replace('s\.','')
indSpecResults.columns = indSpecResults.columns.str.replace('p\.','p')
## not sure what this is, but index is an important word here in pandas:
indSpecResults.columns = indSpecResults.columns.str.replace('index','inde')

## maybe better said this way:
indSpecResults = pd.read_csv("indSpecResults.csv", index_col=0)
indSpecResults.columns = (indSpecResults.columns
                    .str.replace('s\.','')
                    .str.replace('p\.','p')
                    .str.replace('index','inde'))


## we need to find the all zero rows for the other sites, isolate FF 
aa = indSpecResults.query("French_flat > 0")
bb = aa[["Hazel_Dell","Horse_Rock","Lower_Table","Roxy_Ann","Upper_Table","Upper_Weir","Whetstone","Whidbey"]]
cc = ~bb.any(axis=1)
onlyFF = aa[cc]
onlyFFsig = onlyFF.query("pvalue < 0.05")[["stat","pvalue"]]
onlyFFsig.sort_values(by=['stat','pvalue'], ascending=False, inplace=True)
#onlyFFsig.to_csv('FFonlyIndSpec.csv')

## looks good. Now what?

## check our blast results with these

## or maybe reblast, probably actually easier. 

## need the sequences for these:

FFonlyIndSpecNames = list(onlyFFsig.index.values)
cc = []
bb = SeqIO.parse('../otus_97_uclust_relabel.fasta', "fasta" )
for i in bb: 
    name = re.sub(';size=.*;','',i.name)
    if name in FFonlyIndSpecNames:
        print(name)
        i.name = name
        i.id = name
        i.description = ''
        cc.append(i)

## write out:
#SeqIO.write(cc, "FFindSpecies.fasta", "fasta")

blastn -query FFindSpecies.fasta -db unite2021.fasta -out FFindSpeciesBlast.txt -num_descriptions 10 -num_alignments 5
blastn -query FFindSpecies.fasta -db unite2021.fasta -out FFindSpeciesBlastQuickRead.csv -outfmt 10 -max_target_seqs 1

sed '1 i\qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' FFindSpeciesBlastQuickRead.csv -i

libreoffice --calc FFindSpeciesBlastQuickRead.csv &

## welp, a lot of uncertainty there, but at least there are fewer complete blanks. 
## a whole lot of these point to the same species hypo in UNITE, Colletotrichum graminicola:

Colletotrichum_graminicola|DQ126180|SH2699426.08FU

## so the plan:

## check to see how many of these indicators are in both host species

## group them and make trees - at least colletotrichum, and perhaps the tremallales and preussia groups

##### check indicators for presence in both hosts ######

## back in R

library(phyloseq)
library(indicspecies)

biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')
biom97_noCon = subset_samples(biom97, site != 'Control')

FFonlyIndSpec <- as.character(read.csv('FFonlyIndSpec.csv')[,1])

FFonlyIndSpec

## how to check these for membership in both hosts? 

biom97_noCon

head(sample_data(biom97_noCon))

biom97_FF = subset_samples(biom97, site == 'French_flat')

biom97_FF_festu = subset_samples(biom97_FF, host == 'F. roemeri')
biom97_FF_danth = subset_samples(biom97_FF, host == 'D. californica')

aa <- otu_table(biom97_FF_festu)
otu_FF_festu_n0 <- aa[rowSums(aa) != 0,]

aa <- otu_table(biom97_FF_danth)
otu_FF_danth_n0 <- aa[rowSums(aa) != 0,]

##  

inFestu <- FFonlyIndSpec %in% rownames(otu_FF_festu_n0)
inDanth <- FFonlyIndSpec %in% rownames(otu_FF_danth_n0)

FFonlyIndSpec

FFindHosts <- as.data.frame(cbind(inFestu, inDanth), row.names=FFonlyIndSpec)

## how many of these indicators are in both species?
sum(rowSums(FFindHosts) == 2) ## 29
dim(FFindHosts) ## out of 52

## retain these, find out what they are:
aa <- rownames(FFindHosts[rowSums(FFindHosts) == 2,])

sink('FFindSpecBothHosts.txt')
    for (i in aa){cat(i);cat('\n')}
sink()

## now find out what these are, back to python.

aa = pd.read_csv('FFindSpecBothHosts.txt', header=None).iloc[:,0]
FFindSpecBothHostsNames = list(aa)

cc = []
bb = SeqIO.parse('../otus_97_uclust_relabel.fasta', "fasta" )
for i in bb: 
    name = re.sub(';size=.*;','',i.name)
    if name in FFindSpecBothHostsNames:
        print(name)
        print(i.name)
        i.name = name
        i.id = name
        i.description = ''
        cc.append(i)


## write out:
SeqIO.write(cc, "FFindSpeciesBothHosts.fasta", "fasta")

## now blast:

blastn -query FFindSpeciesBothHosts.fasta -db unite2021.fasta -out FFindSpeciesBothHostsBlast.txt -num_descriptions 10 -num_alignments 10
blastn -query FFindSpeciesBothHosts.fasta -db unite2021.fasta -out FFindSpeciesBothHostsBlastQuickRead.csv -outfmt 10 -max_target_seqs 1

sed '1 i\qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' FFindSpeciesBothHostsBlastQuickRead.csv -i

libreoffice --calc FFindSpeciesBothHostsBlastQuickRead.csv &

## so several of the colletotrichums are in both hosts. 
## and several are found in only one host, I therefore deduce
## Are any colletotrichums unique to FF? 
## doesn't look like it. So why are they important? 
## perhaps due to read abundances, which makes me nervous,
## cuz I don't trust these read abundances
## also splitting may have occurred, understating
## the importance of colle
## we need a better overview

## okay, so we need a list of otus that are possible
## colletotrichums, which sites they were found in, 
## and which hosts, and the relative abundances. 

## first, how do we get every OTU that matched to a 
## colletrichum?

## as a wide net, first pass this would be in graham's
## taxonomy table:

aa = tax_table(biom97)
colnames(aa)=c('kingdom','phylum','class','order','family','genus','species')


aa = as.data.frame(aa@.Data)

aa[1:3,1:3]

write.csv(aa, file='grahamTaxonomyTable.csv')

##### let's go to pandas for this

aa = pd.read_csv('grahamTaxonomyTable.csv', index_col = 0)

aa.iloc[0:2,0:6]

## get rid of unnecessary prefix tags:

## for example
bb = pd.Series(['k__Fungi','p__Ascomycota','c__Sordariomycetes','o__Hypocreales','f__Hypocreales_fam_Incertae_sedis','g__Sarocladium'])
bb.str.replace("[kpcofgs]__","") 

## we really just need genus:

aa.query("genus == 'g__Colletotrichum'")

aa.query("genus == 'g__Glomerella'")


## huh, none of these are graminicola. Weird. 

genera = aa.genus.str.replace("[g]__","")

genera == 


for label, content in aa.iteritems():
    #print(label)
    print(content)

## weird. Graham's results don't resemble mine?
## there are no C. graminicola?

## the following top-blasted to colletotrichum graminicola for me, at around 97% identity:

aa.loc['OTU1028:13grass'] ## close, Glomerella in grahams tax table, but no species
aa.loc['OTU1202:12grass'] ## also Glomerella in grahams tax table, no species
aa.loc['OTU1265:13grass'] ## also Glomerella in grahams tax table, no species
aa.loc['OTU1468:13grass'] ## also Glomerella in grahams tax table, no species

## so it seems like maybe genus isn't where to start here, 
## or we include glomerella everywhere we want to look for Colletotrichum, 
## since they are +/- synonyms
## or maybe best, go family level here and assume its all colletotrichum:

aa.query("family == 'f__Glomerellaceae'")

FFindSpecBothHosts = pd.read_csv('FFindSpecBothHosts.txt', header=None).iloc[:,0]


