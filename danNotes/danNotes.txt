
## let's have a look at Graham's pipeline. 
## for the record, I've added SSH key access for this repo.
## In case I need to do this again (github just stopped 
## password access), you have to change the remote url to a
## format that ssh can work with:

git remote set-url origin git@github.com:gbailes/grass-endophyte-community.git

## Grahams notebook for making his biom can be viewed here:
https://nbviewer.jupyter.org/github/gbailes/grass-endophyte-community/blob/master/alt_biom.ipynb

## my own notebook is being built, viewable here:
https://nbviewer.org/github/gbailes/grass-endophyte-community/blob/master/danNotes/danStatsForGrassAnalysis.ipynb

## as far as grahams paper, I think I was supposed to:

 """
 1. Check trend surfaces of community matrix varation - can we describe the 
 variation in graham's community matrices with a north/sound trend in diversity 
 and dissimilarity? I think I recommended this in place of the original dbmem analysis, 
 if it turns out to be informative.  

 2. Attempt to quantify the apparent S->N increase in diversity. Means repeating 
 Grahams diversity metrics on the pre-variance-stabilization (pre-deseq), then 
 check to see if there is enough statistical power to build a point pattern model from these.  

 3. Figure out the story of the french flat site, the only place where the two grass 
 host species share a lot of endophyte species. What are the shared species? 
 Are they unique to the site? Are they interesting, as in maybe one of the species 
 that have been shown to give panicum grass salt and drought tolerance?
 """

 this all comes from a 1.5 year-old email (Feb 2, 2020), "review of goals for Dan with Graham Paper"
 #3 was the doozy. 

## what is the general strategy here...

## the first two goals (#1 and #2) will be in R. Not sure I agree with the 
## approaches I suggested originally, with point patterns, etc. I think a 
## simple.

## the second will be sort of a treasure hunt. I think that one will take 
## the most time. 

## let's go through them in order...

##### remembering trend surfaces ########

## let's jog our memories about trend surfaces
## with the tutorial from Borcard, on the orobatid mite data:

## ugh, there is a new version of R out, they are on 4.x.x now. The beast grows
## in its power... let's stay in the 3.x.x world for the moment. 

## get the "s.value" function out the supp materials for borcard:

R 

library('ade4')
library('vegan')

source('/home/daniel/Documents/Books/Stats/numericalEcologyR/NEwR-2ed_code_data/NEwR-2ed_code_data/NEwR2-Functions/sr.value.R')

xygrid <- expand.grid(1:10, 1:10)

plot(xygrid)

xygrid.c <- scale(xygrid, scale=FALSE)

X <- xygrid.c[,1]
Y <- xygrid.c[,2]

par(mfrow=c(3,2))

par(mfrow=c(1,1))

sr.value(xygrid, (X))

sr.value(xygrid, (Y))
sr.value(xygrid, (X+Y))
sr.value(xygrid, (X^2+Y))
sr.value(xygrid, (X^2+Y^2))
sr.value(xygrid, (X^3))

sr.value(xygrid, (X^2))


## cool. Lots of patterns are possible. 

## onward with the tutorial...

data('mite')
data('mite.env')
data('mite.xy')

mite.h <- decostand(mite, "hellinger")

mite.xy.c <- scale(mite.xy, center=TRUE, scale=FALSE)

mite.poly <- poly(as.matrix(mite.xy.c), degree=3, raw=TRUE)

## raw=TRUE retains all the requested polynomials. if FALSE,
## apparently it retains only the truly orthogonal ones.

## so we can try all kinds of polynomial surfaces out as 
## possible models. 

## modeling the N-S bioiversity trend (#2 above) should be
## pretty simple, then. Just use richness, simpsons, whatever, 
## and model it on a linear S-N increasing plane. Univariate,
## so very straightforward

## butu for modeling patterns in community variation, 
## how can we use these polynomial surfaces to model a multivariate
## community? 

## borcard says with RDAs:

colnames(mite.poly) = c("X", "X2", "X3", "Y", "XY", "X2Y", "Y2", "XY2", "Y3")  

mite.trend.rda <- rda(mite.h ~ ., data=as.data.frame(mite.poly))

R2adj.poly <- RsquareAdj(mite.trend.rda)

## from here Borcard backs up, and uses the orthogonal polynomials.
## I don't really understand how orthogonal polys are made, its magic
## to me. But here we go:

mite.poly.ortho <- poly(as.matrix(mite.xy), degree=3)
colnames(mite.poly.ortho) = c("X", "X2", "X3", "Y", "XY", "X2Y", "Y2", "XY2", "Y3")  
mite.trend.rda.ortho <- rda(mite.h ~ ., data=as.data.frame(mite.poly.ortho))


mite.trend.rda.ortho

mite.trend.rda

R2adj.poly <- RsquareAdj(mite.trend.rda.ortho)$adj.r.squared

## for some reason, these are giving me exactly the same results.
## not going to take the time to figure out why...probably typo...

## forward selection 
## forward.sel doesn't seem to exist anymore.
## I think the ortholog here is ordistep or ordiR2step.

mod0 <- rda(mite.h ~ 1, data=as.data.frame(mite.poly.ortho))
mod1 <- rda(mite.h ~ ., data=as.data.frame(mite.poly.ortho))

mite.trend.fwd <- ordiR2step(mod0, mod1)

## as per example, six terms retained, I think?
## use these

mite.trend.rda2 <- rda(mite.h ~ . 

as.data.frame(mite.poly)

as.data.frame(mite.poly)[

mite.trend.fwd[,2]


str(mite.trend.fwd)
str(mod1)

## great, so ordiR2step drops the uninformative components for you. 

## so how do we know from this which were the important polynomials?


str(mite.trend.fwd)

str(mite.trend.fwd$terminfo)

str(mite.trend.fwd$terminfo$terms)

## um, is this the only way to extract this?
attributes(mite.trend.fwd$terminfo$ordered)$names

## probably not. seems stupid. But for the moment it 
## will work:

sigTerms <- attributes(mite.trend.fwd$terminfo$ordered)$names

## check significance:

anova.cca(mite.trend.fwd)

anova.cca(mite.trend.fwd, by="axis")
## RDA1 is by far the most important. But I don't really understand these 
## new axes. Are they in terms the spatial predictors or the community 
## matrix "response" variables? 
## I assume they are in terms of the spatial predictors, the polynomials,
## because there are six of them, matching the number of polynomials.

## can we look at them?

mite.trend.fit <- scores.cca(mite.trend.fwd, choices = c(1,2,3), display="lc", scaling=1)

## that don't work anymore...how can we get these out of the model....
## looks like they updated their methods to fit in with everybody else:

mite.trend.fit <- scores(mite.trend.fwd, choices = c(1,2,3), display="lc", scaling=1)

par(mfrow=c(1,3))
sr.value(mite.xy,mite.trend.fit[,1])
sr.value(mite.xy,mite.trend.fit[,2])
sr.value(mite.xy,mite.trend.fit[,3])

## great. that all makes some kind of sense. How do we apply to our 
## situation?
## we will have a multivariate community matrix (Graham's endophyte matrix), and a 
## a single variate diversity index. 
## we want to check them both for trend surfaces.

## with the biodiversity, we suspect a simple, first order linear equation 
## with a northward increase in biodiversity

## for the multivariate community data, I think we just want to look for interesting
## spatial patterns. So maybe, rerun the above analysis on grahams community data.

## either way, we need his biome table. From this we can work backword get diversity 
## metrics. 

## where is this? graham has a biome file ready...

## so next step will be to remember how to manipulate 

## phyloseq, etc. Oh jeezus. 
  
########## get graham's biom table into memory ##########

## the way to do this used to be in R, in phyloseq. Are folks still doing this?

## a quick lit search shows lots of citations for phyloseq...

## let's assume phyloseq is still cool. 

library(phyloseq)

biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')

head(sample_data(biom97))

colnames(sample_data(biom97))

tax_table(biom97)[0:3,]

head(tax_table(biom97))


otu_table(biom97)[0:3,]

dim(otu_table(biom97))

## great. This looks pre-deseq transformation, which is also good. 

## so what's task number one?

## the most obvious task is the bioiversity regression. 

## we need to rarefy to an even number of sequences, and 
## get good, UTM-style xy coordinates for these sites. 

## so, for the moment, get the estimated UTMs for each site. How?

## let's go pyproj. lat/long epsg is 4326

## let's export the sample data to something python can use:

class(sample_data(biom97))

str(sample_data(biom97))

write.csv(sample_data(biom97), file='grahamSample.csv')


##### over to python, to wrestle out the UTMs for these sites ######

import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
from scipy import stats
from sklearn.linear_model import LinearRegression


grahamSample = pd.read_csv('grahamSample.csv', index_col=0)

grahamSample.columns

grahamSample.site.to_list()

grahamSample.site.unique().shape

## pare down to one row for each site:

grahamSample.head()

#aa = grahamSample[['samplename','site','latitude','longitude']]
## don't keep unique ids

grahamSample.head()

## seems like we have two locations for Hazel Dell:
grs = grahamSample.groupby('site')
grs.get_group('Hazel_Dell').latitude.unique()
grs.get_group('Hazel_Dell').longitude.unique()
## so let's change all Hazel dell latitudes to be 
## the same, keeping the more southerly (lower) value:

grahamSample.latitude.dtype

sum(grahamSample.latitude == 44.03) ## 12 samples
sum(grahamSample.latitude == 44.02) ## also 12 samples

## assign all to a single value:
hd4403 = grahamSample.latitude == 44.03
grahamSample.latitude[hd4403] = 44.02

sum(grahamSample.latitude == 44.03)
sum(grahamSample.latitude == 44.02)
## looks good

aa = grahamSample[['site','latitude','longitude']]
bb = aa.reset_index(drop=True)
## just to make sure the lat longs are indeed the same for each site:
#cc = bb.groupby('site')
#cc.get_group('Control')
#cc.get_group('French_flat') ## etc
#cc.groups.keys()
## anyway, 
dd = bb.drop_duplicates()
dd.set_index('site', inplace=True)
ee = dd.drop('Control')
## great. Now how to convert these? go to geopanda? 
grahamSites = (gpd.GeoDataFrame(
    ee, geometry=gpd.points_from_xy(ee.longitude, ee.latitude))
    .drop(['latitude','longitude'], axis=1)
)



grahamSites.crs

grahamSites.set_crs('EPSG:4326', inplace=True)

grahamSites.crs

## reproject to UTM zone 10, which is EPSG:26710
grahamSites.to_crs('EPSG:26710', inplace=True)

grahamSites.crs

#grahamSites.to_csv('grahamSiteCoords.csv')
## not a perfect export, have to modify with vim to get a good csv
## but works

## and maybe save us some work later:
#grahamSites.to_pickle('grahamSiteCoords.p')

## check it on some maps. Here are some publicly available geojsons:

## in shell ##
wget https://raw.githubusercontent.com/johan/world.geo.json/master/countries/USA/WA.geo.json
wget https://raw.githubusercontent.com/johan/world.geo.json/master/countries/USA/OR.geo.json
## in shell ##

## so how does it look?:


oregon = gpd.read_file('OR.geo.json')
oregon.to_crs('EPSG:26710', inplace=True)
washington = gpd.read_file('WA.geo.json')
washington.to_crs('EPSG:26710', inplace=True)

plt.close('all')
fig, ax = plt.subplots()
oregon.plot(ax=ax)
washington.plot(ax=ax)
grahamSites.plot(ax=ax, color='red')

## great. What else do we need here? Not much, methinks...back to R?

###### task #2 species richness as a function of latitude ########

## now, we want to give species richness, simpsons or something for each 
## site.

## we need to rarify to some sort of common of denominator. How are our 
## read depths?

## drop the controls

aa = subset_samples(biom97, site != 'Control')

controls = subset_samples(biom97, site == 'Control')


sample_data(biom97)$site

options(scipen=999)

par(mfrow=c(1,3))
barplot(sort(sample_sums(biom97), decreasing=TRUE))
barplot(sort(sample_sums(aa), decreasing=TRUE))
barplot(sort(sample_sums(controls), decreasing=TRUE))


par(mfrow=c(1,1))
barplot(sort(sample_sums(controls), decreasing=TRUE))

sample_sums(biom97)

sort(sample_sums(biom97))


data[order(data$num,decreasing = TRUE),]

bar_plot(sample_sums(biom97))

aalost <- aalost[order(aalost[,'164wood'], decreasing = TRUE),]

data(enterotype)
TopNOTUs = names(sort(taxa_sums(enterotype), TRUE)[1:10])
ent10 = prune_taxa(TopNOTUs, enterotype)
plot_bar(ent10, "SeqTech", fill = "Enterotype", facet_grid = ~Genus)


biom97rar = rarefy_even_depth(biom97)
## wow, we lost a lot of diversity that way,
## but to be expected, I guess. 

aarar = rarefy_even_depth(aa)
## that helps a lot, there was a very small sample in the controls,
## probably the single species control
## we still rarefy down to 7101, which is a pretty low common denominator...
## how much do we lose if we go to 20,000?

min(sample_sums(aa)) 

sort(sample_sums(aa))[0:20]

sum(sample_sums(aa) < 20000) ## six samples. 
## that's not too many out of 155 samples...
## are they important? where are they?

sortedSamples <- sort(sample_sums(aa))

size = 20000
smallerThanSize <- sortedSamples[sortedSamples < size]


sample_data(biom97)[names(smallerThanSize),'site']
## shit. These are mostly french flat. We will lose
## 5 of 12 FF sites if we drop our 5 lowest sample 
## depths. Sucks. 


bb = subset_samples(biom97, site != 'Control')

sample_sums(aarar)

## to answer if I can get rid of this, have to ask a stupid question:
##  why are there so many samples? 
## 9 sites, 2 hosts, some controls... I would have predicted 
## = ~25 samples.
## but graham has 155. 
## how do these part out by site?

biom97 

head(sample_data(biom97))

head(sample_data(biom97))

## I head data exploration in R. look at this with pandas

write.csv(sample_data(biom97), file="grahamsSampleData.csv")

##### back to python #####

## imports above

## I'm betting that graham kept geographic info for each 
## tuft that he sampled, or the origin of each transect,
## and the location of that grass on its transect. 

## so for each of the nine sites, there is probably a sub-map
## that could be made. 

## don't want to make it, don't think we need it. 
## the main question is, can we pool these plants somehow
## to make the sites comparable for diversity metrics?

sampleData = pd.read_csv("grahamsSampleData.csv", index_col=0)

sampleData.columns

aa = sampleData[['host', 'site']]

## there are two hosts, we are going to need to work with each 
## host individually. Not sure, may also be useful to pool them
## at some point. But can't see the point of that for the moment.

## I think we'll want to start with one host, and then repeat 
## all analyses for for the other, copy-n-paste. 

sampleFroem = sampleData[sampleData.host == "F. roemeri"] 
## 84 plants F.roemerii sampled

sampleDanth = sampleData[sampleData.host == 'D. californica'] 
## 71 of Danthonia

## are these relatively evenly distributed among the sites?

plt.close('all')
fig, ax = plt.subplots(1,2)
sampleDanth.site.hist(ax = ax[0])
sampleFroem.site.hist(ax = ax[1])

## yep. Missing one from horse rock,
## but otherwise all have 12 plants each

sampleFroem.site


sampleDanth.site.unique().shape



sampleDanth.site.unique()

dir(sampleFroem.site.unique())

aa = pd.Series(sampleFroem.site.unique())

bb = pd.Series(sampleDanth.site.unique())

aa[aa.isin(bb)]

bb[~bb.isin(aa)]

## great, so we can afford to lose a few 
## low-abundance samples. problem is,
## listed above, these are almost all 
## from french flat
## another option is to go to 10000 reads,
## which seems like a respectable number

## so let's do several levels, rarefy to 20000 reads,
## and also to 10,000 and 7,000. Then we can compare and
## see how much information is lost by going down to 7000.


#### and back in R. #####

## does phyloseq have a built-in adjustment 
## for this?

biom97_noCon = subset_samples(biom97, site != 'Control')
controls = subset_samples(biom97, site == 'Control')


sample_data(biom97)$site

options(scipen=999)

sampsize=7000
biom97_rar.i = rarefy_even_depth(biom97_noCon, sample.size = sampsize, rngseed = 1)

## 8220 OTUs lost. 

sample_sums(biom97_rar.i)

sample_sums(biom97_noCon)

biom97_rar.i
biom97_noCon

biom97_rar.i

## so now, get species richness for each:

plot_richness(biom97_rar.i)

plot_richness(biom97_rar.i, measures='Observed')

## great. but as usual, phyloseq is doing more than I 
## want it to. Can we just get the richnness values without 
## the fancy graphics?

## I think this is what I need:
?estimate_richness

## vegan has some options, too
?estimateR
?vegan::diversity

sample_data(biom97_rar.i)

dim(sample_data(biom97_rar.i))

aa <- estimate_richness(biom97_rar.i, measures=c('Simpson', 'Observed'))

?estimate_richness

head(aa)

## we'll need these:

write.csv(aa, file='diversity7000.csv')
write.csv(sample_data(biom97_rar.i), file='biom97_rar7000.csv')

## great. but we also need site, x, and y on this. I hate doing this 
## shit in R. Anyway, for the moment I think we got what we need out of 
## of phyloseq?  

## pingpong back to python

## okay, so we want to make a geopanda with: 
## unique sample name, host type, site, UTMs
## for each sample...

aa = pd.read_csv('diversity7000.csv')
aa.columns=['sampleName','Observed','Simpson']
aa['sampleName'] = aa.sampleName.str.replace('X', '')
aa.set_index('sampleName', inplace=True)
bb = pd.read_csv('biom97_rar7000.csv', index_col=0)
bb = bb[['host','site', 'samplename']]
cc = pd.read_pickle('grahamSiteCoords.p')
dd = bb.merge(aa, left_index=True, right_index=True)
## make sure to merge using the GPD so it stays GPD
grahamDivGPD = cc.merge(dd, right_on='site', left_index=True)
#grahamDivGPD.to_pickle('grahamDivGPD.p')


## now what? We want to check a general north/south trend in 
## biodiversity...

## can we get our y-coords out somehow?

grahamDivGPD['geometry'].y

## so can we regress this against the richness and simpsons?:

stats.linregress(
grahamDivGPD['geometry'].y,
grahamDivGPD['Observed'],
)

## yeah, that is significant, r=.26, so r2 ~ 0.067?

## tomorrow plot this. 

## here is tomorrow. 

fig, ax = plt.subplots()
plt.scatter(grahamDivGPD['geometry'].y, grahamDivGPD['Observed'])

## combine these into points with error

grahamDivGPD

aa = grahamDivGPD.groupby('site').mean()
bb = grahamDivGPD.groupby('site').std()
cc = aa.merge(bb, left_index=True, right_index=True)
cc.columns = ['richness', 'simpson', 'richnessSD', 'simpsonSD']
siteRichness = grahamSites.merge(cc, left_index=True, right_index=True)

plt.close('all')
fig, ax = plt.subplots()
ax.errorbar(x=siteRichness['geometry'].y, 
            y=siteRichness['richness'],
            yerr=siteRichness['richnessSD'],
            fmt='o',
)
## ols line
X = siteRichness['geometry'].y.to_numpy().reshape(-1,1)
Y = siteRichness['richness'].to_numpy().reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), 
        c='k', linewidth=2, #linestyle= "dotted",
        label='OLS linear regression model'
       )
## site labels:
labs = siteRichness.index.to_series()
xy = zip(siteRichness['geometry'].y, siteRichness['richness'])
for i,j in enumerate(xy):
    ax.annotate(labs[i], xy=j)

## so to sum up:

import os
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
from scipy import stats
from sklearn.linear_model import LinearRegression

## we need a siteRichness gpd for each level...

def siteRichnessGPD(gpdDiv):
    ## convert diversity results
    aa = pd.read_csv(gpdDiv)
    aa.columns=['sampleName','Observed','Simpson']
    aa['sampleName'] = aa.sampleName.str.replace('X', '')
    aa.set_index('sampleName', inplace=True)
    ## convert sample data, just to get the site name and host
    bb = pd.read_csv('grahamSample.csv', index_col=0)
    bb = bb[['host','site', 'samplename']]
    ## bring our site geo info again:
    grahamSites = pd.read_pickle('grahamSiteCoords.p')
    ## attach name and host onto the diversity data
    dd = bb.merge(aa, left_index=True, right_index=True)
    ## make a gpdf
    grahamDivGPD = grahamSites.merge(dd, right_on='site', left_index=True)
    del(aa,bb,dd)
    aa = grahamDivGPD.groupby('site').mean()
    bb = grahamDivGPD.groupby('site').std()
    cc = aa.merge(bb, left_index=True, right_index=True)
    cc.columns = ['richness', 'simpson', 'richnessSD', 'simpsonSD']
    siteRichness = grahamSites.merge(cc, left_index=True, right_index=True)
    return(siteRichness)

def plotDiv(gpdRich, tit='', ax=None):
    if ax is not None: ax=ax
    if ax is None: fig, ax = plt.subplots()
    ax.errorbar(x=gpdRich['geometry'].y, 
                y=gpdRich['richness'],
                yerr=gpdRich['richnessSD'],
                fmt='o',)
    ## ols line
    X = gpdRich['geometry'].y.to_numpy().reshape(-1,1)
    Y = gpdRich['richness'].to_numpy().reshape(-1,1)
    ax.plot( X, LinearRegression().fit(X, Y).predict(X), 
            c='k', linewidth=2, #linestyle= "dotted",
            label='OLS linear regression model'
           )
    ## site labels:
    labs = gpdRich.index.to_series()
    xy = zip(gpdRich['geometry'].y, gpdRich['richness'])
    for i,j in enumerate(xy):
        ax.annotate(labs[i], xy=j)
    ## include linear regression results
    corStats = stats.linregress( gpdRich['geometry'].y, gpdRich['richness'])
    corString = (f'Pearson\'s r = {round(corStats[2],3)}, p = {round(corStats[3],3)}')
    top = ax.get_ylim()[1]
    right = ax.get_xlim()[1]
    ax.text(right, top, corString,
            horizontalalignment='right',
            verticalalignment='top')
    ax.set_title(tit)


## using these two functions, plus the above import block, we should
## be able to streamline this process:

gpdDiv_filename = 'diversity7000.csv'
gpdRich = siteRichnessGPD(gpdDiv_filename)
plotDiv(gpdRich, tit=gpdDiv_filename)

fig, axes = plt.subplots(1,2)
axes = np.ravel(axes)
plotDiv(gpdRich, tit=gpdDiv_filename, ax=axes[1])

## great. Let's go back to R and make a pipeline for creating richness data 
## from each level of rarefaction:


##### in R #########

## need the following:

library(phyloseq)
## read in graham's data:
biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')
## get rid of controls
biom97_noCon = subset_samples(biom97, site != 'Control')


## split by host, with a few different read depths:
sampsizes = c(7000, 18000, 30000, 40000)
hosts = c("F. roemeri", "D. californica")

for (j in hosts){
    biom97.i = subset_samples(biom97_noCon, host == j)
    for (i in sampsizes){
        filename = paste(j,"diversity",i,".csv", sep='')
        biom97_rar.i = rarefy_even_depth(biom97.i, sample.size = i, rngseed = 1)
        aa <- estimate_richness(biom97_rar.i, measures=c('Simpson', 'Observed'))
        write.csv(aa, file=filename)
    }
}


#### now try these out in our code above, in python ####

import os

os.listdir()

## using the above functions and import block:

gpdDiv_filenames = ['diversity30000.csv','diversity20000.csv', 'diversity10000.csv', 'diversity7000.csv']

fig, axes = plt.subplots(2,1)

axes = np.ravel(axes)

gpdDiv_filenames = ['diversity7000.csv','diversity20000.csv']

for i,j in enumerate(gpdDiv_filenames):
    gpdRich = siteRichnessGPD(j)
    plotDiv(gpdRich, tit=j)

gpdDiv_filenames = ['diversity7000.csv','diversity20000.csv']

for i,j in enumerate(gpdDiv_filenames):
    gpdRich = siteRichnessGPD(j)
    fig, axes = plt.subplots(2,1, figsize=(8,20))
    plotDiv(gpdRich, tit=j, ax=axes[i])

## yeah, not much difference. 

## let's start a notebook, to keep a sensible record of all this.

## okay, I've lost track of where I'm going with this
## also, we forgot to split up by host. 
## and it looks like we need to keep the latitudes 
## as hosts were sampled from two different sites, 
## with different latitudes

## how can we straighten all this out....

## I think we need to debug the pipeline as I have it to this point
 
## and then redo as above, split by host, etc.

## honestly, I don't think that the slight difference in 
## latitude of the HD sites matters right now, so leave it for
## simplicity's sake.

## so task number one is split the above analysis by host.

## as it is set up right now, simple to do the all-host 
## div-by-lat check:

gpdDiv_filenames = ['diversity7000.csv','diversity20000.csv']

fig, axes = plt.subplots(2,1, figsize=(8,16))

for i,j in enumerate(gpdDiv_filenames):
    gpdRich = siteRichnessGPD(j)    
    plotDiv(gpdRich, tit=j, ax=axes[i])

## but we have to the split before this, the species richness
## calculations are done in R. So back to the beast.  

plt.close('all')

gpdDiv_filenames = [ i for i in os.listdir() if "diversity" in i ]

for i,j in enumerate(gpdDiv_filenames):
    gpdRich = siteRichnessGPD(j)    
    plotDiv(gpdRich, tit=j)

## in general, it looks like Danthonia has a strong N/S diversity
## gradient, but not Festuca. 

## so, what do we do with this? report it in the notebook...

## wait for feedback from Graham and Bitty on that...

## and move on to another task

############## Task #1 trend surfaces for community matrix ######

## let's apply the above tutorial to grahams data, now that we 
## have some familiarity with graham's setup


library('ade4')
library('vegan')
library('phyloseq')

## custom plot function. It is weird that the R plotter has such a 
## hard time with this. It's just a scatter plot with variable radii.

source('/home/daniel/Documents/Books/Stats/numericalEcologyR/NEwR-2ed_code_data/NEwR-2ed_code_data/NEwR2-Functions/sr.value.R')
i## example data 

data('mite')
data('mite.env')
data('mite.xy')


biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')
## get rid of controls
biom97_noCon = subset_samples(biom97, site != 'Control')

danthBiom = subset_samples(biom97_noCon, host == 'D. californica' )
festuBiom = subset_samples(biom97_noCon, host == 'F. roemeri')

danthOTU = otu_table(danthBiom)
festuOTU = otu_table(festuBiom)

## run it through for Danthonia


## try it all with danthonia
## our community matrix is 
## the summed-by-site matrix made below in python

danSiteOTUsums <- read.csv('danSiteOTUsums.csv', row.names='site')

head(danSiteOTUsums, 1)

danSiteOTUsums[1:5,1:5]

rowSums(danSiteOTUsums)

sum(colSums(danSiteOTUsums) == 0) ## 1395 empty columns. Is this a problem?

nonzeroCols <- colSums(danSiteOTUsums) != 0

aa <- danSiteOTUsums[,nonzeroCols]

dim(danSiteOTUsums)
dim(aa)

## does this change the hellinger transformation results?:

danthOTU.h <- decostand(danSiteOTUsums, "hellinger")

danthOTUnonzero.h <- decostand(aa, "hellinger")


rowSums(danthOTU.h)
rowSums(danthOTUnonzero.h)

## zero columns don't affect the numbers resulting from 
## the transformation  
## let's keep going with both, see if there are any down
## stream effects of zero columns. Don't think there should
## be problems, but worth checking.

## we need graham's danthonia sampling scheme as an xy 

grahamSiteCoords = read.csv('grahamSiteCoords.csv', row.names='site')

grahamSiteCoords

dim(danSiteOTUsums)

danSiteOTUsums[1:5,1:5]

danSiteCoords <- grahamSiteCoords[rownames(danSiteOTUsums),]

head(mite.xy)

head(danSiteCoords)

danSiteCoords.c <- scale(danSiteCoords, center=TRUE, scale=FALSE)

danSiteCoords.c

danth.poly <- poly(as.matrix(danSiteCoords.c), degree=3, raw=TRUE)

danth.poly

head(mite.poly)

head(mite.h)

dim(as.data.frame(mite.poly))

dim(mite.h)

as.data.frame(mite.poly)

danthOTU.h

danthOTUnonzero.h

dim(danthOTU.h)

danth.trend.rda <- rda(danthOTU.h ~ ., data=as.data.frame(danth.poly))

R2adj.poly <- RsquareAdj(danth.trend.rda)


R2adj.poly ## r2 = 100%, that's fishy. Although, makes sense, with a million polynomials

## can we pare down the model...
## still following borcard:

## x^3, y^3
danth.poly.ortho3 <- poly(as.matrix(danSiteCoords), degree=3)
danth.poly.ortho3
danth.poly.ortho3.DF <- as.data.frame(danth.poly.ortho3)
exDeg <- colnames(danth.poly.ortho3.DF)
colnames(danth.poly.ortho3.DF) <- c("X", "X2", "X3", "Y", "XY", "X2Y", "Y2", "XY2", "Y3")  
## x^2, y^2
danth.poly.ortho2 <- poly(as.matrix(danSiteCoords), degree=2)
danth.poly.ortho2
danth.poly.ortho2.DF <- as.data.frame(danth.poly.ortho2)
danth.poly.ortho2.DF
colnames(danth.poly.ortho2.DF) <- c("X", "X2", "Y", "XY", "Y2")  
## x^1, y^1
danth.poly.ortho1 <- poly(as.matrix(danSiteCoords), degree=1)
danth.poly.ortho1
danth.poly.ortho1.DF <- as.data.frame(danth.poly.ortho1)
danth.poly.ortho1.DF
colnames(danth.poly.ortho1.DF) <- c("X", "Y")  

danth.poly.ortho2.DF

danth.poly.ortho


danth.trend.rda.ortho2 <- rda(danthOTU.h ~ ., data=as.data.frame(danth.poly.ortho2.DF))

## still 100 explained. but large differences among axes...

R2adj.poly <- RsquareAdj(danth.trend.rda.ortho) ## pointless, but in the example

## do the model selection as above
mod0 <- rda(danthOTU.h ~ 1, data=danth.poly.ortho2.DF)
mod1 <- rda(danthOTU.h ~ ., data=danth.poly.ortho2.DF)

danth.trend.fwd <- ordiR2step(mod0, mod1)

## huh, that doesn't work. 
## says too many terms. 
## makes sense, we've got 5 explanatory terms for six sites...

mod0 <- rda(danthOTU.h ~ 1, data=danth.poly.ortho1.DF)
mod1 <- rda(danthOTU.h ~ ., data=danth.poly.ortho1.DF)
danth.trend.fwd <- ordiR2step(mod0, mod1)

## meh, generally, this approach is not working
## just not enough points

mod1
danth.trend.rda.ortho 

anova.cca(mod1)

anova.cca(danth.trend.fwd)

## what above just a north-south pattern, centered or not?

aa <- rda(danthOTU.h ~ danSiteCoords$X)
anova.cca(aa)

aa <- rda(danthOTU.h ~ danth.poly.ortho3.DF$Y2)
anova.cca(aa)
## well, that explains a fair amount of variance
## and is statistically significance

## how can we visualize?

## I think we need to visualize our sample scheme here:

plot(danSiteCoords)

X = danSiteCoords$X
Y = danSiteCoords$Y

plot(danSiteCoords, asp=1)

sr.value(danSiteCoords, Y)
## this plotting function sucks. Probably need to redo in matplotlib.

aa = rda(danthOTU.h ~ danth.poly.ortho3.DF$Y1)
anova.cca(aa)

aa = rda(danthOTU.h ~ danth.poly.ortho3.DF$Y2)
anova.cca(aa)
## this is the most relevant patter

danth.poly.ortho3.DF$Y2

aa = rda(danthOTU.h ~ danth.poly.ortho3.DF$Y3)
anova.cca(aa)

## so to review:
## y
danthYmodel <- rda(danthOTU.h ~ danSiteCoords$Y)
#danthYmodel <- rda(danthOTU.h ~ danth.poly.ortho3.DF$Y)
anova.cca(danthYmodel)
## y2 
danthY2model <- rda(danthOTU.h ~ danth.poly.ortho3.DF$Y2)
anova.cca(danthY2model)
## y3
danthY3model <- rda(danthOTU.h ~ danth.poly.ortho3.DF$Y3)
anova.cca(danthY3model)

## y2 is the only significant trend. 
## this is probably a reflection of the general north south 
## trend in diversity, coupled with the weirdness in the 
## south of french flat. The shifted parabolla allows 
## a weird southern point (FF), and monotonic increase after
## that due to increasing similarity

This is probably a reflection of the general north south trend in diversity, coupled with the weirdness in the south of french flat. The shifted parabolla allows a weird southern point (FF), and monotonic increase after that due to increasing diversity.

write.csv(danthOTU, file='danthOTU.csv')
write.csv(festuOTU, file='festOTU.csv')

#### python #####

sampleData = pd.read_csv("grahamsSampleData.csv", index_col=0)

aa = pd.read_csv('danthOTU.csv', index_col=0)
## pretty sure we need rows to be sites, columns to be otus


danOTU = aa.T

danOTU.iloc[0:5,0:5]

sampleData.iloc[0:5,0:5]

sampleData.site

sampleData.shape

danOTU.shape

aa = danOTU.merge(sampleData.site, left_index=True, right_index=True)
## we want a site (not sample) by OTU
## we need to get site info on the OTU table, then split and sum the columns. 
danSiteOTUsums = aa.groupby('site').sum()

## this is what we should transform for further analysis

danSiteOTU.to_csv('danSiteOTUsums.csv')

## now repeat for festuca

aa = pd.read_csv('festOTU.csv')
festOTU = aa.T
aa = festOTU.merge(sampleData.site, left_index=True, right_index=True)
festSiteOTUsums = aa.groupby('site').sum()
festSiteOTUsums.to_csv('festSiteOTUsums.csv')

## and back to R, as above. 

## to check polynomial trends in festuca

library('ade4')
library('vegan')
library('phyloseq')

biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')
## get rid of controls
biom97_noCon = subset_samples(biom97, site != 'Control')
festuBiom = subset_samples(biom97_noCon, host == 'F. roemeri')
festuOTU = otu_table(festuBiom)
festSiteOTUsums <- read.csv('festSiteOTUsums.csv', row.names='site')
festOTU.h <- decostand(festSiteOTUsums, "hellinger")
grahamSiteCoords = read.csv('grahamSiteCoords.csv', row.names='site')
festSiteCoords <- grahamSiteCoords[rownames(festSiteOTUsums),]
fest.poly.ortho3 <- poly(as.matrix(festSiteCoords), degree=3)
fest.poly.ortho3.DF <- as.data.frame(fest.poly.ortho3)
colnames(fest.poly.ortho3.DF) <- c("X", "X2", "X3", "Y", "XY", "X2Y", "Y2", "XY2", "Y3")  

festYmodel <- rda(festOTU.h ~ fest.poly.ortho3.DF$Y)
anova.cca(festYmodel)

festY2model <- rda(festOTU.h ~ fest.poly.ortho3.DF$Y2)
anova.cca(festY2model)

festY3model <- rda(festOTU.h ~ fest.poly.ortho3.DF$Y3)
anova.cca(festY3model)

## back up, here is a cleaner version of the danth pipeline:

danthBiom = subset_samples(biom97_noCon, host == 'F. roemeri')
danthOTU = otu_table(danthBiom)
danthSiteOTUsums <- read.csv('danSiteOTUsums.csv', row.names='site')
danthOTU.h <- decostand(danthSiteOTUsums, "hellinger")
grahamSiteCoords = read.csv('grahamSiteCoords.csv', row.names='site')
danthSiteCoords <- grahamSiteCoords[rownames(danthSiteOTUsums),]
danth.poly.ortho3 <- poly(as.matrix(danthSiteCoords), degree=3)
danth.poly.ortho3.DF <- as.data.frame(danth.poly.ortho3)
colnames(danth.poly.ortho3.DF) <- c("X", "X2", "X3", "Y", "XY", "X2Y", "Y2", "XY2", "Y3")

## yeah, not seeing much there. 

## update notebook, start thinking about the scraping problem.



######## task #3 ######

## we need to 

## 1. see what species are shared at FF among the two hosts, and
## 2. get the sequences for these
## 4. automate blast for these
## 3. figure out if there are any interesting ecological stories known 
##    about these fungi

## 3 will require reading over the sources that blast comes up with, 
## also just generally searching the lit for the names that pop up

## step one should be pretty "easy", with phyloseq:

library(phyloseq)

biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')
ffBiom = subset_samples(biom97, site == "French_flat")
festuFFbiom = subset_samples(ffBiom, host == 'F. roemeri')
danthFFbiom = subset_samples(ffBiom, host == 'D. californica')

festuFFOTU = otu_table(festuFFbiom)
festuFFOTUnoZero = festuFFOTU[rowSums(festuFFOTU) != 0]

danthFFOTU = otu_table(danthFFbiom)
danthFFOTUnoZero = danthFFOTU[rowSums(danthFFOTU) != 0]

danthFFOTUnoZero
festuFFOTUnoZero

## we should be able to do an inner join on these:
class(as.data.frame(danthFFOTUnoZero))

#aa = merge(danthOTUnoZero, festuOTUnoZero)
## okay, that was a very bad idea. almost core dumped. 

## do it the old fashioned way: 

dim(danthFFOTUnoZero) ## 843 species

dim(festuFFOTUnoZero) ## 884 species

sum(rownames(danthFFOTUnoZero) %in% rownames(festuFFOTUnoZero))
sum(rownames(festuFFOTUnoZero) %in% rownames(danthFFOTUnoZero))
## 463 shared species among festuca and danthonia at french flat

sharedSppFilter <- rownames(festuFFOTUnoZero) %in% rownames(danthFFOTUnoZero)

dim(danthFFOTUnoZero)

dim(festuFFOTUnoZero)

length(sharedSppFilter)

sharedSpp <- rownames(festuFFOTUnoZero[sharedSppFilter])

for (i in sharedSpp){print(i)}

sink('sharedFFspecies.txt')
    for (i in sharedSpp){cat(i);cat("\n")}
sink()

## great. how can we get the sequences for this?


## graham has kindly supplied the sequence data for his otus:
## in shell what do these look like:
head ../otus_97_uclust.fasta
## I think this is what we want, has unique identifiers
head otus_97_uclust_relabel.fasta 

##### back to python #####

from Bio import SeqIO
import re

## we should be able to query the the sequences using 
## the otu names

## read in the shared otus

## we need a regex I think
## should work for cleaning up both files:
p =  re.compile('(OTU).*(grass)') 

## shared species are small enough it's okay to read it all in
with open('sharedFFspecies.txt', 'r') as sharedOTUs:
    aa = sharedOTUs.readlines()
    cleanedSharedSpp = [ p.match(i).group() for i in aa]

## ugh, biopython, haven't used in a while. How do we read in the 
## fasta file again?

help(SeqIO.parse)

cc = []
bb = SeqIO.parse('../otus_97_uclust_relabel.fasta', "fasta" )
for i in bb: 
    if p.match(i.id).group() in cleanedSharedSpp:
        i.name = p.match(i.id).group()
        i.id = p.match(i.id).group()
        i.description = ''
        cc.append(i)

SeqIO.write(cc, "ffSharedSpp.faa", "fasta")

## did that work? looks right. 

## check it tomorrow. 

## its tomorrow. Just do a few sanity checks

## so, for example, back in R

library(phyloseq)

biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')
ffBiom = subset_samples(biom97, site == "French_flat")
festuFFbiom = subset_samples(ffBiom, host == 'F. roemeri')
danthFFbiom = subset_samples(ffBiom, host == 'D. californica')
festuFFOTU = otu_table(festuFFbiom)
festuFFOTUnoZero = festuFFOTU[rowSums(festuFFOTU) != 0]
danthFFOTU = otu_table(danthFFbiom)
danthFFOTUnoZero = danthFFOTU[rowSums(danthFFOTU) != 0]

## some shared species examples would be:

tail(festuFFOTUnoZero)

festuFFOTUnoZero['OTU2:5grass',]
###########
danthFFOTUnoZero['OTU2:5grass',]
## common in both.

festuFFOTUnoZero['OTU2164:93grass',] ## just one observation

danthFFOTUnoZero['OTU2164:93grass',] ## common, >half sites

## and how species that are definitely not in both: 

NOTsharedFestu <- !(rownames(festuFFOTUnoZero) %in% rownames(danthFFOTUnoZero))
festuFFOTUnoZero[NOTsharedFestu,]
festuFFOTUnoZero['OTU608:16grass']

danthFFOTUnoZero['OTU608:16grass']
danthFFOTU['OTU608:16grass'] ## yup, not in Danthonia

## flip it

NOTsharedDanth <- !(rownames(danthFFOTUnoZero) %in% rownames(festuFFOTUnoZero))

danthFFOTUnoZero[NOTsharedDanth,]

## two examples
OTU3697:100grass
OTU218:38grass


festuFFOTUnoZero['OTU3697:100grass'] ## nope
festuFFOTU['OTU3697:100grass'] ## all zeros

festuFFOTUnoZero['OTU218:38grass'] ## nope
festuFFOTU['OTU218:38grass'] ## all zeros

## okay, looks good to me. sanity checked 

## now we want to know, what are these fungi?

## the database of choice is important...
## should we use UNITE? or genbank?

## this might actually be a time when low quality accessions 
## may be of interest. 

## start with UNITE, I guess. Time to update UNITE. I hate doing this. 

## get it here:

wget https://files.plutof.ut.ee/public/orig/6A/F9/6AF94919CCB48307734D6256CACA50AE1ECBC0839F644D4B661E3673525E41A4.tgz 

## generally can get the latest at this website:
https://unite.ut.ee/repository.php

## we'll keep it here:
uniteFile = ("/home/daniel/Documents/analyses/UNITE/"
            "sh_general_release_s_10.05.2021/"
            "sh_general_release_dynamic_s_10.05.2021.fasta")

## make a local blastn database, in Bash:

ln -s \
/home/daniel/Documents/analyses/UNITE/sh_general_release_s_10.05.2021/sh_general_release_dynamic_s_10.05.2021.fasta \
unite2021.fasta

makeblastdb -in unite2021.fasta -dbtype nucl -logfile dberrors.txt

## now, how get our fungi identified...

blastn -query ffSharedSpp.faa -db unite2021.fasta -out FrenchFlatSharedSpeciesBlast.txt -num_descriptions 10 -num_alignments 3

## at first glance, a lot of these are pretty typical endophytes (epicoccum, cladosporium, etc)

## might be good to narrow this down to endophytes that are only found at french flat, and that are shared.

## also, we should get Rusty Rodriguez's endophytes from stress environment, see if any are present here...

## atcc's strain of Vishniacozyma victoriae comes from Antarctica? 

## anyway, we need a first glance sheet, to see if anything sticks out:


blastn -query ffSharedSpp.faa -db unite2021.fasta -out FrenchFlatSharedSpeciesBlast.txt -num_descriptions 10 -num_alignments 3

blastn -query ffSharedSpp.faa -db unite2021.fasta -out FrenchFlatSharedSpeciesBlastQuickRead.csv -outfmt 10 -max_target_seqs 1

## we want a nice header on that:
sed '1 i\qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' FrenchFlatSharedSpeciesBlastQuickRead.csv -i

## that can be played with in Pandas.

## but before we go there, should we repeat the above pipeline, but subset to just otus that 
## are unique to FF?:

## back in R

library(phyloseq)
biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')

## we want only those otus that are:
## 1) present in both hosts
## 2) present only at french flat. 

## first, get OTUs unique to french flat:
ffBiom = subset_samples(biom97, site == "French_flat")
## but we also need only those species that are in both danth and festu. We saved these
## somewhere...


## our non-FrenchFlat biomtable
nonFFbiom = subset_samples(biom97, site != "French_flat")

ffBiomOTU <- otu_table(ffBiom)

##get rid of zeros
dim(otu_table(ffBiomOTU)[rowSums(otu_table(ffBiomOTU)) != 0]) ## 1264


ffOTUnoZeros <- ffBiomOTU[rowSums(ffBiomOTU) != 0]

colSums(ffOTUnoZeros) ## no zero sites, of course

nonFFbiomOTU <- otu_table(nonFFbiom)

nonFFbiomNoZeros <- nonFFbiomOTU[rowSums(nonFFbiomOTU) != 0]

dim(otu_table(nonFFbiom))

dim(nonFFbiomNoZeros)
## we lose ~200 species when we subset to nonFF sites. What are they?

rownames(nonFFbiomNoZeros)

rownames(ffOTUnoZeros)

## but how do we subset to species found only at FF? and no where else?

## should be this:
uniqFFfilt <- !(rownames(ffOTUnoZeros) %in% rownames(nonFFbiomNoZeros))
uniqFFotu <- ffOTUnoZeros[uniqFFfilt,]

## but got to get out the shared species, which we figured out
## above:

sharedFFspecies <- scan("sharedFFspecies.txt", what='character')

sum(sharedFFspecies %in% rownames(uniqFFotu)) ## only eight species. 

## and they are?
sharedOnlyFFfilt <- rownames(uniqFFotu) %in% sharedFFspecies 
sharedOnlyFF <- uniqFFotu[sharedOnlyFFfilt,]

for (i in rownames(sharedOnlyFF)){
    print(i)
    }

## write it out:
#sink('uniqueFFspeciesSharedBothHosts.txt')
#    for (i in rownames(sharedOnlyFF)){cat(i);cat("\n")}
#sink()

## do we have a blast match for these?

## OTU1879:6grass : no hits
## OTU3919:89grass : Sporormiella maybe?, 96% match
## OTU534:7grass : good hits (99%) to completely unidentified fungi
## OTU284:8grass : good hits (99%) to completely unidentified fungi
## OTU3455:8grass : weak hits (96%) to completely unidentified fungi
## OTU870:9grass : weak hits (93%) to tremellales
## OTU3897:98grass : weak hits (95%) to Rachicladosporium, black rock fungi
## OTU2164:93grass : okay hits (97%) to completely unidentified fungi

## great, so we have a slew of unidentified fungi

## so what now? let's get out the sequences of the unique, shared FF species

## dip into python:

from Bio import SeqIO
import re

p =  re.compile('(OTU).*(grass)') 

#with open('sharedFFspecies.txt', 'r') as sharedOTUs:
with open('uniqueFFspeciesSharedBothHosts.txt', 'r') as sharedOTUs:
    aa = sharedOTUs.readlines()
    cleanedSharedSpp = [ p.match(i).group() for i in aa]


cc = []
bb = SeqIO.parse('../otus_97_uclust_relabel.fasta', "fasta" )
for i in bb: 
    if p.match(i.id).group() in cleanedSharedSpp:
        i.name = p.match(i.id).group()
        i.id = p.match(i.id).group()
        i.description = ''
        cc.append(i)

SeqIO.write(cc, "uniqueFFspeciesSharedBothHosts.fasta", "fasta")

## back to bash 

## try to use rdp or sintax, get some idea of what these are. 

## we'll use Edgar's sintax program. 

## make a database:

ln -s /home/daniel/Documents/analyses/UNITE/sh_general_release_s_10.05.2021/UNITE_usearch.fasta  UNITE_usearch.fasta

usearch11 -makeudb_usearch UNITE_usearch.fasta -output UNITE_usearch.udb

UNITEUS=/home/daniel/Documents/analyses/UNITE/sh_general_release_s_10.05.2021/UNITE_usearch.udb

ls -l $UNITEUS

## let's start with just the OTUs unique to FF, held by both hosts:

usearch11 -sintax uniqueFFspeciesSharedBothHosts.fasta -db $UNITEUS \
 -tabbedout uniqueFFspeciesSharedBothHosts.sintax \
 -strand both -sintax_cutoff 0.8

less uniqueFFspeciesSharedBothHosts.sintax

## not a ton of new information

## great. 

## next steps. 

###############################

## we might want to see if we can quantify ecologically important species 
## at the FF site 

## two strategies come to mind: do an indicator species analysis, 
## and transform/PCA the species matrix, check the loadings on the big axes

## in R

library(phyloseq)
library(indicspecies)

## indicator species:

## need a sites as rows OTU table:
biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')
biom97_noCon = subset_samples(biom97, site != 'Control')

biom97_noCon

help(otu_table)

biomNoConOTU <- t(otu_table(biom97_noCon))

help(multipatt)

## we want to use site as our partitioning variable


sample_data(biom97_noCon)[1:5,1:5]

siteNu = which( colnames(sample_data(biom97_noCon)) == "site")
sitesDF <- sample_data(biom97_noCon)[,siteNu, drop=FALSE]

biomNoConOTU

## can we use this
all(rownames(biomNoConOTU) == rownames(sitesDF))

## great:

sites <- sample_data(biom97_noCon)$site

bothHostIndSpec <- multipatt(biomNoConOTU, sites, func = 'r.g', control=how(nperm=9999))

bothHostIndSpec <- habIndSpp

#save(bothHostIndSpec, "bothHostIndSpec.rda")

load("bothHostIndSpec.rda")

## and that takes forever

## and done.

## interesting, only one of the above unique species for FF appears as an indicator 
## species: OTU1879:6grass

## and it isn't that strong of an indicator. 

## okay, so what do we do with this? can we extract just the FF-associated species?

## look at the blast results for these:

str(bothHostIndSpec)

str(bothHostIndSpec$comb) ## not what we want...

bothHostIndSpec$comb[1:5,]

bothHostIndSpec$comb

## think this is what we need:
bothHostIndSpec$sign

str(bothHostIndSpec$sign)

stat p.value

aa <- bothHostIndSpec$sign

## don't want to fuck with this in R. get out to Pandas

write.csv(aa, file="indSpecResults.csv")

import numpy as np
import pandas as pd
import geopandas as gpd
from Bio import SeqIO
import re

## we want only those OTUS that have a hit for FF, and only FF

indSpecResults = pd.read_csv("indSpecResults.csv", index_col=0)
## those periods are fucking things up. 
indSpecResults.columns = indSpecResults.columns.str.replace('s\.','')
indSpecResults.columns = indSpecResults.columns.str.replace('p\.','p')
## not sure what this is, but index is an important word here in pandas:
indSpecResults.columns = indSpecResults.columns.str.replace('index','inde')

## maybe better said this way:
indSpecResults = pd.read_csv("indSpecResults.csv", index_col=0)
indSpecResults.columns = (indSpecResults.columns
                    .str.replace('s\.','')
                    .str.replace('p\.','p')
                    .str.replace('index','inde'))


## we need to find the all zero rows for the other sites, isolate FF 
aa = indSpecResults.query("French_flat > 0")
bb = aa[["Hazel_Dell","Horse_Rock","Lower_Table","Roxy_Ann","Upper_Table","Upper_Weir","Whetstone","Whidbey"]]
cc = ~bb.any(axis=1)
onlyFF = aa[cc]
onlyFFsig = onlyFF.query("pvalue < 0.05")[["stat","pvalue"]]
onlyFFsig.sort_values(by=['stat','pvalue'], ascending=False, inplace=True)
#onlyFFsig.to_csv('FFonlyIndSpec.csv')

## looks good. Now what?

## check our blast results with these

## or maybe reblast, probably actually easier. 

## need the sequences for these:

FFonlyIndSpecNames = list(onlyFFsig.index.values)
cc = []
bb = SeqIO.parse('../otus_97_uclust_relabel.fasta', "fasta" )
for i in bb: 
    name = re.sub(';size=.*;','',i.name)
    if name in FFonlyIndSpecNames:
        print(name)
        i.name = name
        i.id = name
        i.description = ''
        cc.append(i)

## write out:
#SeqIO.write(cc, "FFindSpecies.fasta", "fasta")

blastn -query FFindSpecies.fasta -db unite2021.fasta -out FFindSpeciesBlast.txt -num_descriptions 10 -num_alignments 5
blastn -query FFindSpecies.fasta -db unite2021.fasta -out FFindSpeciesBlastQuickRead.csv -outfmt 10 -max_target_seqs 1

sed '1 i\qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' FFindSpeciesBlastQuickRead.csv -i

libreoffice --calc FFindSpeciesBlastQuickRead.csv &

## welp, a lot of uncertainty there, but at least there are fewer complete blanks. 
## a whole lot of these point to the same species hypo in UNITE, Colletotrichum graminicola:

Colletotrichum_graminicola|DQ126180|SH2699426.08FU

## so the plan:

## check to see how many of these indicators are in both host species

## group them and make trees - at least colletotrichum, and perhaps the tremallales and preussia groups

##### check indicators for presence in both hosts ######

## back in R

library(phyloseq)
library(indicspecies)

biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')
biom97_noCon = subset_samples(biom97, site != 'Control')

#write.csv(otu_table(biom97), file="biom97otuTable.csv")

FFonlyIndSpec <- as.character(read.csv('FFonlyIndSpec.csv')[,1])

FFonlyIndSpec

## how to check these for membership in both hosts? 

biom97_noCon

head(sample_data(biom97_noCon))

biom97_FF = subset_samples(biom97, site == 'French_flat')

biom97_FF_festu = subset_samples(biom97_FF, host == 'F. roemeri')
biom97_FF_danth = subset_samples(biom97_FF, host == 'D. californica')

aa <- otu_table(biom97_FF_festu)
otu_FF_festu_n0 <- aa[rowSums(aa) != 0,]

aa <- otu_table(biom97_FF_danth)
otu_FF_danth_n0 <- aa[rowSums(aa) != 0,]

##  

inFestu <- FFonlyIndSpec %in% rownames(otu_FF_festu_n0)
inDanth <- FFonlyIndSpec %in% rownames(otu_FF_danth_n0)

FFonlyIndSpec

FFindHosts <- as.data.frame(cbind(inFestu, inDanth), row.names=FFonlyIndSpec)

## how many of these indicators are in both species?
sum(rowSums(FFindHosts) == 2) ## 29
dim(FFindHosts) ## out of 52

## retain these, find out what they are:
aa <- rownames(FFindHosts[rowSums(FFindHosts) == 2,])

sink('FFindSpecBothHosts.txt')
    for (i in aa){cat(i);cat('\n')}
sink()

## now find out what these are, back to python.

aa = pd.read_csv('FFindSpecBothHosts.txt', header=None).iloc[:,0]
FFindSpecBothHostsNames = list(aa)

cc = []
bb = SeqIO.parse('../otus_97_uclust_relabel.fasta', "fasta" )
for i in bb: 
    name = re.sub(';size=.*;','',i.name)
    if name in FFindSpecBothHostsNames:
        print(name)
        print(i.name)
        i.name = name
        i.id = name
        i.description = ''
        cc.append(i)


## write out:
SeqIO.write(cc, "FFindSpeciesBothHosts.fasta", "fasta")

## now blast:

blastn -query FFindSpeciesBothHosts.fasta -db unite2021.fasta -out FFindSpeciesBothHostsBlast.txt -num_descriptions 10 -num_alignments 10
blastn -query FFindSpeciesBothHosts.fasta -db unite2021.fasta -out FFindSpeciesBothHostsBlastQuickRead.csv -outfmt 10 -max_target_seqs 1

sed '1 i\qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' FFindSpeciesBothHostsBlastQuickRead.csv -i

libreoffice --calc FFindSpeciesBothHostsBlastQuickRead.csv &

## so several of the colletotrichums are in both hosts. 
## and several are found in only one host, I therefore deduce
## Are any colletotrichums unique to FF? 
## doesn't look like it. So why are they important? 
## perhaps due to read abundances, which makes me nervous,
## cuz I don't trust these read abundances
## also splitting may have occurred, understating
## the importance of colle
## we need a better overview

## okay, so we need a list of otus that are possible
## colletotrichums, which sites they were found in, 
## and which hosts, and the relative abundances. 

## first, how do we get every OTU that matched to a 
## colletrichum?

## as a wide net, first pass this would be in graham's
## taxonomy table:

aa = tax_table(biom97)
colnames(aa)=c('kingdom','phylum','class','order','family','genus','species')


aa = as.data.frame(aa@.Data)

aa[1:3,1:3]

write.csv(aa, file='grahamTaxonomyTable.csv')

##### let's go to pandas for this

aa = pd.read_csv('grahamTaxonomyTable.csv', index_col = 0)

aa.iloc[0:2,0:6]


bb = pd.Series(['k__Fungi','p__Ascomycota','c__Sordariomycetes','o__Hypocreales','f__Hypocreales_fam_Incertae_sedis','g__Sarocladium'])
bb.str.replace("[kpcofgs]__","") 

aa.query("genus == 'g__Colletotrichum'")

aa.query("genus == 'g__Glomerella'")


## huh, none of these are graminicola. Weird. 

genera = aa.genus.str.replace("[g]__","")

genera == 


for label, content in aa.iteritems():
    #print(label)
    print(content)

## weird. Graham's results don't resemble mine?
## there are no C. graminicola?

## the following top-blasted to colletotrichum graminicola for me, at around 97% identity:

aa.loc['OTU1028:13grass'] ## close, Glomerella in grahams tax table, but no species
aa.loc['OTU1202:12grass'] ## also Glomerella in grahams tax table, no species
aa.loc['OTU1265:13grass'] ## also Glomerella in grahams tax table, no species
aa.loc['OTU1468:13grass'] ## also Glomerella in grahams tax table, no species

## so it seems like maybe genus isn't where to start here, 
## or we include glomerella everywhere we want to look for Colletotrichum, 
## since they are +/- synonyms
## or maybe best, go family level here and assume its all colletotrichum:

glomerellaceaeOTUs = aa.query("family == 'f__Glomerellaceae'")

## how many of these are in our FF indicator species of both hosts
FFindSpecBothHosts = pd.read_csv('FFindSpecBothHosts.txt', header=None).iloc[:,0]


glomerellaceaeOTUs.index 

glomerellaceaeOTUs[glomerellaceaeOTUs.index.isin(FFindSpecBothHosts)]

## yes, all of the colletotrichum indicator species in FF that are shared by both species
## are in there, as Glomerella

## and are all the colletotrichum indicator species in FF, shared or not, in there? 


FFindSpec = pd.read_csv('FFonlyIndSpec.csv', header=None).iloc[:,0]

glomerellaceaeOTUs[glomerellaceaeOTUs.index.isin(FFindSpec)]

glomerellaceaeOTUs.shape ## so maybe 39 colletotrichums in the entire study

glomerellaceaeOTUs[glomerellaceaeOTUs.index.isin(FFindSpec)].shape ## 16 of them are indicators for FF 
## only three are shared?

## the sixteen number fits with our blast results exactly. And all of them blast to 
## to c. graminicola as the closest match. 

## okay, so we're not losing any important sequences by using grahams
## taxanomy with a glomerellaceae query. so pull all of those sequences,
## and any other colletotrichums that pop up in the study

## we need to take a look at the map of these, are they really that unique to the 
## site? Or is this an illumina illusion...I hate relying on abundance data 

## if it seems real, make a tree, see if the site has a unique strain or something. 

## next we need geolocation, are the indicator species really actually localized
## in FF? 

## subset our OTU table to just these indicator colletos:

glomerellaceaeOTUs = aa.query("family == 'f__Glomerellaceae'")

## find these in our OTU table:

biom97otu = pd.read_csv("biom97otuTable.csv", index_col=0)

sampDat = pd.read_csv("grahamSample.csv", index_col=0)


glomerellaceaeOTUs

FFindSpec

grahamDivGPD = pd.read_pickle('grahamDivGPD.p')

## to see where the colletotrichum indicator species are:

glomIndOtu = glomerellaceaeOTUs[glomerellaceaeOTUs.index.isin(FFindSpec)].index.values.tolist()

## get sites for each OTU:
aa = biom97otu.loc[glomIndOtu,:]

## can we collapse this by site?:

bb = aa.T

cc = bb.merge(sampDat.site,left_index=True, right_index=True) 

dd = cc.groupby('site').sum()

dd.T

## well, that's interesting. It does look like the colletotrichum group 
## has something going on at FF. 

## what next? 
## get the sequences and do the trees.

## oh wait, I'm not sure if biom97 is variance stabilized...shit...
## certainly doesn't look like it.

## okay, so back up and invoke the deseq gods:

## we'll try this tutorial, using phyloseq and deseq:
## https://joey711.github.io/phyloseq-extensions/DESeq2.html

library("phyloseq")
library("DESeq2") 
library("indicspecies")

biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')

biom97_Con = subset_samples(biom97, site == 'Control')

biom97_noCon = subset_samples(biom97, site != 'Control')

biom97 

otu_table(biom97_Con)

head(sample_data(biom97 ))

packageVersion("phyloseq")

packageVersion("DESeq2")

## great. I guess our "experimental" design is basically going to be based on 
## site.

## for some reason, this only works if I remove the controls. 
## this probably has to do with Graham's (and my) prepocessing, 
## as 

biome = biom97_noCon
biomStab <- phyloseq_to_deseq2(biome, ~ site)
biomStab <- DESeq(biomStab, test="Wald", fitType="parametric")
## okay, runs when controls are removed.

str(biomStab)

## I don't totally understand this but it looks like the 
## adjusted read counts are all based on a contrast of 
## each site against french flat. I guess the function 
## had to pick one site as the y-intercept for the model?
## And therefore it picked the first category, FF? not sure. 
## not sure it matters, but it seems like that means 
## all the adjusted, modeled counts are relative to FF? 

## still following the example:
res = results(biomStab, cooksCutoff = FALSE)
alpha = 0.05
sigtab = res[which(res$padj < alpha), ]
sigtab = cbind(as(sigtab, "data.frame"), as(tax_table(biome)[rownames(sigtab), ], "matrix"))

head(sigtab)

dim(sigtab)

sigtab

## that's nice, but it's making some decisions for us. 
## as in, it is deciding if the log-fold changes in 
## abundances among sites are statistically significant
## and in the process dropping most of the species

## I'm also still not totally sure that it is just 
## doing this on the basis of the single comparison
## between FF and Whidby. I think I just want to 
## know the data that deseq used to make these decisions.
## long ago Roo wrote a script to help with this,
## I wonder if it is still useful:

deseq.vst = getVarianceStabilizedData(deseq)
# replace negatives with zeros
deseq.vst[deseq.vst <0] <- 0
# add the varience stabilized otu numbers into the dataset:
otu_table(phyloseq) <- otu_table(deseq.vst, taxa_are_rows = TRUE)
# create a new object for the varience stabalized set
phyloseq -> phyloseq.DESeq
# And, filter any taxa that became 0s all the way across
phyloseq.DESeq = filter_taxa(phyloseq.DESeq, function(x) sum(x) > 0.1, T)

## therefore, is it as simple as?:
aa <- getVarianceStabilizedData(biomStab)

?getVarianceStabilizedData

?varianceStabilizingTransformation

## but lots of negative values. Not sure if that's right. Just to check,
## I think things have gotten easier since Roo wrote that. I think 
## you can now just give DESeq2 a matrix of count data, and it will
## give you a matrix of recommended, stabilized counts.

browseVignettes("DESeq2")

## as input, deseq2 is expecting a rows-are-genes, columns-are-samples.

## for us, I think this is analagous to rows-are-OTUs, columns are samples:

head(otu_table(biom97_noCon))

## and this is how phyloseq sets things up (otus are rows)

## so can we just feed deseq the OTU table from phyloseq?

type(otu_table(biom97_noCon)) ## looks like a matrix to me

## so something like this?
aa <- otu_table(biom97_noCon)@.Data
bb <- varianceStabilizingTransformation(aa)

someRows <- c(
            'OTU194:10grass',
            'OTU178:17grass',
            'OTU2287:112grass',
            'OTU1727:23grass',
            'OTU1587:35grass',
            'OTU2071:23grass'
            )


otu_table(biom97_noCon)[someRows,1:10]
bb[someRows,1:10]

## well, that looks promising.
#


otu_table(biom97_noCon)[100:110,1:5]
bb[100:110,1:5]

## just checking, are there any rows that 
## are negative, that are not "-7.24..."?

bb[bb <= 0] 
## yeah. that's confusing. What are the actual 
## counts on these?

zeroVal <- bb['OTU2071:23grass','159grass']


cc <- bb

cc[cc == zeroVal] <- 0

## okay, so the remaining negative values - what were there 
## original count values?

cc['OTU4:5grass','156grass'] ## deseq value is -1.15
otu_table(biom97_noCon)['OTU4:5grass','156grass'] ## it's count value was one

## interesting. other examples?

## our most negative deseq value is ~ -2.5:
 

otu_table(biome)['OTU263:5grass',25]

otu_table(biome)[,25] 

dd <- which(cc == min(cc), arr.ind=TRUE)[,1] ## lots, but all from column 25
## this is sample "29grass", a hazel dell site

otu_table(biom97_noCon)[,25] 

otu_table(biom97_noCon)[dd,25] 

sample_data(biome)["29grass",] ## again, all singletons (count equal 1)

## so generally, after removing the zero values, is a negative value 
## in the deseq results a singleton in the original count data?

## what is the maximum (closest to zero) negative value?

smallNeg <- max(cc[cc < 0]) ## -0.0114

which(cc == smallNeg, arr.ind=TRUE)

length(otu_table(biom97_noCon))

length(cc < 0)

cc[cc < 0]
cc[cc < 0]

cc < 0

str(otu_table(biome))

[cc < 0]

## okay, so to get the big picture:
otu_table(biome)@.Data[cc < 0]


## so I believe that by removing the variance stabilized values 
## under 0 in this dataset, we are losing some observations of 
## 4 counts or less. Small numbers, for sure. And with index
## bleed, who knows how important or not these are. 

## we can't simply add 7 or whatever, this will mess with the 
## relative importance of the OTUs. There is no single value 
## other than the "zero" value that means the same thing among 
## all the samples. We can't simply shift it all up...

## what to do here? Let's follow Roo's lead and just lose
## the negatives. Think of it as a quality control step. 

aa <- otu_table(biom97_noCon)@.Data
bb <- varianceStabilizingTransformation(aa)
bb[bb < 0] <- 0
biom97vs <- biom97_noCon
otu_table(biom97vs) <- otu_table(bb, taxa_are_rows = TRUE)

biom97vs

## any zero rows or columns?

sum(rowSums(otu_table(biom97vs)) == 0) ## 194


biom97vs = filter_taxa(biom97vs, function(x) sum(x) > 0.1, T)

biom97vs
biom97

## any sites? seems unlikely
sum(rowSums(otu_table(biom97vs)) == 0) ## 0

## okay. With these new numbers, how do the indicator species 
## look now?

siteNu = which( colnames(sample_data(biom97vs)) == "site")

sitesDF <- sample_data(biom97vs)[,siteNu, drop=FALSE]

biomOTU <- t(otu_table(biom97vs))


all(rownames(sitesDF) == rownames(biomOTU))


sites <- sample_data(biom97vs)$site

## great, so now rerun the indicator spec analysis:

bothHostIndSpecVS <- multipatt(biomOTU, sites, func = 'r.g', control=how(nperm=5000))

#save(bothHostIndSpecVS, file='bothHostIndSpecVS.rda')

#sink(file='resultsBothHostIndSpecVS.txt')
    summary( bothHostIndSpecVS)
#sink()

## so how does this compare to our previous results?

load("bothHostIndSpec.rda")



indSpecOTUsFF_VS <- scan('indSpecFF_VS.txt', what='character')

indSpecOTUsFF_VS 

## get a new OTU table for our variance stabilized results:


#write.csv(otu_table(biom97vs), file="otuTable_vs.csv")

## interesting, we find more indicator species after variance 
## stabilization. 

## hmm, probably a denoising result, we just got rid of a lot of 
## low-abundance OTUs. 

## now sure how to feel about that. But let's go with it, 
## I think it is justifiable, or more justifiable than 
## the raw read stance, knowing what I know about this 
## sequencing run. 

## what's the goal here? 
## See if the same species are important

## look for species that are only found at french flat
## see if colletotrichum remains the most important
## if so, see if colletotrichum 

## larger goal is to get Bitty and Graham some species to
## talk about, maybe hand off to them to o the lit search, etc.

## anyway, first step, get some blast matches to these, 
## in python:

import numpy as np
import matplotlib.pyplot as plt; plt.ion()
import pandas as pd
import geopandas as gpd
from Bio import SeqIO
from Bio import Phylo
from Bio import Entrez
import re

p =  re.compile('(OTU).*(grass)') 

with open('indSpecFF_VS.txt', 'r') as indSpp:
    aa = indSpp.readlines()
    cleanedNames = [ p.match(i).group() for i in aa]


cc = []
bb = SeqIO.parse('../otus_97_uclust_relabel.fasta', "fasta" )
for i in bb: 
    if p.match(i.id).group() in cleanedNames:
        i.name = p.match(i.id).group()
        i.id = p.match(i.id).group()
        i.description = ''
        cc.append(i)

SeqIO.write(cc, "indSpp_vs.fasta", "fasta")

## now blast these in bash

blastn -query indSpp_vs.fasta -db unite2021.fasta -out indSpp_vsBlastLong.txt -num_descriptions 10 -num_alignments 10

blastn -query indSpp_vs.fasta -db unite2021.fasta -out indSpp_vsBlastQuickRead.csv -outfmt 10 -max_target_seqs 1

## only ~27 blasted to anything. 

sed '1 i\qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' indSpp_vsBlastQuickRead.csv -i

libreoffice --calc FFindSpeciesBothHostsBlastQuickRead.csv &

## still more colletotrichum than anything else. Okay, so what now? 

## how unique are these colletotrichums to French Flat?

## bring up graham's tax table and sample_data again...

## python

taxtab = pd.read_csv('grahamTaxonomyTable.csv', index_col = 0)

taxtab.query("genus == 'g__Colletotrichum'")

taxtab.query("genus == 'g__Glomerella'")

taxtab.query("family == 'f__Glomerellaceae'")


taxtab.loc[cleanedNames,:].genus

taxtab.loc[cleanedNames,:].genus.isna().sum() ## 30 unidentified genera. Lots.


cleanedNames  

glomOTUs = taxtab.query("family == 'f__Glomerellaceae'").index.tolist()

## we want to get the indicator species that matched to Glomerellaceae:


glomFFindSpp = [i for i in cleanedNames if i in glomOTUs] ## 12, all blasting at 97% to c. graminicola in the new UNITE data

taxtab.loc[glomFFindSpp] ## and all glomerella in the old database

## and which of these are in both hosts?

## get our other two tables
sampleData = pd.read_csv('grahamSample.csv', index_col=0)
otuTable_vs = pd.read_csv('otuTable_vs.csv', index_col=0)

## subset otuTable to our glomerellaceae:
aa = otuTable_vs.loc[glomFFindSpp]
## subset to FF:


sampleData.site

ffFestuSites = sampleData.query("(site == 'French_flat') and (host == 'F. roemeri')").index.tolist()
ffDanthSites = sampleData.query("(site == 'French_flat') and (host == 'D. californica')").index.tolist()

## subset our otuTable further with these:

bb = aa.loc[:,ffFestuSites] ## lots

bb[bb.sum(axis=1) != 0]

cc = aa.loc[:,ffDanthSites] ## not lots
cc[cc.sum(axis=1) != 0] ## OTU1265:13grass is the only shared colletotrichum
## also the only colletrichum that appears in our FF site at all. 


## great, so which of our glomerellaceous FF indicator species are in both Danthonia and Festuca?

## just:
## OTU1265:13grass

## but they all blast to same damn thing. It is indeed time to do a tree. 

## give it a rest? Or charge on and make some trees?

## ned to get ready for the meeting to tonight...

#### make a tree of the colletotrichums ######

## okay tomorrow is today...

## for visualizing, biopython has some nice builtins

## following the example here: http://biopython.org/DIST/docs/tutorial/Tutorial.html#sec253

## a newick format tree is here:

(((A,B),(C,D)),(E,F,G));

## this is saved as "simple.dnd"

tree = Phylo.read('simple.dnd', 'newick')

Phylo.draw_ascii(tree)

tree.rooted = True
## this doesn't seem to make a difference to the plotter.
tree.rooted = False

## or with matplotlib:
Phylo.draw(tree)

## or with the oranges example from here:  
## https://bioclimate.commons.gc.cuny.edu/bioinformatics/sequence-alignment-and-tree-building/

tree = Phylo.read('/home/daniel/Desktop/orange/oranges.nwk', 'newick')
Phylo.draw(tree)

## to get our tree, I guess I'll be using phyml, which is 
## included in the anaconda distribution...

## I have conda but haven't thought about it in a while...

## anyway, doesn't matter, because phyml seems to be a standalone
## can get it with apt. 

## okay, so let's try some of our sequences...

## what sequences do we want? I think we want all glomellaceae sequences,
## ultimately colored by their status as an indicator of FF, and 
## which hosts they are in. 

## we might also want some other colletotrichums, like the ones that 
## Rusty Rodriguez looked at.

## to start, get all the glomelleraceous sequences (I think they are 
## all colletotrichums)



## we want to make note of: 

## all glomerellaceae OTUs and then
## ones that are FFindicators and
## ones that are in danth or 
## ones that are in festu or 
## ones that are in both

## so for each OTU we need to find the sites it was observed in,
## then look up the host for those sites, 
## then categorize our results as either danth, festu, or both

taxtab = pd.read_csv('grahamTaxonomyTable.csv', index_col = 0)

otutab = pd.read_csv('otuTable_vs.csv', index_col = 0)

sampleData = pd.read_csv('grahamSample.csv', index_col=0)

## to get the glomerellaceous taxa:
glomOTUs = taxtab.query("family == 'f__Glomerellaceae'").index.tolist()

## to get which of these are FF indicators:
indSpecFF_VS = (pd.read_csv('indSpecFF_VS.txt', header=None)
                .iloc[:,0]
                .str.strip())

## how to get host again?
## first get samples:
aa = glomOTUs[30]
bb = otutab.loc[aa,:]
cc = bb[bb > 0]
dd = cc.index.tolist()
ee = sampleData.loc[dd].host.unique().tolist()
ff = hostClassif(ee)


## that can be a function. But first, let's clean out the 
## funky Colletotrichum we found below (search "ditch it")

glomOTUs = taxtab.query("family == 'f__Glomerellaceae'").index.tolist()
glomOTUs = [ i for i in glomOTUs if i != 'OTU2257:136grass' ]

def getHosts(otu):
    bb = otutab.loc[otu,:]
    cc = bb[bb > 0]
    dd = cc.index.tolist()
    ee = sampleData.loc[dd].host.unique().tolist()
    ff = hostClassif(ee)
    return(ff)

## need one for just FF hosts
def getFFHosts(otu):
    bb = otutab.loc[otu,:]
    cc = bb[bb > 0]
    FFsites = sampleData.query("site == 'French_flat'").index.tolist()
    dd = cc[cc.index.to_series().isin(FFsites)].index.tolist()
    ee = sampleData.loc[dd].host.unique().tolist()
    ff = hostClassif(ee)
    return(ff)

def hostClassif(hosts):
    if 'F. roemeri' in hosts and 'D. californica' in hosts:
        label = "bothHosts"
    elif 'F. roemeri' in hosts and 'D. californica' not in hosts:
        label = "festu"
    elif "D. californica" in hosts and 'F. roemeri' not in hosts:
        label = "danth"
    else:
        label = "NoHost"
    return(label)

## also might be useful:

def getHostandSites(otu):
    bb = otutab.loc[otu,:]
    cc = bb[bb > 0]
    dd = cc.index.tolist()
    ee = sampleData.loc[dd,:].host
    return(ee)

getHosts(glomOTUs[10])

getHostandSites(glomOTUs[10])

## seems to work okay:

[ getHostandSites(i) for i in glomOTUs ]

## like a lot more "bothhosts" happened than I would expect. 


sharedallSitesDF = pd.DataFrame.from_dict(
                    {'otu': glomOTUs,
                    'cleanedName': pd.Series(glomOTUs).str.replace(":[0-9]+grass", ""),
                    'generalHost': [ getHosts(i) for i in glomOTUs ],
                    'inFF'  : [ any(getHostandSites(i).index.to_series().isin(FFsites)) for i in glomOTUs ],
                    'indSpp': [ i in indSpecFF_VS.values for i in glomOTUs ],
                    'FFhost': [ getFFHosts(i) for i in glomOTUs ],
                   })


#sharedallSitesDF.to_csv('sharedallSitesDF.csv')

sharedallSitesDF.otu.str.replace(":[0-9]+grass", "")

## huh, I did retain OTU1265 after all, thought I lost it:
sharedallSitesDF.query("indSpp == True and FFhost == 'bothHosts'")

## in general I think we should a "build up" approach here, and build the 
## descriptor string as a series of true conditions.

## condition 1: is the OTU in FF?
## condition 2: if so, is the OTU an indicator of FF?
## condition 3: also, if it is at FF, what is/are its hosts at FF? 
## condition 4: What are its hosts generally (not just at FF)?

## condition 1: is the OTU in FF? 

[ any([ i in FFsites for i in getHostandSites(j).index.tolist() ]) 
        for j in glomOTUs ]

any([ i in FFsites for i in getHostandSites(j).index.tolist() ])

any([ i in FFsites for i in getHostandSites(glomOTUs[10]).index.tolist() ])

FFsites = sampleData.query("site == 'French_flat'").index.tolist()

FFsitesFestu = sampleData.query("site == 'French_flat' and host == 'F. roemeri'").index.tolist()
FFsitesDanth = sampleData.query("site == 'French_flat' and host == 'D. californica'").index.tolist()

glomOTUs[30]

glomOTUs[10]

## actually, better solution below:

## condition 3: also, if it is at FF, what is/are its hosts at FF? 


getHostandSites(glomOTUs[10]).index.to_series().isin(FFsites)

aa = [ any(getHostandSites(i).index.to_series().isin(FFsites)) for i in glomOTUs ]

[ getFFHosts(i) for i in glomOTUs ]

## okay, so how can we turn this into an FF-specific host

otu = glomOTUs[10]

getHosts(otu)

[ getHostandSites(i).index.to_series().isin(FFsites) for i in glomOTUs ]

FFsites

## great, so we have a good data frame summarizing the important labels


otu = 'OTU2257:136grass' ## should be only danthonia
otu = 'OTU3169:13grass'

getHostandSites(otu).values

getHostandSites(otu).index

sampleData.query("site == 'French_flat'").host

FFsitesFestu = sampleData.query("site == 'French_flat' and host == 'F. roemeri'").index.tolist()
FFsitesDanth = sampleData.query("site == 'French_flat' and host == 'D. californica'").index.tolist()

## so my memory is that only one colletrichum OTU was found in 
## both hosts at FF, as an indicator species for FF:

j=0
aa = getHostandSites(glomOTUs[j]).index.tolist()
bb = any([ (i in FFsitesFestu and i in FFsitesDanth) for i in aa ])

for j in glomOTUs:
    aa = getHostandSites(j).index.tolist()
    bb = ([ (i in FFsitesFestu and i in FFsitesDanth) for i in aa ])
    print(j)
    print(bb)
    print(any(bb))

for j in glomOTUs:
    aa = getHostandSites(j).index.tolist()
    bb = ([ (i in FFsitesFestu and i in FFsitesDanth) for i in aa ])
    if any(bb): print(j)


## really getting lost here. I could have sworn there was a shared indicator 
## OTU. But not finding it. 

## I thought I found:
otu = 'OTU1265:13grass'
## in both

aa = getHostandSites(otu).index.tolist()

[ (i in FFsitesFestu) for i in aa ]
[ (i in FFsitesDanth) for i in aa ] 

[ (i in FFsites) for i in aa ] 

## my brain is scrambled

## is it still an indicator?

otu in indSpecFF_VS.values

indSpecFF_VS.values

## yeah

[ i for i in aa ]

any([ (i in indSpecFF_VS.values) for i in aa ])


## now out of date, defined elsewhere
#sharedallSitesDF = pd.DataFrame.from_dict(
#                    {'otu': glomOTUs,
#                    'host': [ getHosts(i) for i in glomOTUs ],
#                    'FFindSpp': [ i in indSpecFF_VS.values for i in glomOTUs ],}
#                    )
# 

sharedallSitesDF.query("FFindSpp == True")

indSpecFF_VS

## really, we were looking for colletos that were shared only 
## at French Flat. 

## no strictly shared OTUs, but numerous blasts to basically the 
## same species complex

## okay, so where does that leave us?

## I think it is time to build a tree, to 
## really see if these colletotrichums
## cluster together


## for cleaning up names:
p =  re.compile('(OTU).*(grass)') 

cc = []
bb = SeqIO.parse('../otus_97_uclust_relabel.fasta', "fasta" )
for i in bb: 
    if p.match(i.id).group() in glomOTUs:
        na = p.match(i.id).group()
        i.name = na
        i.id = na
        i.description = (f'{getHosts(na)}_{na in indSpecFF_VS.values}')
        cc.append(i)

#SeqIO.write(cc, "glomSeqsFromGrahamData.fasta", "fasta")

## great, those should be the sequences from our study that we want to look at.


## we'll also want some outside sequences

## make this tree, update notebook with indicator species for Graham and bitty to 
## look at, then give the graham project 

## what outside sequences do we want? we get a couple random colletrichums from 
## unite? 

## I think we want to see if we can track down the colletotrichums in 
## table 2 of Singh 2011, "Unraveling the role of fungal symbionts in plant
## abiotic stress tolerance", which is pretty much a summary of 
## Redman and Rodriguez's work.


## are sequences from these publically available?
## doesn't seem like they reported the its from 
## their particular strains, but genbank purports
## to have the type specimens:

## Colletotrichum gloeosporioides:
NR_160754
## Colletotrichum magnum
NR_160836
## Colletotrichum musae
NR_120132
## Colletotrichum orbiculare
NR_152271

## can we get these and strip down to ITS1 region?
idList = ['NR_160754', 'NR_160836', 'NR_120132', 'NR_152271']
Entrez.email = "danchurchthomas@gmail.com"  
handle = Entrez.efetch(db="nucleotide", id=idList, rettype="fasta")
## add these to our collection of sequences from the study
for seq_record in SeqIO.parse(handle, "fasta"): cc.append(seq_record)

## and some seqs from Crouch et al, which is what a lot of these colletos blast to.
## but which ones?

## they identify two major lineages, one which infects corn (C. graminicola sensu stricto),
## and C. cereale for former C. graminicolas that infect pooideae grasses. 

## btw are Danthonia and Festuca in pooideae?

## Festuca is, but Danthonia is in...wait for it...Danthonioideae (according to wikipedia)

## which would sort of fit with the wandering host situation described by Redmen, Rodriguez, etc:
## there are hosts for which a Colleto is pathogenic, and there are non-disease hosts, for the same
## strain of Colleto

## it might be good to get these from UNITE, as they will have 
## likely reviewed the 100 or sequences and lumped them into 
## their own species hypothesis?

## anyway, some strains from Crouch:
## a couple of C. graminicola sensu stricto:

grep "DQ1[0-9]\{5\}" unite2021.fasta 

grep "DQ126[0-9]\{3\}" unite2021.fasta | wc -l ## 21, includes graminicolas plus one C. sublineola
## only 20? and only graminicola? where are the cereales?

grep "cereale" unite2021.fasta ## not colletos

grep ">Glomerella_" unite2021.fasta ## finds a few species, including one likely from the Crouch study
## but don't feel like following that up

grep "DQ132[0-9]\{3\}" unite2021.fasta ## nada. 
## presumably these were the cereale sequences

grep "DQ126[0-9]\{3\}" unite2021.fasta ## the graminicolas. 
## okay, but which ones to use? They have little metadata, like 
## what strain they came from. 

## is the sequence the same as the NCBI record sequence?
grep DQ126180 unite2021.fasta -A 1
## looks like it. so just grab these from genbank with the others

## Glomerella graminicola strain MO-178
DQ126254
## Glomerella graminicola strain IN-300170
DQ126248


## and a couple of C. cereale sensu stricto:

## it looks like we can't get the exact organisms we want from UNITE:
grep "Colletotrichum_cereale" unite2021.fasta ## nada. 
grep "cereale" unite2021.fasta ## no colletotrichums...
## These must have been absorbed into some other project's sequences, or other taxa.
## funny, I wonder how that decision was made. someone doesn't like that 
## proposed species. Anyway, let's get them from gene bank:

## Colletotrichum cereale strain CA-62
DQ126158
## Colletotrichum cereale strain NY-16
DQ126226
## Colletotrichum cereale strain NJ-4990
DQ126219

## it might be because these are only partial for the spacer region.
## they were using the 5.8 gene for their phylogeny, I guess.
## maybe the other sequences, that actually were in UNITE are full ITS,
## and that is why they were included?
## anyway, can't find full ITS1, so try these


## and we also want a rooting sequence from the Reblova, Gams, Seifert 2011
## paper

## I think figure 4 shows us what we want. Something from Australiascaceae

## PRM 915720 Australiasca laensis GU180624
## or 
## DAOM 226788 Australiasca laensis GU180623

## are these in unite?

grep GU18062. unite2021.fasta ## not colletos

## nope. but the Monilochaetes guadalcanalensis is, could use this...

## so weird. Can you get any more authoritative than Keith Seifert?
## he literally wrote the book...

## anyway, grab from Genbank:


## Australiasca laensis 
GU180623

## so make a list of all the above outside sequences that 
## we need from genbank


## Colletotrichum gloeosporioides:
NR_160754
## Colletotrichum magnum
NR_160836
## Colletotrichum musae
NR_120132
## Colletotrichum orbiculare
NR_152271
## Glomerella graminicola strain MO-178
DQ126254
## Glomerella graminicola strain IN-300170
DQ126248
## Colletotrichum cereale strain CA-62
DQ126158
## Colletotrichum cereale strain NY-16
DQ126226
## Colletotrichum cereale strain NJ-4990
DQ126219

## Australiasca laensis (root)
## GU180623
## Never mind! It fucks up 

glomOTUs = taxtab.query("family == 'f__Glomerellaceae'").index.tolist()
glomOTUs = [ i for i in glomOTUs if i != 'OTU2257:136grass' ]
p =  re.compile('(OTU).*(grass)') 
cc = []
bb = SeqIO.parse('../otus_97_uclust_relabel.fasta', "fasta" )
for i in bb: 
    if p.match(i.id).group() in glomOTUs:
        na = p.match(i.id).group()
        i.name = na
        i.id = na
        i.description = (f'{getHosts(na)}_{na in indSpecFF_VS.values}')
        cc.append(i)


#### and starting over:

glomOTUs = taxtab.query("family == 'f__Glomerellaceae'").index.tolist()
glomOTUs = [ i for i in glomOTUs if i != 'OTU2257:136grass' ]
p =  re.compile('(OTU).*(grass)') 
cc = []
bb = SeqIO.parse('../otus_97_uclust_relabel.fasta', "fasta" )
for i in bb: 
    if p.match(i.id).group() in glomOTUs:
        na = p.match(i.id).group()
        i.name = na
        i.id = na
        i.description = desc
        cc.append(i)

genbankSeqs = [
    'NR_160754','NR_160836','NR_120132',' NR_152271',
    'DQ126254','DQ126248','DQ126158','DQ126226',
    'DQ126219']#,'GU180623']


idList = genbankSeqs
Entrez.email = "danchurchthomas@gmail.com"  
handle = Entrez.efetch(db="nucleotide", id=idList, rettype="fasta")
## add these to our collection of sequences from the study
for seq_record in SeqIO.parse(handle, "fasta"): cc.append(seq_record)
#SeqIO.write(cc, "allSeqsForGrahamTree.fasta", "fasta")

## look at these in ugene, come back

## well, it is easy to see where to trim the sequences from genbank
## for ITS1 region. Do we need to?

## also looks like the Australiasca laensis root idea didn't work out. 
## I guess we need something else from the genus, ITS is too wonky

## tomorrow - 
## root tree with a sequence outside Glomerellaceae from Reblova, Gams, Seifert 2011
## paper. 
## add in some represntative C. graminicola sequences from Crouch 2006 paper.
## correct issue with shared hosts at French Flat. There should only be one 
## shared species?

######## side note, fucked up OTU ##########
## OTU2257:136grass doesn't really fit in the other sequences at all

## did this really blast to a colletotrichum?

## get this sequence out:

OTU2257 = [ i for i in cc if i.id == 'OTU2257:136grass' ]
#SeqIO.write(OTU2257, "OTU2257.fasta", "fasta")

## in bash
blastn -query OTU2257.fasta -db unite2021.fasta -out OTU2257BlastLong.txt -num_descriptions 10 -num_alignments 10

## huh, nope, that doesn't blast to shit

## for some reason, the taxonomy table has it as glomerella?:

taxtab.head()

taxtab.loc['OTU2257:136grass',:]

## graham has it identified down to species, with an accession number...

## maybe grab that from UNITE? 

## but not that important, methinks. Just ditch it? It's not an indicator

## how frequent is it?

otutab.head()

OTU2257ser = otutab.loc['OTU2257:136grass',:]

OTU2257ser.sum() 

OTU2257ser[OTU2257ser > 0] ## one observation, in its namesake of 136grass
## which is hazel dell, ditch it.

########## end side note of fucked up otu ###############

## tree coming from the ITS data
plt.close('all')

#tree = Phylo.read('glomSeqsFromGrahamData.nwk', 'newick')
#tree = Phylo.read('allSeqsForGrahamTree.phylip_phyml_tree.txt', 'newick')

tree = Phylo.read('grahamAllSeqsTreeRelabeled.nwk', 'newick')
Phylo.draw(tree)

plt.title("only Graham's colletos")

tree = Phylo.read('allSeqsForGrahamTree.nwk', 'newick')
Phylo.draw(tree)
plt.title('alignment untrimmed')

tree = Phylo.read('allSeqsForGrahamTree_trimmed.nwk', 'newick')
Phylo.draw(tree)
plt.title('alignment trimmed to ITS1')

Phylo.draw(tree)
Phylo.draw(tree)


## great, so what's the plan?

## reset all the above to be done on the command line somehow?

## figure out the branch support metrics 

## trim the outside sequences, see if it matters

## get rid of non-colleto sequence, not useful

## for tree labeling:
## change true/false to IndSpp or blank
## clip samplenames from otus
## add "FF" if present there
## if at FF, add name of host at FF?

## then update jupyter, await further instructions from 
## Bitty and Graham

## do the above:

### 1. get rid of non-colleto sequence, not useful - done

### 2. trim the sequences outside ITS1 from the alignment, see if it matters
## how long are grahams sequences?
## go above, reconstruct just grahams seqs in "cc", check lengths:
[ len(i.seq) for i in cc ]
## all 178 BP

## redoing all three trees (without outside seqs, with but untrimmed, and with, trimmed)
## to use bootstrap method for confidence values on branches
## trimmed tree built with 100 bootstrap cycles
## using HKY85 substitution model, have no idea if this appropriate...

### 3. for tree relabeling
## change true/false to IndSpp or blank
## clip samplenames from otus
## add "FF" if present there
## if at FF, add name of host at FF?

## we have a nice dataframe made above, for labels for 
## our glomerellaceaous OTUs of interest:

sharedallSitesDF

sharedallSitesDF.head()

## one label might look like:

aa = sharedallSitesDF.iloc[0,:]

aa = sharedallSitesDF.iloc[1,:]

aa = sharedallSitesDF.iloc[1,:]

sharedallSitesDF.loc[otu,:]

## have to set index on shareddf
#sharedallSitesDF.set_index('otu', drop=True, inplace=True)

## redefined below, don't use this
###def makeLabel(otu):
#    """otu is an otu from glomOTUs"""
#    sharedf = sharedallSitesDF.set_index('otu', drop=True)
#    aa = sharedf.loc[otu,:]    
#    ## clean name:
#    na = re.sub(":[0-9]+grass","", aa.name)
#    ## generalHost
#    gh = aa.generalHost
#    ## FF presence
#    if aa.inFF: inFF='.FF' 
#    else: inFF = ''
#    ## indSpp
#    if aa.indSpp: indSpp='+++' 
#    else: indSpp = ''
#    ## FFhost 
#    if inFF: FFhost = ('.' + aa.FFhost)
#    else: FFhost = ''
#    ## make label, for now leave out the name:
#    glomTipLabel = (f'{gh}{inFF}{FFhost}{indSpp}')
#    return(glomTipLabel)


makeLabel('OTU2526:13grass')


sharedallSitesDF

sharedallSitesDF.head()

##  great, so how to use this? 
## we want to relabel our glom sequence Fasta file:

taxtab = pd.read_csv('grahamTaxonomyTable.csv', index_col = 0)
otutab = pd.read_csv('otuTable_vs.csv', index_col = 0)
sampleData = pd.read_csv('grahamSample.csv', index_col=0)


glomOTUs = taxtab.query("family == 'f__Glomerellaceae'").index.tolist()
glomOTUs = [ i for i in glomOTUs if i != 'OTU2257:136grass' ]

p =  re.compile('(OTU).*(grass)') 
cc = []
bb = SeqIO.parse('../otus_97_uclust_relabel.fasta', "fasta" )
for i in bb: 
    if p.match(i.id).group() in glomOTUs:
        na = p.match(i.id).group()
        i.name = re.sub(":[0-9]+grass","", na)
        i.id = i.name
        #i.description = makeLabel(na) ## too complex
        i.description = ''
        cc.append(i)

#SeqIO.write(cc, "glomSeqsFromGrahamData.fasta", "fasta")

genbankSeqs = [ 'NR_160754','NR_160836','NR_120132',' NR_152271',
    'DQ126254','DQ126248','DQ126158','DQ126226',
    'DQ126219']#,'GU180623']


idList = genbankSeqs
Entrez.email = "danchurchthomas@gmail.com"  
handle = Entrez.efetch(db="nucleotide", id=idList, rettype="fasta")
## add these to our collection of sequences from the study
for seq_record in SeqIO.parse(handle, "fasta"): cc.append(seq_record)

#SeqIO.write(cc, "allSeqsForGrahamTree.fasta", "fasta")

## now? rerun trees, see if they are pretty:

## how common/abundant is OTU1265? does it partially explain the 
## similarity of FF hosts?


otuTable_vs.loc['OTU1265:13grass'].to_list()

aa = otuTable_vs.loc['OTU1265:13grass']
bb = aa[aa > 0]

otuTable_vs.max().max() ## the maximum value for the whole otu table is ~20, and this OTU is 
## up to 13 at one site, and often over 10. So probably pretty important

 
sampleData.query("site == 'French_flat' and host == 'F. roemeri'").site
sampleData.query("site == 'French_flat' and host == 'D. californica'").site

bb

## so that was generally interesting side project, but we still 
## have no answer as to why those two sites converged?

## probably doesn't have much to do with the colletotrichums, 
## they are mostly in the festuca. 

## It is likely more to do with the species unique to the site. 
## Check their abundances. 

## this will have to wait.

## on monday:
## 1. generate the nice trees, with trimmed alignments,
## both with and without the outside sequences.
## 2. update notebook with the colleto story (will probably take all day)
## 3. then do your presentation

## and then see what you have time for after this. 


## starting over a bit, how can we repeat our ugene steps programmatically?

## command line muscle alignment of just the graham sequences:

fafi=glomSeqsFromGrahamData.fasta

muscle -in $fafi -out glomSeqsFromGrahamData.alni -clw

## this works better:
muscle -in $fafi -out glomSeqsFromGrahamData.alni -clwstrict

## but I think we need phylip format?

muscle -in $fafi -out glomSeqsFromGrahamData.phylip -phyi

phyml -i glomSeqsFromGrahamData.phylip

## and weird errors about the expected length of 
## OTUs results. Must be some sort of file formatting
## issue.

## I'm betting it's an issue with the spaces in the labels
## so a test file, take spaces out of labels for first OTU:

phyml -i test.phylip 

## yup, problem bumps to second OTU.

## ok, so for the moment, keep the labels simple

## start over, with all seqs, grahams and genbank

fafi=allSeqsForGrahamTree.fasta
muscle -in $fafi -out allSeqsForGrahamTree.phy -phyi

## here we have to trim out the non-ITS1 basepairs, doing this in 
## ugene. Trimming out bps 1-58, and 302-667

aliview allSeqsForGrahamTree.phy

## ugene did not work. Used aliview instead, which offered to pad 
## the sequences...fishy, why was this necessary?

phyml -h

phyml -i allSeqsForGrahamTree_trimmed.phy -b 100

## did that work?

with open ('allSeqsForGrahamTree_trimmed.phy_phyml_tree.txt', 'r') as f:
    tree = f.readlines()[0]

sharedallSitesDF = pd.read_csv('sharedallSitesDF.csv', index_col=0)

otuName =  re.compile('(OTU[0-9]+):')

otusInTree = otuName.findall(tree)

## look up this match in our tables, print a new label
## create a label:

def makeLabel(otuInTree):
    """ get labels out of the phyml results and give them more 
    information. Also need the sharedAllSitesDF dataframe"""
    sharedf = sharedallSitesDF.set_index('cleanedName', drop=True)
    info = sharedf.loc[i]
    ## generalHost
    gh = info.generalHost
    ## FF presence
    if info.inFF: inFF='.FF' 
    else: inFF = ''
    ## indSpp
    if info.indSpp: indSpp='*' 
    else: indSpp = ''
    ## FFhost 
    if inFF: FFhost = ('.' + info.FFhost)
    else: FFhost = ''
    glomTipLabel = (f'{i}.{gh}{inFF}{FFhost}{indSpp}')
    return(glomTipLabel)

aa = tree
for i in otusInTree:
    ## find it in a:
    aa = aa.replace(i, makeLabel(i))

## we need to map the external read names to something
## more informative:

## write that back out, see it plots okay:


## try this with our tree drawing code above



## looks good. We'll also need to give more information to the genbank sequences. 

## the place to do this is in the Newick file:

genbankDict = {
        'NR_160754': 'C.gloeosporioides',
        'NR_160836': 'C.magnum',
        'NR_120132': 'C.musae',
        'NR_152271': 'C.orbiculare',
        'DQ126254': 'G.graminicola.MO-178',
        'DQ126248': 'G.graminicola.IN-300170',
        'DQ126158': 'C.cereale.CA-62',
        'DQ126226': 'C.cereale.NY-16',
        'DQ126219': 'C.cereale.NJ-4990'
        }

## okay, how to use this to relabel the tree

accs = list(genbankDict.keys())
for i in accs:
    ## find it in the newick string, change it 
    r = re.compile(i+'\.[0-9]{,1}')
    aa = r.sub(genbankDict[i], aa)

## write it out:
with open('grahamAllSeqsTreeRelabeled.nwk', 'w') as fout:
    fout.write(aa)

plt.close('all')
ITStree = Phylo.read('grahamAllSeqsTreeRelabeled.nwk', 'newick')
Phylo.draw(ITStree)


##############################

## can we get this up and running on my work machine?

## the first step would be to try and repeat graham's NMS's

source('http://bioconductor.org/biocLite.R') 
## nope

if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

library(BiocManager)

BiocManager::install(version = '3.12')

BiocManager::install('phyloseq')

BiocManager::install('DESeq2')

## that took forever. 

## while we're at it can we get jupyter notebooks going on this machine:

conda install -c conda-forge jupyter notebook

## can this installation handle all the old data no problem?

## let's find out by trying to repeat graham's nms for French flat. 

## first, can we get the biom table in no problem?

library(phyloseq)
library(vegan)
library('DESeq2')

biom97 <- import_biom('/home/daniel/Documents/analyses/grass-endophyte-community/grass_97_wmeta.biom')

biom97
## looks good. 

## so, we want to be able to subset by site, then run nmses on each categorized by 
## the host

## get our otu tables for each site

## for 1:


biom97_noCon = subset_samples(biom97, site != 'Control')

## oh shit, we never saved our variance stabilized biom table. That was silly. 

## so rerun:

aa <- otu_table(biom97_noCon)@.Data
bb <- varianceStabilizingTransformation(aa)
bb[bb < 0] <- 0
biom97vs <- biom97_noCon
otu_table(biom97vs) <- otu_table(bb, taxa_are_rows = TRUE)
biom97vs = filter_taxa(biom97vs, function(x) sum(x) > 0.1, T)

#save(biom97vs, file='biom97vs.rda')
load('biom97vs.rda')

## okay, now how can we rerun the NMSes from this?

## start with FF, for example:
ffBiom <- subset_samples(biom97vs, site == "French_flat")

biom97
biom97vs
ffBiom 

metaMDS

help(metaMDS)
## I think sites are rows for vegan?
data(package='vegan')
data(dune)
dune[1:5,1:5] ## yup

## OTU tables from 
otu_table(ffBiom)

ffComm <- t(otu_table(ffBiom)@.Data)
ffComm[1:5,1:5] ## yup, better


ffCommNMS = metaMDS(ffComm)

## pretty clean, stress = 0.17

MDS1 <- ffCommNMS$points[,'MDS1']
MDS2 <- ffCommNMS$points[,'MDS2']

## color by host...
## just to be sure the hosts line up properly:

host = sample_data(ffBiom)[,'host']@.Data[[1]]
hostNu <- sapply(host, function(x){as.numeric(x == "D. californica")+1})

## I forget, can we use colors as numbers?:

plot(MDS1, MDS2,
    col=hostNu,
    pch=19,
    cex=2.0,
)
text(MDS1, MDS2-0.05, host)
ordiellipse(ffCommNMS, host, col=c(2,1), lwd=3)

names(table(host))

plot(1:2,c(1,1), col=2)

## hmm. Seems like pretty clean separation to me. 
## might be due to the way I variance-stabilized.
## as in, might have been due to shared rare OTUs. 
## anyway, before I think about this too much
## generalize into a function

unique(sample_data(biom97vs)$site)


load('biom97vs.rda')
par(mfrow=c(2,2))
for (i in c("French_flat","Hazel_Dell","Horse_Rock","Whidbey")){
    sBiom <- subset_samples(biom97vs, site == i)
    sComm <- t(otu_table(sBiom)@.Data)
    sCommNMS = metaMDS(sComm)
    MDS1 <- sCommNMS$points[,'MDS1']
    MDS2 <- sCommNMS$points[,'MDS2']
    host = sample_data(sBiom)[,'host']@.Data[[1]]
    hostNu <- sapply(host, function(x){as.numeric(x == "D. californica")+1})
    plot(MDS1, MDS2,
        col=hostNu,
        pch=19,
        cex=2.0,
        main=i,
    )
    text(MDS1, MDS2-0.05, host)
    ordiellipse(sCommNMS, host, col=c(2,1), lwd=3)
}

## okay. So At french flat, things are more intermixed than the others, somehow. 
## do we have more shared species at ff?

## at french flat, shared otus in the variance stab are:

ffBiom

ffDanthBiom <- subset_samples(ffBiom, host == "D. californica")
ffFestuBiom <- subset_samples(ffBiom, host == "F. roemeri")

ffDanthBiom
ffFestuBiom

ffDanthBiomNo0 <- otu_table(ffDanthBiom)[rowSums(otu_table(ffDanthBiom)) > 0]
ffFestuBiomNo0 <- otu_table(ffFestuBiom)[rowSums(otu_table(ffFestuBiom)) > 0]

sum(rownames(ffDanthBiomNo0) %in% rownames(ffFestuBiomNo0))
sum(rownames(ffFestuBiomNo0) %in% rownames(ffDanthBiomNo0))
## they share 404 species out of 
dim(ffDanthBiomNo0)
dim(ffFestuBiomNo0)
## 734 OTUs (Danth) and 857 OTUs (Festu)
## wow, so they shared ~50% of their species? Is this normal?

for (i in c("French_flat","Hazel_Dell","Horse_Rock","Whidbey")){
    sBiom <- subset_samples(biom97vs, site == i)
    DanthBiom <- subset_samples(sBiom, host == "D. californica")
    FestuBiom <- subset_samples(sBiom, host == "F. roemeri")
    DanthBiomNo0 <- otu_table(DanthBiom)[rowSums(otu_table(DanthBiom)) > 0]
    FestuBiomNo0 <- otu_table(FestuBiom)[rowSums(otu_table(FestuBiom)) > 0]
    cat(i)
    cat(paste(" shared OTUs, both hosts: ", 
          sum(rownames(FestuBiomNo0) %in% rownames(DanthBiomNo0)), "\n", sep=''))
    cat("Out of: \n") 
    cat(paste("Danthonia OTU total: ", nrow(DanthBiomNo0), "\n", sep=''))
    cat(paste("Festuca OTU total: ", nrow(FestuBiomNo0), sep=''))
    cat("\n\n")
}

## actually, it's kind of normal...at most of these sites the
## hosts are sharing 1/3 or 1/2 of their OTUs


## I think the way to do this is to sum the individual plots into one grand
## site (so FF has only two rows, one for each host, with columns for OTUs) 
## calculate BC distance between them. If graham's nmses are representative, then 
## at FF we should see a lower BC between the two hosts than at other 
## sites

## how to do this in R? do we need to maintain the same colums to keep our 
## dimensions intact to be comparable?

vegdist

rowSums(otu_table(ffDanthBiom))
rowSums(otu_table(ffFestuBiom))

aa <- rbind(rowSums(otu_table(ffDanthBiom)), rowSums(otu_table(ffFestuBiom)))
dim(aa)
vegdist(aa, method='bray')

## generalize this:

for (i in c("French_flat","Hazel_Dell","Horse_Rock","Whidbey")){
    sBiom <- subset_samples(biom97vs, site == i)
    sDanthBiom <- subset_samples(sBiom, host == "D. californica")
    sFestuBiom <- subset_samples(sBiom, host == "F. roemeri")
    aa <- rbind(rowSums(otu_table(sDanthBiom)), rowSums(otu_table(sFestuBiom)))
    cat(paste("BC distance",i,"=", vegdist(aa, method='bray')[1], "\n", sep=' '))
}

vegdist(aa, method='bray')

paste("BC distance between hosts at"+i)

## another thing to look at here is how many of the indicator species are 
## shared? If a higher proportion of species unique to site are shared,
## also suggests some convergence on microbiomes

dim(ffDanthBiomNo0 )

dim(ffFestuBiomNo0 )

## I think that settles that, there isn't really anything different 

cat(i);cat('\n')

############ check graham's OTU pipeline ###############

## we need to check graham's variance stabilization, see 
## how different is from ours.

## basically, I need to exactly recover his NMSes and see
## why the are different from the ones I generated. 

## some notes

## I think the file of interest, as in the graphic we are trying to repeat is:
"grass_host_nmds_facet.tiff"

## this is on line 895 or so in Graham's text

## if this is right, and if grahams notes are correct,
## looks like this was done with presence/absence data?

## so I'll work backward and try to collect all the code to create this...

#### get a ref seq file in BASH ####

## the closest thing we have to a reference seq file is:

oldFasta="/home/daniel/Documents/analyses/grass-endophyte-community/otus_97_uclust_relabel.fasta"

## I think we need to get rid of the size labels on this
## otherwise it fits

sed "s/;size=[0-9]*;//" <(tail $oldFasta) ## looks okay, do it

sed "s/;size=[0-9]*;//" $oldFasta > otus_97_forPhyloseqRef.fasta

################################################

###### blasting ref seqs to mock community #####

## my understanding is that we made a blastn database out of our 
## mock sequences, then blasted our reference sequences against 
## this and made removed all the strong matches to positive 
## controls, because with index bleed we accidentally contaminated
## the run with my mock community.   

## so reconstructing graham's biom as used to make his NMS graphic
## will only be possible if we can get or build that mock community
## fasta...graham doesn't have it in the repo... do I have that somewhere? 

## ah yes, in the old taiwan repo...

#### make a mc blast database ####

## bash

## make our database for blast:
makeblastdb -in BioI-6098_OConnor_34875.seq.txt -dbtype nucl -logfile dberrors.txt

## now we need to blast our reference seqs against this

## the goal is to be able to exclude anything that matches 
## the positive controls, and to remove all contaminates

refseqfilename="otus_97_forPhyloseqRef.fasta"

blastn -query "otus_97_forPhyloseqRef.fasta" -db "BioI-6098_OConnor_34875.seq.txt" -out mc_matches.csv -outfmt 10 -max_target_seqs 1

sed '1 i\qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' mc_matches.csv -i
#

## check it
libreoffice --calc mc_matches.csv &

###############################################

## and back into R, did this work okay?

library(phyloseq)
library(Biostrings)

biom <- '/home/daniel/Documents/analyses/grass-endophyte-community/grass_97_wmeta.biom'
refseqfilename="otus_97_forPhyloseqRef.fasta"
grass_biom <- import_biom(biom, refseqfilename=refseqfilename)
grass_biom_eco <- subset_samples(grass_biom, site != "Control")


blast <- read.csv('mc_matches.csv')

blast <- read.csv('/Users/grahambailes/grass_endophyte_community/alt_biom/mock_community/mcblast_all_97.csv', header=T)

goodblast <- blast[blast$pident > 95 & blast$length > 100,]



dim(blast)

dim(goodblast)

#####

pcotus <- !(rownames(otu_table(grass_biom)) %in% goodblast$qseqid)

 
grass.cont.rem <- grass_biom_eco

grass_biom_rem_mc <- prune_taxa(pcotus, grass.cont.rem) 

grass_biom_vs_cont_rem <- DESeq_varstab(grass_biom_rem_mc, ~site) 
## ^ this what he used to make grass_host_nmds_facet.tiff 

## v below this he removes negative taxa and goes P/A 
grass_mat <- t(otu_table(grass_biom_vs_cont_rem)@.Data) 
grass_mat <- grass_mat[,colSums(grass_mat)>0] 
grass_mat_PA <- grass_mat
grass_mat_PA[grass_mat_PA > 0] <- 1 
grass_dist_pa <- vegdist(grass_mat_PA, method = 'jaccard')
grass_ord <- metaMDS(grass_mat_PA, distance = 'jaccard', try = 20, trymax = 100)
## but does he ever use this?

## seems to go the vegan route instead
fd_host <- plot_ordination(grass_biom_vs_cont_rem, grass_ord, color = "host")
fd_host <- fd_host + theme_bw() + stat_ellipse()
fd_host <- fd_host + theme(legend.text = element_text(face = "italic"))+
  theme(legend.text=element_text(size=13), legend.title = element_text(size = 14))
plot(fd_host)

## so he has two phyloseq objs at this point of interest, one sort of according my 
## pipelines from the 2017 paper that tried to reduce the signature of 
## index hopping in our data, and one on the raw results of the DEseq2 results
 


