## let's have a look at Graham's pipeline. 
## for the record, I've added SSH key access for this repo.
## In case I need to do this again (github just stopped 
## password access), you have to change the remote url to a
## format that ssh can work with:

git remote set-url origin git@github.com:gbailes/grass-endophyte-community.git

## as far as grahams paper, I think I was supposed to:

## """
## 1. Check trend surfaces of community matrix varation - can we describe the 
## variation in graham's community matrices with a north/sound trend in diversity 
## and dissimilarity? I think I recommended this in place of the original dbmem analysis, 
## if it turns out to be informative.  

## 2. Attempt to quantify the apparent S->N increase in diversity. Means repeating 
## Grahams diversity metrics on the pre-variance-stabilization (pre-deseq), then 
## check to see if there is enough statistical power to build a point pattern model from these.  

## 3. Figure out the story of the french flat site, the only place where the two grass 
## host species share a lot of endophyte species. What are the shared species? 
## Are they unique to the site? Are they interesting, as in maybe one of the species 
## that have been shown to give panicum grass salt and drought tolerance?
## """

## this all comes from a 1.5 year-old email (Feb 2, 2020), "review of goals for Dan with Graham Paper"
## #3 was the doozy. 

## what is the general strategy here...

## the first two goals (#1 and #2) will be in R. Not sure I agree with the 
## approaches I suggested originally, with point patterns, etc. I think a 
## simple.

## the second will be sort of a treasure hunt. I think that one will take 
## the most time. 

## let's go through them in order...

###### #1 general N/S trend ########
## let's jog our memories about trend surfaces
## with the tutorial from Borcard, on the orobatid mite data:

## ugh, there is a new version of R out, they are on 4.x.x now. The beast grows
## in its power... let's stay in the 3.x.x world for the moment. 

## get the "s.value" function out the supp materials for borcard:

R 

library('ade4')
library('vegan')

source('/home/daniel/Documents/Books/Stats/numericalEcologyR/NEwR-2ed_code_data/NEwR-2ed_code_data/NEwR2-Functions/sr.value.R')

xygrid <- expand.grid(1:10, 1:10)

plot(xygrid)

xygrid.c <- scale(xygrid, scale=FALSE)

X <- xygrid.c[,1]
Y <- xygrid.c[,2]

par(mfrow=c(3,2))

par(mfrow=c(1,1))

sr.value(xygrid, (X))
sr.value(xygrid, (Y))
sr.value(xygrid, (X+Y))
sr.value(xygrid, (X^2+Y))
sr.value(xygrid, (X^2+Y^2))
sr.value(xygrid, (X^3))

sr.value(xygrid, (X^2))


## cool. Lots of patterns are possible. 

## onward with the tutorial...

data('mite')
data('mite.env')
data('mite.xy')

mite.h <- decostand(mite, "hellinger")

mite.xy.c <- scale(mite.xy, center=TRUE, scale=FALSE)

mite.poly <- poly(as.matrix(mite.xy.c), degree=3, raw=TRUE)

## raw=TRUE retains all the requested polynomials. if FALSE,
## apparently it retains only the truly orthogonal ones.

## so we can try all kinds of polynomial surfaces out as 
## possible models. 

## modeling the N-S bioiversity trend (#2 above) should be
## pretty simple, then. Just use richness, simpsons, whatever, 
## and model it on a linear S-N increasing plane. Univariate,
## so very straightforward

## butu for modeling patterns in community variation, 
## how can we use these polynomial surfaces to model a multivariate
## community? 

## borcard says with RDAs:

colnames(mite.poly) = c("X", "X2", "X3", "Y", "XY", "X2Y", "Y2", "XY2", "Y3")  

mite.trend.rda <- rda(mite.h ~ ., data=as.data.frame(mite.poly))

R2adj.poly <- RsquareAdj(mite.trend.rda)

## from here Borcard backs up, and uses the orthogonal polynomials.
## I don't really understand how orthogonal polys are made, its magic
## to me. But here we go:

mite.poly.ortho <- poly(as.matrix(mite.xy), degree=3)
colnames(mite.poly.ortho) = c("X", "X2", "X3", "Y", "XY", "X2Y", "Y2", "XY2", "Y3")  
mite.trend.rda.ortho <- rda(mite.h ~ ., data=as.data.frame(mite.poly.ortho))


mite.trend.rda.ortho

mite.trend.rda

R2adj.poly <- RsquareAdj(mite.trend.rda.ortho)$adj.r.squared

## for some reason, these are giving me exactly the same results.
## not going to take the time to figure out why...probably typo...

## forward selection 
## forward.sel doesn't seem to exist anymore.
## I think the ortholog here is ordistep or ordiR2step.

mod0 <- rda(mite.h ~ 1, data=as.data.frame(mite.poly.ortho))
mod1 <- rda(mite.h ~ ., data=as.data.frame(mite.poly.ortho))

mite.trend.fwd <- ordiR2step(mod0, mod1)

## as per example, six terms retained, I think?
## use these

mite.trend.rda2 <- rda(mite.h ~ . 

as.data.frame(mite.poly)

as.data.frame(mite.poly)[

mite.trend.fwd[,2]


str(mite.trend.fwd)
#############################################################################
str(mod1)

## great, so ordiR2step drops the uninformative components for you. 

## so how do we know from this which were the important polynomials?


str(mite.trend.fwd)

str(mite.trend.fwd$terminfo)

str(mite.trend.fwd$terminfo$terms)

## um, is this the only way to extract this?
attributes(mite.trend.fwd$terminfo$ordered)$names

## probably not. seems stupid. But for the moment it 
## will work:

sigTerms <- attributes(mite.trend.fwd$terminfo$ordered)$names

## check significance:

anova.cca(mite.trend.fwd)

anova.cca(mite.trend.fwd, by="axis")
## RDA1 is by far the most important. But I don't really understand these 
## new axes. Are they in terms the spatial predictors or the community 
## matrix "response" variables? 
## I assume they are in terms of the spatial predictors, the polynomials,
## because there are six of them, matching the number of polynomials.

## can we look at them?

mite.trend.fit <- scores.cca(mite.trend.fwd, choices = c(1,2,3), display="lc", scaling=1)

## that don't work anymore...how can we get these out of the model....
## looks like they updated their methods to fit in with everybody else:

mite.trend.fit <- scores(mite.trend.fwd, choices = c(1,2,3), display="lc", scaling=1)

par(mfrow=c(1,3))
sr.value(mite.xy,mite.trend.fit[,1])
sr.value(mite.xy,mite.trend.fit[,2])
sr.value(mite.xy,mite.trend.fit[,3])

## great. that all makes some kind of sense. How do we apply to our 
## situation?
## we will have a multivariate community matrix (Graham's endophyte matrix), and a 
## a single variate diversity index. 
## we want to check them both for trend surfaces.

## with the biodiversity, we suspect a simple, first order linear equation 
## with a northward increase in biodiversity

## for the multivariate community data, I think we just want to look for interesting
## spatial patterns. So maybe, rerun the above analysis on grahams community data.

## either way, we need his biome table. From this we can work backword get diversity 
## metrics. 

## where is this? graham has a biome file ready...

## so next step will be to remember how to manipulate 

## phyloseq, etc. Oh jeezus. 
  
########## get graham's biom table into memory ##########

## the way to do this used to be in R, in phyloseq. Are folks still doing this?

## a quick lit search shows lots of citations for phyloseq...

## let's assume phyloseq is still cool. 

library(phyloseq)

biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')

head(sample_data(biom97))

colnames(sample_data(biom97))

tax_table(biom97)[0:3,]

head(tax_table(biom97))


otu_table(biom97)[0:3,]

dim(otu_table(biom97))

## great. This looks pre-deseq transformation, which is also good. 

## so what's task number one?

## the most obvious task is the bioiversity regression. 

## we need to rarefy to an even number of sequences, and 
## get good, UTM-style xy coordinates for these sites. 

## so, for the moment, get the estimated UTMs for each site. How?

## let's go pyproj. lat/long epsg is 4326

## let's export the sample data to something python can use:

class(sample_data(biom97))

str(sample_data(biom97))

write.csv(sample_data(biom97), file='grahamSample.csv')


##### over to python, to wrestle out the UTMs for these sites ######

import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
from scipy import stats
from sklearn.linear_model import LinearRegression


grahamSample = pd.read_csv('grahamSample.csv', index_col=0)

grahamSample.columns

grahamSample.site.to_list()

grahamSample.site.unique().shape

## pare down to one row for each site:

grahamSample.head()

#aa = grahamSample[['samplename','site','latitude','longitude']]
## don't keep unique ids

grahamSample.head()

## seems like we have two locations for Hazel Dell:
grs = grahamSample.groupby('site')
grs.get_group('Hazel_Dell').latitude.unique()
grs.get_group('Hazel_Dell').longitude.unique()
## so let's change all Hazel dell latitudes to be 
## the same, keeping the more southerly (lower) value:

grahamSample.latitude.dtype

sum(grahamSample.latitude == 44.03) ## 12 samples
sum(grahamSample.latitude == 44.02) ## also 12 samples

## assign all to a single value:
hd4403 = grahamSample.latitude == 44.03
grahamSample.latitude[hd4403] = 44.02

sum(grahamSample.latitude == 44.03)
sum(grahamSample.latitude == 44.02)
## looks good

aa = grahamSample[['site','latitude','longitude']]
bb = aa.reset_index(drop=True)
## just to make sure the lat longs are indeed the same for each site:
#cc = bb.groupby('site')
#cc.get_group('Control')
#cc.get_group('French_flat') ## etc
#cc.groups.keys()
## anyway, 
dd = bb.drop_duplicates()
dd.set_index('site', inplace=True)
ee = dd.drop('Control')
## great. Now how to convert these? go to geopanda? 
grahamSites = (gpd.GeoDataFrame(
    ee, geometry=gpd.points_from_xy(ee.longitude, ee.latitude))
    .drop(['latitude','longitude'], axis=1)
)



grahamSites.crs

grahamSites.set_crs('EPSG:4326', inplace=True)

grahamSites.crs

## reproject to UTM zone 10, which is EPSG:26710
grahamSites.to_crs('EPSG:26710', inplace=True)

grahamSites.crs

#grahamSites.to_csv('grahamSiteCoords.csv')
## not a perfect export, have to modify with vim to get a good csv
## but works

## and maybe save us some work later:
#grahamSites.to_pickle('grahamSiteCoords.p')

## check it on some maps. Here are some publicly available geojsons:

## in shell ##
wget https://raw.githubusercontent.com/johan/world.geo.json/master/countries/USA/WA.geo.json
wget https://raw.githubusercontent.com/johan/world.geo.json/master/countries/USA/OR.geo.json
## in shell ##

## so how does it look?:


oregon = gpd.read_file('OR.geo.json')
oregon.to_crs('EPSG:26710', inplace=True)
washington = gpd.read_file('WA.geo.json')
washington.to_crs('EPSG:26710', inplace=True)

plt.close('all')
fig, ax = plt.subplots()
oregon.plot(ax=ax)
washington.plot(ax=ax)
grahamSites.plot(ax=ax, color='red')

## great. What else do we need here? Not much, methinks...back to R?

######

## now, we want to give species richness, simpsons or something for each 
## site.

## we need to rarify to some sort of common of denominator. How are our 
## read depths?

## drop the controls

aa = subset_samples(biom97, site != 'Control')

controls = subset_samples(biom97, site == 'Control')


sample_data(biom97)$site

options(scipen=999)

par(mfrow=c(1,3))
barplot(sort(sample_sums(biom97), decreasing=TRUE))
barplot(sort(sample_sums(aa), decreasing=TRUE))
barplot(sort(sample_sums(controls), decreasing=TRUE))


par(mfrow=c(1,1))
barplot(sort(sample_sums(controls), decreasing=TRUE))

sample_sums(biom97)

sort(sample_sums(biom97))


data[order(data$num,decreasing = TRUE),]

bar_plot(sample_sums(biom97))

aalost <- aalost[order(aalost[,'164wood'], decreasing = TRUE),]

data(enterotype)
TopNOTUs = names(sort(taxa_sums(enterotype), TRUE)[1:10])
ent10 = prune_taxa(TopNOTUs, enterotype)
plot_bar(ent10, "SeqTech", fill = "Enterotype", facet_grid = ~Genus)


biom97rar = rarefy_even_depth(biom97)
## wow, we lost a lot of diversity that way,
## but to be expected, I guess. 

aarar = rarefy_even_depth(aa)
## that helps a lot, there was a very small sample in the controls,
## probably the single species control
## we still rarefy down to 7101, which is a pretty low common denominator...
## how much do we lose if we go to 20,000?

min(sample_sums(aa)) 

sort(sample_sums(aa))[0:20]

sum(sample_sums(aa) < 20000) ## six samples. 
## that's not too many out of 155 samples...
## are they important? where are they?

sortedSamples <- sort(sample_sums(aa))

size = 20000
smallerThanSize <- sortedSamples[sortedSamples < size]


sample_data(biom97)[names(smallerThanSize),'site']
## shit. These are mostly french flat. We will lose
## 5 of 12 FF sites if we drop our 5 lowest sample 
## depths. Sucks. 


bb = subset_samples(biom97, site != 'Control')

sample_sums(aarar)

## to answer if I can get rid of this, have to ask a stupid question:
##  why are there so many samples? 
## 9 sites, 2 hosts, some controls... I would have predicted 
## = ~25 samples.
## but graham has 155. 
## how do these part out by site?

biom97 

head(sample_data(biom97))

head(sample_data(biom97))

## I head data exploration in R. look at this with pandas

write.csv(sample_data(biom97), file="grahamsSampleData.csv")

##### back to python #####

## imports above

## I'm betting that graham kept geographic info for each 
## tuft that he sampled, or the origin of each transect,
## and the location of that grass on its transect. 

## so for each of the nine sites, there is probably a sub-map
## that could be made. 

## don't want to make it, don't think we need it. 
## the main question is, can we pool these plants somehow
## to make the sites comparable for diversity metrics?

sampleData = pd.read_csv("grahamsSampleData.csv", index_col=0)

sampleData.columns

aa = sampleData[['host', 'site']]

## there are two hosts, we are going to need to work with each 
## host individually. Not sure, may also be useful to pool them
## at some point. But can't see the point of that for the moment.

## I think we'll want to start with one host, and then repeat 
## all analyses for for the other, copy-n-paste. 

sampleFroem = sampleData[sampleData.host == "F. roemeri"] 
## 84 plants F.roemerii sampled

sampleDanth = sampleData[sampleData.host == 'D. californica'] 
## 71 of Danthonia

## are these relatively evenly distributed among the sites?

plt.close('all')
fig, ax = plt.subplots(1,2)
sampleDanth.site.hist(ax = ax[0])
sampleFroem.site.hist(ax = ax[1])

## yep. Missing one from horse rock,
## but otherwise all have 12 plants each

sampleFroem.site


sampleDanth.site.unique().shape



sampleDanth.site.unique()

dir(sampleFroem.site.unique())

aa = pd.Series(sampleFroem.site.unique())

bb = pd.Series(sampleDanth.site.unique())

aa[aa.isin(bb)]

bb[~bb.isin(aa)]

## great, so we can afford to lose a few 
## low-abundance samples. problem is,
## listed above, these are almost all 
## from french flat
## another option is to go to 10000 reads,
## which seems like a respectable number

## so let's do several levels, rarefy to 20000 reads,
## and also to 10,000 and 7,000. Then we can compare and
## see how much information is lost by going down to 7000.


#### and back in R. #####

## does phyloseq have a built-in adjustment 
## for this?

biom97_noCon = subset_samples(biom97, site != 'Control')
controls = subset_samples(biom97, site == 'Control')


sample_data(biom97)$site

options(scipen=999)

sampsize=7000
biom97_rar.i = rarefy_even_depth(biom97_noCon, sample.size = sampsize, rngseed = 1)

## 8220 OTUs lost. 

sample_sums(biom97_rar.i)

sample_sums(biom97_noCon)

biom97_rar.i
biom97_noCon

biom97_rar.i

## so now, get species richness for each:

plot_richness(biom97_rar.i)

plot_richness(biom97_rar.i, measures='Observed')

## great. but as usual, phyloseq is doing more than I 
## want it to. Can we just get the richnness values without 
## the fancy graphics?

## I think this is what I need:
?estimate_richness

## vegan has some options, too
?estimateR
?vegan::diversity

sample_data(biom97_rar.i)

dim(sample_data(biom97_rar.i))

aa <- estimate_richness(biom97_rar.i, measures=c('Simpson', 'Observed'))

?estimate_richness

head(aa)

## we'll need these:

write.csv(aa, file='diversity7000.csv')
write.csv(sample_data(biom97_rar.i), file='biom97_rar7000.csv')

## great. but we also need site, x, and y on this. I hate doing this 
## shit in R. Anyway, for the moment I think we got what we need out of 
## of phyloseq?  

## pingpong back to python

## okay, so we want to make a geopanda with: 
## unique sample name, host type, site, UTMs
## for each sample...

aa = pd.read_csv('diversity7000.csv')
aa.columns=['sampleName','Observed','Simpson']
aa['sampleName'] = aa.sampleName.str.replace('X', '')
aa.set_index('sampleName', inplace=True)
bb = pd.read_csv('biom97_rar7000.csv', index_col=0)
bb = bb[['host','site', 'samplename']]
cc = pd.read_pickle('grahamSiteCoords.p')
dd = bb.merge(aa, left_index=True, right_index=True)
## make sure to merge using the GPD so it stays GPD
grahamDivGPD = cc.merge(dd, right_on='site', left_index=True)
#grahamDivGPD.to_pickle('grahamDivGPD.p')


## now what? We want to check a general north/south trend in 
## biodiversity...

## can we get our y-coords out somehow?

grahamDivGPD['geometry'].y

## so can we regress this against the richness and simpsons?:

stats.linregress(
grahamDivGPD['geometry'].y,
grahamDivGPD['Observed'],
)

## yeah, that is significant, r=.26, so r2 ~ 0.067?

## tomorrow plot this. 

## here is tomorrow. 

fig, ax = plt.subplots()
plt.scatter(grahamDivGPD['geometry'].y, grahamDivGPD['Observed'])

## combine these into points with error

grahamDivGPD

aa = grahamDivGPD.groupby('site').mean()
bb = grahamDivGPD.groupby('site').std()
cc = aa.merge(bb, left_index=True, right_index=True)
cc.columns = ['richness', 'simpson', 'richnessSD', 'simpsonSD']
siteRichness = grahamSites.merge(cc, left_index=True, right_index=True)

plt.close('all')
fig, ax = plt.subplots()
ax.errorbar(x=siteRichness['geometry'].y, 
            y=siteRichness['richness'],
            yerr=siteRichness['richnessSD'],
            fmt='o',
)
## ols line
X = siteRichness['geometry'].y.to_numpy().reshape(-1,1)
Y = siteRichness['richness'].to_numpy().reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), 
        c='k', linewidth=2, #linestyle= "dotted",
        label='OLS linear regression model'
       )
## site labels:
labs = siteRichness.index.to_series()
xy = zip(siteRichness['geometry'].y, siteRichness['richness'])
for i,j in enumerate(xy):
    ax.annotate(labs[i], xy=j)

## so to sum up:

import os
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt; plt.ion()
from scipy import stats
from sklearn.linear_model import LinearRegression

## we need a siteRichness gpd for each level...

def siteRichnessGPD(gpdDiv):
    ## convert diversity results
    aa = pd.read_csv(gpdDiv)
    aa.columns=['sampleName','Observed','Simpson']
    aa['sampleName'] = aa.sampleName.str.replace('X', '')
    aa.set_index('sampleName', inplace=True)
    ## convert sample data, just to get the site name and host
    bb = pd.read_csv('grahamSample.csv', index_col=0)
    bb = bb[['host','site', 'samplename']]
    ## bring our site geo info again:
    grahamSites = pd.read_pickle('grahamSiteCoords.p')
    ## attach name and host onto the diversity data
    dd = bb.merge(aa, left_index=True, right_index=True)
    ## make a gpdf
    grahamDivGPD = grahamSites.merge(dd, right_on='site', left_index=True)
    del(aa,bb,dd)
    aa = grahamDivGPD.groupby('site').mean()
    bb = grahamDivGPD.groupby('site').std()
    cc = aa.merge(bb, left_index=True, right_index=True)
    cc.columns = ['richness', 'simpson', 'richnessSD', 'simpsonSD']
    siteRichness = grahamSites.merge(cc, left_index=True, right_index=True)
    return(siteRichness)

def plotDiv(gpdRich, tit='', ax=None):
    if ax is not None: ax=ax
    if ax is None: fig, ax = plt.subplots()
    ax.errorbar(x=gpdRich['geometry'].y, 
                y=gpdRich['richness'],
                yerr=gpdRich['richnessSD'],
                fmt='o',)
    ## ols line
    X = gpdRich['geometry'].y.to_numpy().reshape(-1,1)
    Y = gpdRich['richness'].to_numpy().reshape(-1,1)
    ax.plot( X, LinearRegression().fit(X, Y).predict(X), 
            c='k', linewidth=2, #linestyle= "dotted",
            label='OLS linear regression model'
           )
    ## site labels:
    labs = gpdRich.index.to_series()
    xy = zip(gpdRich['geometry'].y, gpdRich['richness'])
    for i,j in enumerate(xy):
        ax.annotate(labs[i], xy=j)
    ## include linear regression results
    corStats = stats.linregress( gpdRich['geometry'].y, gpdRich['richness'])
    corString = (f'Pearson\'s r = {round(corStats[2],3)}, p = {round(corStats[3],3)}')
    top = ax.get_ylim()[1]
    right = ax.get_xlim()[1]
    ax.text(right, top, corString,
            horizontalalignment='right',
            verticalalignment='top')
    ax.set_title(tit)


## using these two functions, plus the above import block, we should
## be able to streamline this process:

gpdDiv_filename = 'diversity7000.csv'
gpdRich = siteRichnessGPD(gpdDiv_filename)
plotDiv(gpdRich, tit=gpdDiv_filename)

fig, axes = plt.subplots(1,2)
axes = np.ravel(axes)
plotDiv(gpdRich, tit=gpdDiv_filename, ax=axes[1])

## great. Let's go back to R and make a pipeline for creating richness data 
## from each level of rarefaction:


##### in R #########

## need the following:

library(phyloseq)
## read in graham's data:
biom97 <- import_biom('~/Documents/analyses/grahamGrass/grass-endophyte-community/grass_97_wmeta.biom')
## get rid of controls
biom97_noCon = subset_samples(biom97, site != 'Control')


## split by host, with a few different read depths:
sampsizes = c(7000, 18000, 30000, 40000)
hosts = c("F. roemeri", "D. californica")

for (j in hosts){
    biom97.i = subset_samples(biom97_noCon, host == j)
    for (i in sampsizes){
        filename = paste(j,"diversity",i,".csv", sep='')
        biom97_rar.i = rarefy_even_depth(biom97.i, sample.size = i, rngseed = 1)
        aa <- estimate_richness(biom97_rar.i, measures=c('Simpson', 'Observed'))
        write.csv(aa, file=filename)
    }
}


#### now try these out in our code above, in python ####

import os

os.listdir()

## using the above functions and import block:

gpdDiv_filenames = ['diversity30000.csv','diversity20000.csv', 'diversity10000.csv', 'diversity7000.csv']

fig, axes = plt.subplots(2,1)

axes = np.ravel(axes)

gpdDiv_filenames = ['diversity7000.csv','diversity20000.csv']

for i,j in enumerate(gpdDiv_filenames):
    gpdRich = siteRichnessGPD(j)
    plotDiv(gpdRich, tit=j)

gpdDiv_filenames = ['diversity7000.csv','diversity20000.csv']

for i,j in enumerate(gpdDiv_filenames):
    gpdRich = siteRichnessGPD(j)
    fig, axes = plt.subplots(2,1, figsize=(8,20))
    plotDiv(gpdRich, tit=j, ax=axes[i])

## yeah, not much difference. 

## let's start a notebook, to keep a sensible record of all this.

## okay, I've lost track of where I'm going with this
## also, we forgot to split up by host. 
## and it looks like we need to keep the latitudes 
## as hosts were sampled from two different sites, 
## with different latitudes

## how can we straighten all this out....

## I think we need to debug the pipeline as I have it to this point
 
## and then redo as above, split by host, etc.

## honestly, I don't think that the slight difference in 
## latitude of the HD sites matters right now, so leave it for
## simplicity's sake.

## so task number one is split the above analysis by host.

## as it is set up right now, simple to do the all-host 
## div-by-lat check:

gpdDiv_filenames = ['diversity7000.csv','diversity20000.csv']

fig, axes = plt.subplots(2,1, figsize=(8,16))

for i,j in enumerate(gpdDiv_filenames):
    gpdRich = siteRichnessGPD(j)    
    plotDiv(gpdRich, tit=j, ax=axes[i])

## but we have to the split before this, the species richness
## calculations are done in R. So back to the beast.  

plt.close('all')

gpdDiv_filenames = [ i for i in os.listdir() if "diversity" in i ]

for i,j in enumerate(gpdDiv_filenames):
    gpdRich = siteRichnessGPD(j)    
    plotDiv(gpdRich, tit=j)

## in general, it looks like Danthonia has a strong N/S diversity
## gradient, but not Festuca. 

## so, what do we do with this? report it in the notebook...




